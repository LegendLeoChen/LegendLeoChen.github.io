<!DOCTYPE html>


<html lang="zh-CN">
  

    <head>
      <meta charset="utf-8" />
        
      <meta name="description" content="一个秘密空间" />
      
      <meta
        name="viewport"
        content="width=device-width, initial-scale=1, maximum-scale=1"
      />
      <title>大模型微调：SFT（LoRA、Prefix）和RLHF |  LegendLeo Chen 的空间</title>
  <meta name="generator" content="hexo-theme-ayer">
      
      <link rel="shortcut icon" href="/mylogo.ico" />
       
<link rel="stylesheet" href="/dist/main.css">

      
<link rel="stylesheet" href="/css/fonts/remixicon.css">

      
<link rel="stylesheet" href="/css/custom.css">
 
      <script src="https://cdn.staticfile.org/pace/1.2.4/pace.min.js"></script>
       
 

      <link
        rel="stylesheet"
        href="https://cdn.jsdelivr.net/npm/@sweetalert2/theme-bulma@5.0.1/bulma.min.css"
      />
      <script src="https://cdn.jsdelivr.net/npm/sweetalert2@11.0.19/dist/sweetalert2.min.js"></script>

      <!-- mermaid -->
      
      <style>
        .swal2-styled.swal2-confirm {
          font-size: 1.6rem;
        }
      </style>
<!-- 封面标闪烁 -->
<link rel="stylesheet" href="/css/zhyBlogTitle.css">
<script src="https://cdn.bootcdn.net/ajax/libs/highlight.js/9.18.1/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad();</script>
<!-- jquery，懒加载、统计、说说需要的jquery -->
<script src="https://cdn.bootcdn.net/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css">
<link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"></head>
  </html>
</html>


<body>
  <div id="app">
    
      
    <main class="content on">
      <section class="outer">
  <article
  id="post-大模型微调：SFT（LoRA、Prefix）和RLHF"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h1 class="article-title sea-center" style="border-left:0" itemprop="name">
  大模型微调：SFT（LoRA、Prefix）和RLHF
</h1>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2025/02/06/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%EF%BC%9ASFT%EF%BC%88LoRA%E3%80%81Prefix%EF%BC%89%E5%92%8CRLHF/" class="article-date">
  <time datetime="2025-02-06T12:22:13.000Z" itemprop="datePublished">2025-02-06</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/">强化学习</a> / <a class="article-category-link" href="/categories/%E6%95%99%E7%A8%8B/">教程</a> / <a class="article-category-link" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a>
  </div>
  
<div class="word_count">
    <span class="post-time">
        <span class="post-meta-item-icon">
            <i class="ri-quill-pen-line"></i>
            <span class="post-meta-item-text"> 字数统计:</span>
            <span class="post-count">3.6k</span>
        </span>
    </span>

    <span class="post-time">
        &nbsp; | &nbsp;
        <span class="post-meta-item-icon">
            <i class="ri-book-open-line"></i>
            <span class="post-meta-item-text"> 阅读时长≈</span>
            <span class="post-count">17 分钟</span>
        </span>
    </span>
</div>
 
    </div>
      
    <div class="tocbot"></div>




  
    <div class="article-entry" itemprop="articleBody">
       
  <p>本次借助huggingface平台下载大模型和数据集，并在本地尝试进行微调，目的是跑通并体验微调过程。微调使用的是参数高效微调也就是PEFT，策略是有监督微调SFT（LoRA和Prefix）和RLHF。<span id="more"></span><br><strong>注意：huggingface下载都是需要代理的，文中出现的python库自行用pip安装即可。</strong></p>
<h1 id="🔥模型下载"><a href="#🔥模型下载" class="headerlink" title="🔥模型下载"></a>🔥模型下载</h1><p><a target="_blank" rel="noopener" href="https://huggingface.co/">huggingface</a>上选择模型，这里以Qwen&#x2F;Qwen2.5-3B-Instruct为例。</p>
<ul>
<li><p>如下图可以直接在界面中的files and versions中下载模型，把所有文件下载进一个文件夹即可。<br><img src="/2025/02/06/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%EF%BC%9ASFT%EF%BC%88LoRA%E3%80%81Prefix%EF%BC%89%E5%92%8CRLHF/%E6%A8%A1%E5%9E%8B%E9%80%89%E6%8B%A9.jpg" alt="huggingface模型界面"></p>
</li>
<li><p>也可以通过命令行一次性下载，先通过<code>pip install -U huggingface_hub</code>安装命令行工具，在cmd中通过以下指令登录huggingface并下载模型：</p>
</li>
</ul>
<pre class="line-numbers language-bash"><code class="language-bash">huggingface-cli login
huggingface-cli download --resume-download Qwen/Qwen2.5-3B-Instruct --local-dir D://你的存储路径
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>
<ul>
<li>首先第一句指令登录，会让你输入huggingface个人账户界面的access tokens的令牌，输入后即可通过第二条指令下载模型到指定位置。</li>
</ul>
<h1 id="🔥数据集下载"><a href="#🔥数据集下载" class="headerlink" title="🔥数据集下载"></a>🔥数据集下载</h1><p><a target="_blank" rel="noopener" href="https://huggingface.co/">huggingface</a>上选择数据集，以弱智吧数据集 LooksJuicy&#x2F;ruozhiba 为例。</p>
<ul>
<li>通过python的datasets库可以直接下载源文件：</li>
</ul>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> torch
<span class="token keyword">from</span> datasets <span class="token keyword">import</span> Dataset<span class="token punctuation">,</span> load_dataset<span class="token punctuation">,</span> load_from_disk
<span class="token keyword">import</span> os

<span class="token comment" spellcheck="true"># 配置你的代理</span>
os<span class="token punctuation">.</span>environ<span class="token punctuation">[</span><span class="token string">'HTTP_PROXY'</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token string">'http://127.0.0.1:7890'</span>
os<span class="token punctuation">.</span>environ<span class="token punctuation">[</span><span class="token string">'HTTPS_PROXY'</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token string">'http://127.0.0.1:7890'</span>

dataset <span class="token operator">=</span> load_dataset<span class="token punctuation">(</span><span class="token string">"LooksJuicy/ruozhiba"</span><span class="token punctuation">,</span> split<span class="token operator">=</span><span class="token string">'train'</span><span class="token punctuation">)</span>
dataset<span class="token punctuation">.</span>save_to_disk<span class="token punctuation">(</span><span class="token string">"./datasets/ruozhiba"</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 保存到该目录下</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p><img src="/2025/02/06/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%EF%BC%9ASFT%EF%BC%88LoRA%E3%80%81Prefix%EF%BC%89%E5%92%8CRLHF/%E6%95%B0%E6%8D%AE%E9%9B%86%E6%A0%BC%E5%BC%8F.jpg" alt="数据集格式"></p>
<ul>
<li>这样只是下载源文件没有进行处理，在官网可以预览文件内容如上，该数据集由 instruction 和 output 组成，比较简单。</li>
<li>我们可以直接通过如下代码进行预处理，思路就是缓存数据集源文件并转换成可以用于训练的message（json格式），最后保存到json文件当中，这次不划分数据集，直接全部用于训练。</li>
<li>缓存是不会自动清掉的，可以在C盘的.cache里面手动删掉。</li>
</ul>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">from</span> datasets <span class="token keyword">import</span> load_from_disk<span class="token punctuation">,</span> load_dataset
<span class="token keyword">import</span> os

<span class="token comment" spellcheck="true"># 配置代理</span>
os<span class="token punctuation">.</span>environ<span class="token punctuation">[</span><span class="token string">'HTTP_PROXY'</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token string">'http://127.0.0.1:7890'</span>
os<span class="token punctuation">.</span>environ<span class="token punctuation">[</span><span class="token string">'HTTPS_PROXY'</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token string">'http://127.0.0.1:7890'</span>
<span class="token comment" spellcheck="true"># 系统 prompt，可以自行设置</span>
system_message <span class="token operator">=</span> <span class="token string">"回答问题"</span>

<span class="token comment" spellcheck="true"># 转换为 messages</span>
<span class="token keyword">def</span> <span class="token function">create_conversation</span><span class="token punctuation">(</span>sample<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> <span class="token punctuation">{</span>
        <span class="token string">"messages"</span><span class="token punctuation">:</span> <span class="token punctuation">[</span>
            <span class="token punctuation">{</span><span class="token string">"role"</span><span class="token punctuation">:</span> <span class="token string">"system"</span><span class="token punctuation">,</span> <span class="token string">"content"</span><span class="token punctuation">:</span> system_message<span class="token punctuation">}</span><span class="token punctuation">,</span>
            <span class="token punctuation">{</span><span class="token string">"role"</span><span class="token punctuation">:</span> <span class="token string">"user"</span><span class="token punctuation">,</span> <span class="token string">"content"</span><span class="token punctuation">:</span> sample<span class="token punctuation">[</span><span class="token string">"instruction"</span><span class="token punctuation">]</span><span class="token punctuation">}</span><span class="token punctuation">,</span>
            <span class="token punctuation">{</span><span class="token string">"role"</span><span class="token punctuation">:</span> <span class="token string">"assistant"</span><span class="token punctuation">,</span> <span class="token string">"content"</span><span class="token punctuation">:</span> sample<span class="token punctuation">[</span><span class="token string">"output"</span><span class="token punctuation">]</span><span class="token punctuation">}</span>
        <span class="token punctuation">]</span>
    <span class="token punctuation">}</span>

<span class="token comment" spellcheck="true"># 从 hub 加载数据集</span>
dataset <span class="token operator">=</span> load_dataset<span class="token punctuation">(</span><span class="token string">"LooksJuicy/ruozhiba"</span><span class="token punctuation">,</span> split<span class="token operator">=</span><span class="token string">"train"</span><span class="token punctuation">)</span>

<span class="token comment" spellcheck="true"># 转换 dataset 为 OAI messages</span>
dataset <span class="token operator">=</span> dataset<span class="token punctuation">.</span>map<span class="token punctuation">(</span>create_conversation<span class="token punctuation">,</span> remove_columns<span class="token operator">=</span>dataset<span class="token punctuation">.</span>features<span class="token punctuation">,</span> batched<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span>dataset<span class="token punctuation">[</span><span class="token number">345</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">"messages"</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

<span class="token comment" spellcheck="true"># 保存到磁盘</span>
dataset<span class="token punctuation">.</span>to_json<span class="token punctuation">(</span><span class="token string">"train_dataset.json"</span><span class="token punctuation">,</span> orient<span class="token operator">=</span><span class="token string">"records"</span><span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h1 id="🔥SFT（LoRA）"><a href="#🔥SFT（LoRA）" class="headerlink" title="🔥SFT（LoRA）"></a>🔥SFT（LoRA）</h1><h2 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h2><ul>
<li>有了数据集和模型，就可以进行微调了，首先是LoRA微调，使用transformers、trl等库进行微调。</li>
<li>主要流程就是加载数据集、加载模型及其分词器、配置LoRA参数、配置训练参数，最后就可以定义训练器进行训练了。流程上和传统深度学习一样，只不过对应的库都进行了封装，不需要手动编写训练的常规流程。</li>
<li>超参数可以自行调整，<strong>减少显存占用</strong>可以降低批次大小<code>per_device_train_batch_size</code>和梯度积累<code>gradient_accumulation_steps</code>，<strong>减少训练总时长</strong>可以减少迭代数<code>num_train_epochs</code>（可以小于1）或调整批次大小。其他参数可以自行尝试。</li>
<li>注意：生成的LoRA模型是<strong>增量模型</strong>，也就是依赖原模型存在，所以生成的模型不会很大，如有需要可以自行查找方法来保存完整模型。</li>
<li>代码如下：</li>
</ul>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token punctuation">,</span> os
<span class="token keyword">from</span> transformers <span class="token keyword">import</span> AutoTokenizer<span class="token punctuation">,</span> AutoModelForCausalLM<span class="token punctuation">,</span> BitsAndBytesConfig
<span class="token keyword">from</span> trl <span class="token keyword">import</span> SFTTrainer<span class="token punctuation">,</span> setup_chat_format
<span class="token keyword">from</span> peft <span class="token keyword">import</span> LoraConfig
<span class="token keyword">from</span> datasets <span class="token keyword">import</span> load_dataset
<span class="token keyword">from</span> transformers <span class="token keyword">import</span> TrainingArguments
<span class="token keyword">import</span> warnings
warnings<span class="token punctuation">.</span>filterwarnings<span class="token punctuation">(</span><span class="token string">"ignore"</span><span class="token punctuation">,</span> message<span class="token operator">=</span><span class="token string">"`tokenizer` is deprecated"</span><span class="token punctuation">)</span>
warnings<span class="token punctuation">.</span>filterwarnings<span class="token punctuation">(</span><span class="token string">"ignore"</span><span class="token punctuation">,</span> message<span class="token operator">=</span><span class="token string">"`use_cache=True` is incompatible"</span><span class="token punctuation">)</span>
warnings<span class="token punctuation">.</span>filterwarnings<span class="token punctuation">(</span><span class="token string">"ignore"</span><span class="token punctuation">,</span> message<span class="token operator">=</span><span class="token string">"torch.utils.checkpoint: please pass in use_reentrant"</span><span class="token punctuation">)</span>
warnings<span class="token punctuation">.</span>filterwarnings<span class="token punctuation">(</span><span class="token string">"ignore"</span><span class="token punctuation">,</span> message<span class="token operator">=</span><span class="token string">"Torch was not compiled with flash attention"</span><span class="token punctuation">)</span>

<span class="token comment" spellcheck="true"># 设置设备和环境</span>
device <span class="token operator">=</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">"cuda"</span> <span class="token keyword">if</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">else</span> <span class="token string">"cpu"</span><span class="token punctuation">)</span>

<span class="token comment" spellcheck="true"># 加载数据集</span>
dataset <span class="token operator">=</span> load_dataset<span class="token punctuation">(</span><span class="token string">"json"</span><span class="token punctuation">,</span> data_files<span class="token operator">=</span><span class="token string">"train_dataset.json所在路径"</span><span class="token punctuation">,</span> split<span class="token operator">=</span><span class="token string">"train"</span><span class="token punctuation">)</span>

<span class="token comment" spellcheck="true"># 加载模型和分词器</span>
model_id <span class="token operator">=</span> <span class="token string">"qwen2.5-3b模型文件夹的路径"</span>
bnb_config <span class="token operator">=</span> BitsAndBytesConfig<span class="token punctuation">(</span>
    load_in_4bit<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
    bnb_4bit_use_double_quant<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
    bnb_4bit_quant_type<span class="token operator">=</span><span class="token string">"nf4"</span><span class="token punctuation">,</span>
    bnb_4bit_compute_dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>bfloat16
<span class="token punctuation">)</span>

model <span class="token operator">=</span> AutoModelForCausalLM<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>
    model_id<span class="token punctuation">,</span>
    device_map<span class="token operator">=</span><span class="token string">"auto"</span><span class="token punctuation">,</span>
    torch_dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>bfloat16<span class="token punctuation">,</span>
    quantization_config<span class="token operator">=</span>bnb_config<span class="token punctuation">,</span>
<span class="token punctuation">)</span>
tokenizer <span class="token operator">=</span> AutoTokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>model_id<span class="token punctuation">)</span>
tokenizer<span class="token punctuation">.</span>padding_side <span class="token operator">=</span> <span class="token string">'right'</span>

<span class="token comment" spellcheck="true"># 配置LoRA</span>
peft_config <span class="token operator">=</span> LoraConfig<span class="token punctuation">(</span>
    lora_alpha<span class="token operator">=</span><span class="token number">16</span><span class="token punctuation">,</span>
    lora_dropout<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">,</span>
    r<span class="token operator">=</span><span class="token number">64</span><span class="token punctuation">,</span>
    bias<span class="token operator">=</span><span class="token string">"none"</span><span class="token punctuation">,</span>
    task_type<span class="token operator">=</span><span class="token string">"CAUSAL_LM"</span><span class="token punctuation">,</span>
<span class="token punctuation">)</span>

<span class="token comment" spellcheck="true"># 定义训练参数</span>
args <span class="token operator">=</span> TrainingArguments<span class="token punctuation">(</span>
    output_dir<span class="token operator">=</span>os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span>os<span class="token punctuation">.</span>getcwd<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">"保存模型的路径"</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    num_train_epochs<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">,</span>
    per_device_train_batch_size<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">,</span>
    gradient_accumulation_steps<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span>
    gradient_checkpointing<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
    optim<span class="token operator">=</span><span class="token string">"adamw_torch_fused"</span><span class="token punctuation">,</span>
    logging_steps<span class="token operator">=</span><span class="token number">200</span><span class="token punctuation">,</span>
    save_strategy<span class="token operator">=</span><span class="token string">"steps"</span><span class="token punctuation">,</span>
    save_steps<span class="token operator">=</span><span class="token number">1000</span><span class="token punctuation">,</span>
    learning_rate<span class="token operator">=</span><span class="token number">3e</span><span class="token operator">-</span><span class="token number">4</span><span class="token punctuation">,</span>
    fp16<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>  <span class="token comment" spellcheck="true"># 启用 fp16 混合精度训练</span>
    max_grad_norm<span class="token operator">=</span><span class="token number">0.3</span><span class="token punctuation">,</span>
    warmup_ratio<span class="token operator">=</span><span class="token number">0.03</span><span class="token punctuation">,</span>
    lr_scheduler_type<span class="token operator">=</span><span class="token string">"constant"</span><span class="token punctuation">,</span>
    push_to_hub<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>
    report_to<span class="token operator">=</span><span class="token string">"tensorboard"</span><span class="token punctuation">,</span>
<span class="token punctuation">)</span>

<span class="token comment" spellcheck="true"># 创建SFTTrainer</span>
trainer <span class="token operator">=</span> SFTTrainer<span class="token punctuation">(</span>
    model<span class="token operator">=</span>model<span class="token punctuation">,</span>
    args<span class="token operator">=</span>args<span class="token punctuation">,</span>
    train_dataset<span class="token operator">=</span>dataset<span class="token punctuation">,</span>
    peft_config<span class="token operator">=</span>peft_config<span class="token punctuation">,</span>
    tokenizer<span class="token operator">=</span>tokenizer<span class="token punctuation">,</span>
<span class="token punctuation">)</span>

<span class="token comment" spellcheck="true"># 开始训练</span>
trainer<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span>
trainer<span class="token punctuation">.</span>save_model<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h2 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h2><ul>
<li>训练完成后就可以进行问答，通过以下脚本实现，测试时为了使得模型稳定生成结果，可以将温度值<code>temperature</code>调小一些。</li>
</ul>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">from</span> transformers <span class="token keyword">import</span> pipeline

<span class="token comment" spellcheck="true"># 加载模型</span>
pipe <span class="token operator">=</span> pipeline<span class="token punctuation">(</span><span class="token string">"text-generation"</span><span class="token punctuation">,</span> model<span class="token operator">=</span><span class="token string">"微调后的模型文件夹"</span><span class="token punctuation">)</span>
<span class="token comment" spellcheck="true"># 提供输入</span>
input_text <span class="token operator">=</span> <span class="token string">"为什么我的银行卡在高压锅里煮了一晚上，还是冻结状态？"</span>

<span class="token comment" spellcheck="true"># 调整生成参数</span>
output <span class="token operator">=</span> pipe<span class="token punctuation">(</span>
    input_text<span class="token punctuation">,</span>
    max_length<span class="token operator">=</span><span class="token number">1024</span><span class="token punctuation">,</span>  <span class="token comment" spellcheck="true"># 增加生成的最大长度</span>
    num_return_sequences<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>  <span class="token comment" spellcheck="true"># 生成多个序列</span>
    temperature<span class="token operator">=</span><span class="token number">0.01</span><span class="token punctuation">,</span>
<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>output<span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p><img src="/2025/02/06/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%EF%BC%9ASFT%EF%BC%88LoRA%E3%80%81Prefix%EF%BC%89%E5%92%8CRLHF/%E6%B5%8B%E8%AF%95.png" alt="测试结果"></p>
<ul>
<li>可以看到模型是可以应对弱智吧的问题。</li>
</ul>
<h1 id="🔥SFT（Prefix）"><a href="#🔥SFT（Prefix）" class="headerlink" title="🔥SFT（Prefix）"></a>🔥SFT（Prefix）</h1><ul>
<li>prefix和lora在代码的区别上很小，效果差不太多，所以就只展示代码。</li>
</ul>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> torch<span class="token punctuation">,</span> os
<span class="token keyword">from</span> transformers <span class="token keyword">import</span> AutoTokenizer<span class="token punctuation">,</span> AutoModelForCausalLM<span class="token punctuation">,</span> BitsAndBytesConfig
<span class="token keyword">from</span> trl <span class="token keyword">import</span> SFTTrainer
<span class="token keyword">from</span> peft <span class="token keyword">import</span> PrefixTuningConfig
<span class="token keyword">from</span> datasets <span class="token keyword">import</span> load_dataset<span class="token punctuation">,</span> DatasetDict
<span class="token keyword">from</span> transformers <span class="token keyword">import</span> TrainingArguments
<span class="token keyword">import</span> warnings

warnings<span class="token punctuation">.</span>filterwarnings<span class="token punctuation">(</span><span class="token string">"ignore"</span><span class="token punctuation">,</span> message<span class="token operator">=</span><span class="token string">"`tokenizer` is deprecated"</span><span class="token punctuation">)</span>
warnings<span class="token punctuation">.</span>filterwarnings<span class="token punctuation">(</span><span class="token string">"ignore"</span><span class="token punctuation">,</span> message<span class="token operator">=</span><span class="token string">"`use_cache=True` is incompatible"</span><span class="token punctuation">)</span>
warnings<span class="token punctuation">.</span>filterwarnings<span class="token punctuation">(</span><span class="token string">"ignore"</span><span class="token punctuation">,</span> message<span class="token operator">=</span><span class="token string">"torch.utils.checkpoint: please pass in use_reentrant"</span><span class="token punctuation">)</span>
warnings<span class="token punctuation">.</span>filterwarnings<span class="token punctuation">(</span><span class="token string">"ignore"</span><span class="token punctuation">,</span> message<span class="token operator">=</span><span class="token string">"Torch was not compiled with flash attention"</span><span class="token punctuation">)</span>

<span class="token comment" spellcheck="true"># 设置设备和环境</span>
device <span class="token operator">=</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">"cuda"</span> <span class="token keyword">if</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">else</span> <span class="token string">"cpu"</span><span class="token punctuation">)</span>

<span class="token comment" spellcheck="true"># 加载数据集</span>
dataset <span class="token operator">=</span> load_dataset<span class="token punctuation">(</span><span class="token string">"json"</span><span class="token punctuation">,</span> data_files<span class="token operator">=</span><span class="token string">"./datasets/ruozhiba/train_dataset_1.json"</span><span class="token punctuation">,</span> split<span class="token operator">=</span><span class="token string">"train"</span><span class="token punctuation">)</span>
<span class="token comment" spellcheck="true"># 划分训练集和验证集</span>
train_test_split <span class="token operator">=</span> dataset<span class="token punctuation">.</span>train_test_split<span class="token punctuation">(</span>test_size<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">,</span> seed<span class="token operator">=</span><span class="token number">42</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 10% 的数据作为验证集</span>
<span class="token comment" spellcheck="true"># 创建 DatasetDict</span>
dataset_dict <span class="token operator">=</span> DatasetDict<span class="token punctuation">(</span><span class="token punctuation">{</span>
    <span class="token string">"train"</span><span class="token punctuation">:</span> train_test_split<span class="token punctuation">[</span><span class="token string">"train"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    <span class="token string">"validation"</span><span class="token punctuation">:</span> train_test_split<span class="token punctuation">[</span><span class="token string">"test"</span><span class="token punctuation">]</span>
<span class="token punctuation">}</span><span class="token punctuation">)</span>

<span class="token comment" spellcheck="true"># 加载模型和分词器</span>
model_id <span class="token operator">=</span> <span class="token string">"./models/qwen2.5-3b"</span>
bnb_config <span class="token operator">=</span> BitsAndBytesConfig<span class="token punctuation">(</span>
    load_in_4bit<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
    bnb_4bit_use_double_quant<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
    bnb_4bit_quant_type<span class="token operator">=</span><span class="token string">"nf4"</span><span class="token punctuation">,</span>
    bnb_4bit_compute_dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>bfloat16
<span class="token punctuation">)</span>

model <span class="token operator">=</span> AutoModelForCausalLM<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>
    model_id<span class="token punctuation">,</span>
    device_map<span class="token operator">=</span><span class="token string">"auto"</span><span class="token punctuation">,</span>
    torch_dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>bfloat16<span class="token punctuation">,</span>
    quantization_config<span class="token operator">=</span>bnb_config<span class="token punctuation">,</span>
<span class="token punctuation">)</span>
tokenizer <span class="token operator">=</span> AutoTokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>model_id<span class="token punctuation">,</span> padding_side<span class="token operator">=</span><span class="token string">"right"</span><span class="token punctuation">,</span> truncation<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>

<span class="token comment" spellcheck="true"># 配置Prefix Tuning</span>
prefix_config <span class="token operator">=</span> PrefixTuningConfig<span class="token punctuation">(</span>
    peft_type<span class="token operator">=</span><span class="token string">"PREFIX_TUNING"</span><span class="token punctuation">,</span>
    task_type<span class="token operator">=</span><span class="token string">"CAUSAL_LM"</span><span class="token punctuation">,</span>
    num_virtual_tokens<span class="token operator">=</span><span class="token number">20</span><span class="token punctuation">,</span>      <span class="token comment" spellcheck="true"># 虚拟前缀的长度</span>
    token_dim<span class="token operator">=</span><span class="token number">2048</span><span class="token punctuation">,</span>             <span class="token comment" spellcheck="true"># 前缀嵌入的维度，同原模型</span>
    num_layers<span class="token operator">=</span><span class="token number">36</span><span class="token punctuation">,</span>              <span class="token comment" spellcheck="true"># 模型隐藏层数，同原模型</span>
    prefix_projection<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>     <span class="token comment" spellcheck="true"># 是否对前缀进行投影</span>
    encoder_hidden_size<span class="token operator">=</span><span class="token number">2048</span>    <span class="token comment" spellcheck="true"># 前缀编码器的隐藏层大小，同原模型</span>
<span class="token punctuation">)</span>

<span class="token comment" spellcheck="true"># 定义训练参数</span>
args <span class="token operator">=</span> TrainingArguments<span class="token punctuation">(</span>
    output_dir<span class="token operator">=</span>os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span>os<span class="token punctuation">.</span>getcwd<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">"results/qwen2.5-finetuned-prefix"</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    num_train_epochs<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>
    per_device_train_batch_size<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span>
    gradient_accumulation_steps<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span>
    gradient_checkpointing<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>
    optim<span class="token operator">=</span><span class="token string">"adamw_torch_fused"</span><span class="token punctuation">,</span>
    logging_steps<span class="token operator">=</span><span class="token number">200</span><span class="token punctuation">,</span>
    save_strategy<span class="token operator">=</span><span class="token string">"steps"</span><span class="token punctuation">,</span>
    save_steps<span class="token operator">=</span><span class="token number">400</span><span class="token punctuation">,</span>
    evaluation_strategy<span class="token operator">=</span><span class="token string">"steps"</span><span class="token punctuation">,</span>  <span class="token comment" spellcheck="true"># 添加评估策略</span>
    eval_steps<span class="token operator">=</span><span class="token number">200</span><span class="token punctuation">,</span>  <span class="token comment" spellcheck="true"># 每 200 步评估一次</span>
    learning_rate<span class="token operator">=</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">4</span><span class="token punctuation">,</span>
    fp16<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>  <span class="token comment" spellcheck="true"># 启用 fp16 混合精度训练，提升效率</span>
    max_grad_norm<span class="token operator">=</span><span class="token number">0.3</span><span class="token punctuation">,</span>
    warmup_ratio<span class="token operator">=</span><span class="token number">0.3</span><span class="token punctuation">,</span>
    lr_scheduler_type<span class="token operator">=</span><span class="token string">"linear"</span><span class="token punctuation">,</span>
    push_to_hub<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>
    report_to<span class="token operator">=</span><span class="token string">"tensorboard"</span><span class="token punctuation">,</span>
    load_best_model_at_end<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>  <span class="token comment" spellcheck="true"># 确保加载最佳模型</span>
    metric_for_best_model<span class="token operator">=</span><span class="token string">"loss"</span><span class="token punctuation">,</span>  <span class="token comment" spellcheck="true"># 监控验证集损失</span>
    greater_is_better<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>  <span class="token comment" spellcheck="true"># 损失越小越好</span>
<span class="token punctuation">)</span>

<span class="token comment" spellcheck="true"># 创建SFTTrainer</span>
trainer <span class="token operator">=</span> SFTTrainer<span class="token punctuation">(</span>
    model<span class="token operator">=</span>model<span class="token punctuation">,</span>
    args<span class="token operator">=</span>args<span class="token punctuation">,</span>
    train_dataset<span class="token operator">=</span>dataset_dict<span class="token punctuation">[</span><span class="token string">"train"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    eval_dataset<span class="token operator">=</span>dataset_dict<span class="token punctuation">[</span><span class="token string">"validation"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>  <span class="token comment" spellcheck="true"># 添加验证集</span>
    peft_config<span class="token operator">=</span>prefix_config<span class="token punctuation">,</span>  <span class="token comment" spellcheck="true"># 使用PrefixTuningConfig</span>
    tokenizer<span class="token operator">=</span>tokenizer<span class="token punctuation">,</span>
<span class="token punctuation">)</span>

<span class="token comment" spellcheck="true"># 开始训练</span>
trainer<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span>
trainer<span class="token punctuation">.</span>save_model<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h1 id="🔥RLHF（GRPO）"><a href="#🔥RLHF（GRPO）" class="headerlink" title="🔥RLHF（GRPO）"></a>🔥RLHF（GRPO）</h1><p>接下来就是强化学习方法，用的是GRPO算法。</p>
<blockquote>
<p>GRPO的核心思想是通过组内相对奖励来优化策略模型，而不是依赖传统的价值网络（critic model）。具体来说，对于每个输入问题，模型会生成一组可能的输出，然后通过这些输出之间的相对表现来进行优化。这种方法消除了对独立评估器的需求，使得训练过程更加高效。减少计算负担、稳定性和可控性强。</p>
</blockquote>
<h2 id="准备"><a href="#准备" class="headerlink" title="准备"></a>准备</h2><ul>
<li>由于个人电脑有可能没法应付qwen2.5-3b的RLHF微调，所以可以按照之前的方法下载<strong>Qwen&#x2F;Qwen2.5-0.5B-Instruct</strong>，然后数据集使用<strong>openai&#x2F;gsm8k</strong>，是一个英文的数学题的数据集，在输出最末尾附有正确答案。</li>
</ul>
<h2 id="训练-1"><a href="#训练-1" class="headerlink" title="训练"></a>训练</h2><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> re
<span class="token keyword">import</span> torch
<span class="token keyword">from</span> datasets <span class="token keyword">import</span> load_dataset<span class="token punctuation">,</span> Dataset
<span class="token keyword">from</span> transformers <span class="token keyword">import</span> AutoTokenizer<span class="token punctuation">,</span> AutoModelForCausalLM
<span class="token keyword">from</span> trl <span class="token keyword">import</span> GRPOConfig<span class="token punctuation">,</span> GRPOTrainer
<span class="token keyword">from</span> typing <span class="token keyword">import</span> Union

SYSTEM_PROMPT <span class="token operator">=</span> <span class="token triple-quoted-string string">"""
Respond in the following format:
&lt;reasoning>
...
&lt;/reasoning>
&lt;answer>
...
&lt;/answer>
"""</span>
XML_COT_FORMAT <span class="token operator">=</span> <span class="token triple-quoted-string string">"""\
&lt;reasoning>
{reasoning}
&lt;/reasoning>
&lt;answer>
{answer}
&lt;/answer>
"""</span>

<span class="token comment" spellcheck="true"># 数据、数据集处理部分</span>
<span class="token keyword">def</span> <span class="token function">extract_xml_answer</span><span class="token punctuation">(</span>text<span class="token punctuation">:</span> str<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> str<span class="token punctuation">:</span>       <span class="token comment" spellcheck="true"># 获取llm的回答部分</span>
    answer <span class="token operator">=</span> text<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">"&lt;answer>"</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span>
    answer <span class="token operator">=</span> answer<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">"&lt;/answer>"</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
    <span class="token keyword">return</span> answer<span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token keyword">def</span> <span class="token function">extract_hash_answer</span><span class="token punctuation">(</span>text<span class="token punctuation">:</span> str<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> Union<span class="token punctuation">[</span>str<span class="token punctuation">,</span> None<span class="token punctuation">]</span><span class="token punctuation">:</span>   <span class="token comment" spellcheck="true"># 获取正确答案</span>
    <span class="token keyword">if</span> <span class="token string">"####"</span> <span class="token operator">not</span> <span class="token keyword">in</span> text<span class="token punctuation">:</span>
        <span class="token keyword">return</span> None
    <span class="token keyword">return</span> text<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">"####"</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token comment" spellcheck="true"># uncomment middle messages for 1-shot prompting</span>
<span class="token keyword">def</span> <span class="token function">get_gsm8k_questions</span><span class="token punctuation">(</span>split <span class="token operator">=</span> <span class="token string">"train"</span><span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> Dataset<span class="token punctuation">:</span>    <span class="token comment" spellcheck="true"># 获得数据集并加入prompt</span>
    data <span class="token operator">=</span> load_dataset<span class="token punctuation">(</span><span class="token string">'openai/gsm8k'</span><span class="token punctuation">,</span> <span class="token string">'main'</span><span class="token punctuation">)</span><span class="token punctuation">[</span>split<span class="token punctuation">]</span> <span class="token comment" spellcheck="true"># type: ignore</span>
    data <span class="token operator">=</span> data<span class="token punctuation">.</span>map<span class="token punctuation">(</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span> <span class="token punctuation">{</span> <span class="token comment" spellcheck="true"># type: ignore</span>
        <span class="token string">'prompt'</span><span class="token punctuation">:</span> <span class="token punctuation">[</span>
            <span class="token punctuation">{</span><span class="token string">'role'</span><span class="token punctuation">:</span> <span class="token string">'system'</span><span class="token punctuation">,</span> <span class="token string">'content'</span><span class="token punctuation">:</span> SYSTEM_PROMPT<span class="token punctuation">}</span><span class="token punctuation">,</span>
            <span class="token punctuation">{</span><span class="token string">'role'</span><span class="token punctuation">:</span> <span class="token string">'user'</span><span class="token punctuation">,</span> <span class="token string">'content'</span><span class="token punctuation">:</span> x<span class="token punctuation">[</span><span class="token string">'question'</span><span class="token punctuation">]</span><span class="token punctuation">}</span>
        <span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token string">'answer'</span><span class="token punctuation">:</span> extract_hash_answer<span class="token punctuation">(</span>x<span class="token punctuation">[</span><span class="token string">'answer'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    <span class="token punctuation">}</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># type: ignore</span>
    <span class="token keyword">return</span> data <span class="token comment" spellcheck="true"># type: ignore</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<ul>
<li>首先导入库后，设置好prompt来引导模型按<code>&lt;reasoning&gt;</code>和<code>&lt;answer&gt;</code>的xml格式输出推导过程和答案，这样方便我们提取出答案来进行奖励判断。</li>
<li>然后就是处理数据集和输出文本的函数了，<code>extract_xml_answer</code>用于将生成好的文本提取出答案；<code>extract_hash_answer</code>则是获取数据集中的答案；<code>get_gsm8k_questions</code>则加载数据集，并且融入prompt。</li>
</ul>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">correctness_reward_func</span><span class="token punctuation">(</span>prompts<span class="token punctuation">,</span> completions<span class="token punctuation">,</span> answer<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> list<span class="token punctuation">[</span>float<span class="token punctuation">]</span><span class="token punctuation">:</span>
    responses <span class="token operator">=</span> <span class="token punctuation">[</span>completion<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">'content'</span><span class="token punctuation">]</span> <span class="token keyword">for</span> completion <span class="token keyword">in</span> completions<span class="token punctuation">]</span>
    q <span class="token operator">=</span> prompts<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">'content'</span><span class="token punctuation">]</span>
    extracted_responses <span class="token operator">=</span> <span class="token punctuation">[</span>extract_xml_answer<span class="token punctuation">(</span>r<span class="token punctuation">)</span> <span class="token keyword">for</span> r <span class="token keyword">in</span> responses<span class="token punctuation">]</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'-'</span><span class="token operator">*</span><span class="token number">20</span><span class="token punctuation">,</span> f<span class="token string">"Question:\n{q}"</span><span class="token punctuation">,</span> f<span class="token string">"\nAnswer:\n{answer[0]}"</span><span class="token punctuation">,</span> f<span class="token string">"\nResponse:\n{responses[0]}"</span><span class="token punctuation">,</span> f<span class="token string">"\nExtracted:\n{extracted_responses[0]}"</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> <span class="token punctuation">[</span><span class="token number">2.0</span> <span class="token keyword">if</span> r <span class="token operator">==</span> a <span class="token keyword">else</span> <span class="token number">0.0</span> <span class="token keyword">for</span> r<span class="token punctuation">,</span> a <span class="token keyword">in</span> zip<span class="token punctuation">(</span>extracted_responses<span class="token punctuation">,</span> answer<span class="token punctuation">)</span><span class="token punctuation">]</span>

<span class="token keyword">def</span> <span class="token function">int_reward_func</span><span class="token punctuation">(</span>completions<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> list<span class="token punctuation">[</span>float<span class="token punctuation">]</span><span class="token punctuation">:</span>
    responses <span class="token operator">=</span> <span class="token punctuation">[</span>completion<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">'content'</span><span class="token punctuation">]</span> <span class="token keyword">for</span> completion <span class="token keyword">in</span> completions<span class="token punctuation">]</span>
    extracted_responses <span class="token operator">=</span> <span class="token punctuation">[</span>extract_xml_answer<span class="token punctuation">(</span>r<span class="token punctuation">)</span> <span class="token keyword">for</span> r <span class="token keyword">in</span> responses<span class="token punctuation">]</span>
    <span class="token keyword">return</span> <span class="token punctuation">[</span><span class="token number">0.5</span> <span class="token keyword">if</span> r<span class="token punctuation">.</span>isdigit<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">else</span> <span class="token number">0.0</span> <span class="token keyword">for</span> r <span class="token keyword">in</span> extracted_responses<span class="token punctuation">]</span>

<span class="token keyword">def</span> <span class="token function">strict_format_reward_func</span><span class="token punctuation">(</span>completions<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> list<span class="token punctuation">[</span>float<span class="token punctuation">]</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""Reward function that checks if the completion has a specific format."""</span>
    pattern <span class="token operator">=</span> r<span class="token string">"^&lt;reasoning>\n.*?\n&lt;/reasoning>\n&lt;answer>\n.*?\n&lt;/answer>\n$"</span>
    responses <span class="token operator">=</span> <span class="token punctuation">[</span>completion<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">"content"</span><span class="token punctuation">]</span> <span class="token keyword">for</span> completion <span class="token keyword">in</span> completions<span class="token punctuation">]</span>
    matches <span class="token operator">=</span> <span class="token punctuation">[</span>re<span class="token punctuation">.</span>match<span class="token punctuation">(</span>pattern<span class="token punctuation">,</span> r<span class="token punctuation">)</span> <span class="token keyword">for</span> r <span class="token keyword">in</span> responses<span class="token punctuation">]</span>
    <span class="token keyword">return</span> <span class="token punctuation">[</span><span class="token number">0.5</span> <span class="token keyword">if</span> match <span class="token keyword">else</span> <span class="token number">0.0</span> <span class="token keyword">for</span> match <span class="token keyword">in</span> matches<span class="token punctuation">]</span>

<span class="token keyword">def</span> <span class="token function">soft_format_reward_func</span><span class="token punctuation">(</span>completions<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> list<span class="token punctuation">[</span>float<span class="token punctuation">]</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""Reward function that checks if the completion has a specific format."""</span>
    pattern <span class="token operator">=</span> r<span class="token string">"&lt;reasoning>.*?&lt;/reasoning>\s*&lt;answer>.*?&lt;/answer>"</span>
    responses <span class="token operator">=</span> <span class="token punctuation">[</span>completion<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">"content"</span><span class="token punctuation">]</span> <span class="token keyword">for</span> completion <span class="token keyword">in</span> completions<span class="token punctuation">]</span>
    matches <span class="token operator">=</span> <span class="token punctuation">[</span>re<span class="token punctuation">.</span>match<span class="token punctuation">(</span>pattern<span class="token punctuation">,</span> r<span class="token punctuation">)</span> <span class="token keyword">for</span> r <span class="token keyword">in</span> responses<span class="token punctuation">]</span>
    <span class="token keyword">return</span> <span class="token punctuation">[</span><span class="token number">0.5</span> <span class="token keyword">if</span> match <span class="token keyword">else</span> <span class="token number">0.0</span> <span class="token keyword">for</span> match <span class="token keyword">in</span> matches<span class="token punctuation">]</span>

<span class="token keyword">def</span> <span class="token function">count_xml</span><span class="token punctuation">(</span>text<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> float<span class="token punctuation">:</span>
    count <span class="token operator">=</span> <span class="token number">0.0</span>
    <span class="token keyword">if</span> text<span class="token punctuation">.</span>count<span class="token punctuation">(</span><span class="token string">"&lt;reasoning>\n"</span><span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">1</span><span class="token punctuation">:</span>
        count <span class="token operator">+=</span> <span class="token number">0.125</span>
    <span class="token keyword">if</span> text<span class="token punctuation">.</span>count<span class="token punctuation">(</span><span class="token string">"\n&lt;/reasoning>\n"</span><span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">1</span><span class="token punctuation">:</span>
        count <span class="token operator">+=</span> <span class="token number">0.125</span>
    <span class="token keyword">if</span> text<span class="token punctuation">.</span>count<span class="token punctuation">(</span><span class="token string">"\n&lt;answer>\n"</span><span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">1</span><span class="token punctuation">:</span>
        count <span class="token operator">+=</span> <span class="token number">0.125</span>
        count <span class="token operator">-=</span> len<span class="token punctuation">(</span>text<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">"\n&lt;/answer>\n"</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">*</span><span class="token number">0.001</span>
    <span class="token keyword">if</span> text<span class="token punctuation">.</span>count<span class="token punctuation">(</span><span class="token string">"\n&lt;/answer>"</span><span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">1</span><span class="token punctuation">:</span>
        count <span class="token operator">+=</span> <span class="token number">0.125</span>
        count <span class="token operator">-=</span> <span class="token punctuation">(</span>len<span class="token punctuation">(</span>text<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">"\n&lt;/answer>"</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token operator">*</span><span class="token number">0.001</span>
    <span class="token keyword">return</span> count

<span class="token keyword">def</span> <span class="token function">xmlcount_reward_func</span><span class="token punctuation">(</span>completions<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> list<span class="token punctuation">[</span>float<span class="token punctuation">]</span><span class="token punctuation">:</span>
    contents <span class="token operator">=</span> <span class="token punctuation">[</span>completion<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">"content"</span><span class="token punctuation">]</span> <span class="token keyword">for</span> completion <span class="token keyword">in</span> completions<span class="token punctuation">]</span>
    <span class="token keyword">return</span> <span class="token punctuation">[</span>count_xml<span class="token punctuation">(</span>c<span class="token punctuation">)</span> <span class="token keyword">for</span> c <span class="token keyword">in</span> contents<span class="token punctuation">]</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<ul>
<li>接着就是4个奖励函数，分别用于<strong>判断答案是否正确、是否回答整数、是否按格式生成答案（严格和宽松两种）</strong>。</li>
</ul>
<pre class="line-numbers language-python"><code class="language-python">dataset <span class="token operator">=</span> get_gsm8k_questions<span class="token punctuation">(</span><span class="token punctuation">)</span>
model_name <span class="token operator">=</span> <span class="token string">"./models/qwen2.5-3b"</span>

output_dir<span class="token operator">=</span><span class="token string">"results/qwen2.5-finetuned_GRPO"</span>
run_name<span class="token operator">=</span><span class="token string">"Qwen-2.5B-GRPO-gsm8k"</span>

training_args <span class="token operator">=</span> GRPOConfig<span class="token punctuation">(</span>
    output_dir<span class="token operator">=</span>output_dir<span class="token punctuation">,</span>
    run_name<span class="token operator">=</span>run_name<span class="token punctuation">,</span>
    learning_rate<span class="token operator">=</span><span class="token number">5e</span><span class="token operator">-</span><span class="token number">6</span><span class="token punctuation">,</span>
    adam_beta1 <span class="token operator">=</span> <span class="token number">0.9</span><span class="token punctuation">,</span>
    adam_beta2 <span class="token operator">=</span> <span class="token number">0.99</span><span class="token punctuation">,</span>
    weight_decay <span class="token operator">=</span> <span class="token number">0.1</span><span class="token punctuation">,</span>
    warmup_ratio <span class="token operator">=</span> <span class="token number">0.1</span><span class="token punctuation">,</span>
    lr_scheduler_type<span class="token operator">=</span><span class="token string">'cosine'</span><span class="token punctuation">,</span>
    logging_steps<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>
    bf16<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
    per_device_train_batch_size<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>
    gradient_accumulation_steps<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">,</span>
    num_generations<span class="token operator">=</span><span class="token number">16</span><span class="token punctuation">,</span>
    max_prompt_length<span class="token operator">=</span><span class="token number">256</span><span class="token punctuation">,</span>
    max_completion_length<span class="token operator">=</span><span class="token number">200</span><span class="token punctuation">,</span>
    num_train_epochs<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>
    save_steps<span class="token operator">=</span><span class="token number">100</span><span class="token punctuation">,</span>
    max_grad_norm<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">,</span>
    log_on_each_node<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>
    use_vllm<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>
    vllm_gpu_memory_utilization<span class="token operator">=</span><span class="token punctuation">.</span><span class="token number">3</span><span class="token punctuation">,</span>
    vllm_device<span class="token operator">=</span><span class="token string">"cuda:0"</span><span class="token punctuation">,</span>
    report_to<span class="token operator">=</span><span class="token string">"none"</span> <span class="token comment" spellcheck="true">#I'm disabling Wandb.</span>
<span class="token punctuation">)</span>

model <span class="token operator">=</span> AutoModelForCausalLM<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>
    model_name<span class="token punctuation">,</span>
    torch_dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>bfloat16<span class="token punctuation">,</span>
    device_map<span class="token operator">=</span>None
<span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span><span class="token string">"cuda"</span><span class="token punctuation">)</span>

tokenizer <span class="token operator">=</span> AutoTokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>model_name<span class="token punctuation">)</span>
tokenizer<span class="token punctuation">.</span>pad_token <span class="token operator">=</span> tokenizer<span class="token punctuation">.</span>eos_token

trainer <span class="token operator">=</span> GRPOTrainer<span class="token punctuation">(</span>
    model<span class="token operator">=</span>model<span class="token punctuation">,</span>
    processing_class<span class="token operator">=</span>tokenizer<span class="token punctuation">,</span>
    reward_funcs<span class="token operator">=</span><span class="token punctuation">[</span>
        xmlcount_reward_func<span class="token punctuation">,</span>
        soft_format_reward_func<span class="token punctuation">,</span>
        strict_format_reward_func<span class="token punctuation">,</span>
        int_reward_func<span class="token punctuation">,</span>
        correctness_reward_func<span class="token punctuation">]</span><span class="token punctuation">,</span>
    args<span class="token operator">=</span>training_args<span class="token punctuation">,</span>
    train_dataset<span class="token operator">=</span>dataset<span class="token punctuation">,</span>
    <span class="token comment" spellcheck="true">#peft_config=peft_config</span>
<span class="token punctuation">)</span>
trainer<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span>

trainer<span class="token punctuation">.</span>save_model<span class="token punctuation">(</span>output_dir<span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<ul>
<li>最后是训练主流程，这就和SFT差不多了，就是加载模型和数据集、配置微调参数、定义GRPO算法的训练器，就可以训练了。</li>
<li>所以整体代码就是多了奖励函数，还有为了方便判断结果要用提示词引导模型，毕竟核心的GRPO算法已经被集成到库里了，调用就行。</li>
</ul>
<h2 id="效果"><a href="#效果" class="headerlink" title="效果"></a>效果</h2><ul>
<li>由于训练不太可能全跑完，所以观察到了有改善就停了。</li>
</ul>
<pre class="line-numbers language-txt"><code class="language-txt">-------------------- Question:
Ahmed and Emily are having a contest to see who can get the best grade in the class. There have been 9 assignments and Ahmed has a 91 in the class. Emily has a 92. The final assignment is worth the same amount as all the other assignments. Emily got a 90 on the final assignment. What is the minimum grade Ahmed needs to get to beat Emily if all grades are whole numbers? 
Answer:
100 
Response:
To determine the minimum grade Ahmed needs to get to beat Emily, we first need to calculate the total grade the class will have for all assignments. The class has 9 assignments, and Emily has already achieved a 92 in all assignments. Therefore, the total grade for Emily is already 92.

Since Emily's final assignment is worth the same amount as all the other assignments, Emily will score another 90 on that assignment. Let's denote Ahmed's score on the final assignment as \(x\). The total grade for Emily will then be:
\[ x + 90 = 92 + 90 = 182 \]
The total grade for Ahmed will be:
\[ 91 + x \]
To beat Emily, Ahmed needs to score more than 182. Therefore, we need to find the smallest integer \(x\) such that:
\[ 91 + x > 182 \]
Subtracting 91 from 
Extracted:
To determine the minimum grade Ahmed needs to get to beat Emily, ……（这里提取函数提取不出答案就会把整个答案打出来，就省略了）
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<ul>
<li>可以看到刚开始模型基本不会按格式回答，回答长度也超限制了，答案也很容易错。这种情况会持续比较久，因为长时间没有正反馈，模型需要误打误撞才能得到正确的行为来获得奖励。</li>
</ul>
<pre class="line-numbers language-xml"><code class="language-xml">-------------------- Question:
A basketball team won 35 out of the 50 games that they have played. They still have 25 games left to play for the season. How many of the remaining games must they win so that their percentage of wins in the entire season is 64%? 
Answer:
13 
Response:
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>reasoning</span><span class="token punctuation">></span></span>
To calculate the number of games the team must win to reach a 64% win rate across all games in the season, we can follow these steps:

1. Let's assume the total number of games in the season is \( x \). We know that the team has played \( x - 25 \) games, and they have won 35 out of those games. Therefore, the team has won \( \frac{35}{x - 25} \) games.

2. The team wants to achieve a 64% win rate across all 50 games. We can set up the following equation to represent this:
   \[
   \frac{35}{x - 25} \times 100 = 64 \times x
   \]

3. Solving this equation will give us the value of \( x \). However, since the specific value of \( x \) is not 
Extracted:
&lt;reasoning
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<ul>
<li>接下来模型就出现了按格式答题的情况，虽然可能还会长度超限或者答错，但是一旦出现这种情况，就很容易按格式答题了。</li>
</ul>
<pre class="line-numbers language-xml"><code class="language-xml">-------------------- Question:
Dave bought 8 books about animals, 6 books about outer space, and 3 books about trains to keep him busy over the holidays. Each book cost $6. How much did Dave spend on the books? 
Answer:
102 
Response:
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>reasoning</span><span class="token punctuation">></span></span>
Dave bought a total of \(8 + 6 + 3 = 17\) books. Each book cost $6, so the total cost is \(17 \times 6 = \$102\).

<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>reasoning</span><span class="token punctuation">></span></span>
<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>answer</span><span class="token punctuation">></span></span>102<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>answer</span><span class="token punctuation">></span></span> 
Extracted:
102
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<ul>
<li>最后训练了几百题之后，模型就可以稳定地按格式回答问题，并且对于不复杂的题目基本都能做对了（Answer是数据集正确答案，Extracted是模型回答中提取的答案）。说明RLHF是有效果的，即便应对不怎么智能的小模型。</li>
</ul>
<h1 id="🔥总结"><a href="#🔥总结" class="headerlink" title="🔥总结"></a>🔥总结</h1><ul>
<li>本次通过LoRA和Prefix方法体验了SFT微调，通过GRPO算法体验了RLHF微调，算是初步尝试了最主流的PEFT微调方案及其方法。</li>
<li>通过该方案可以比较容易地上手微调，但是微调本身还是很吃算力的，即便是这种参数高效微调对个人电脑也相当慢，效果也不一定好，像传统深度学习训练一样，也需要很多调整，强化学习就更是需要更多的时间和算力了。所以只是用于尝试和体验微调过程，丰富相关经验，增强知识的理解。</li>
</ul>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
<div class="share-btn">
      <span class="share-sns share-outer">
        <i class="ri-share-forward-line"></i>
        分享
      </span>
      <div class="share-wrap">
        <i class="arrow"></i>
        <div class="share-icons">
          
          <a class="weibo share-sns" href="javascript:;" data-type="weibo">
            <i class="ri-weibo-fill"></i>
          </a>
          <a class="weixin share-sns wxFab" href="javascript:;" data-type="weixin">
            <i class="ri-wechat-fill"></i>
          </a>
          <a class="qq share-sns" href="javascript:;" data-type="qq">
            <i class="ri-qq-fill"></i>
          </a>
          <a class="douban share-sns" href="javascript:;" data-type="douban">
            <i class="ri-douban-line"></i>
          </a>
          <!-- <a class="qzone share-sns" href="javascript:;" data-type="qzone">
            <i class="icon icon-qzone"></i>
          </a> -->
          
          <a class="facebook share-sns" href="javascript:;" data-type="facebook">
            <i class="ri-facebook-circle-fill"></i>
          </a>
          <a class="twitter share-sns" href="javascript:;" data-type="twitter">
            <i class="ri-twitter-fill"></i>
          </a>
          <a class="google share-sns" href="javascript:;" data-type="google">
            <i class="ri-google-fill"></i>
          </a>
        </div>
      </div>
</div>

<div class="wx-share-modal">
    <a class="modal-close" href="javascript:;"><i class="ri-close-circle-line"></i></a>
    <p>扫一扫，分享到微信</p>
    <div class="wx-qrcode">
      <img src="//api.qrserver.com/v1/create-qr-code/?size=150x150&data=https://legendleochen.top/2025/02/06/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%EF%BC%9ASFT%EF%BC%88LoRA%E3%80%81Prefix%EF%BC%89%E5%92%8CRLHF/" alt="微信分享二维码">
    </div>
</div>

<div id="share-mask"></div>  
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/LLM/" rel="tag">LLM</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/LORA/" rel="tag">LORA</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/PEFT/" rel="tag">PEFT</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Python/" rel="tag">Python</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/RLHF/" rel="tag">RLHF</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/SFT/" rel="tag">SFT</a></li></ul>

    </footer>
  </div>

   
  <nav class="article-nav">
    
      <a href="/2025/02/17/%E4%B8%BB%E6%B5%81%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/" class="article-nav-link">
        <strong class="article-nav-caption">上一篇</strong>
        <div class="article-nav-title">
          
            主流大模型技术笔记
          
        </div>
      </a>
    
    
      <a href="/2025/01/04/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%8F%8ARAG%E6%9E%B6%E6%9E%84%E7%9A%84%E6%9C%AC%E5%9C%B0%E9%83%A8%E7%BD%B2/" class="article-nav-link">
        <strong class="article-nav-caption">下一篇</strong>
        <div class="article-nav-title">大模型及RAG架构的本地部署</div>
      </a>
    
  </nav>

  
   
  
    
</article>

</section>
      <footer class="footer">
  <div class="outer">
    <ul>
      <li>
        Copyrights &copy;
        2023-2025
        <i class="ri-heart-fill heart_icon"></i> LegendLeo Chen
      </li>
    </ul>
    <ul>
      <li>
        
      </li>
    </ul>
    <ul>
      <li>
        
        
        <span>
  <span><i class="ri-user-3-fill"></i>访问人数:<span id="busuanzi_value_site_uv"></span></span>
  <span class="division">|</span>
  <span><i class="ri-eye-fill"></i>浏览次数:<span id="busuanzi_value_page_pv"></span></span>
</span>
        
      </li>
    </ul>
    <ul>
      
    </ul>
    <ul>
      
    </ul>
    <ul>
      <li>
        <!-- cnzz统计 -->
        
        <script type="text/javascript" src='https://s9.cnzz.com/z_stat.php?id=1278069914&amp;web_id=1278069914'></script>
        
      </li>
    </ul>
  </div>
</footer>    
    </main>
    <div class="float_btns">
      <div class="totop" id="totop">
  <i class="ri-arrow-up-line"></i>
</div>

<div class="todark" id="todark">
  <i class="ri-moon-line"></i>
</div>

    </div>
    <aside class="sidebar on">
      <button class="navbar-toggle"></button>
<nav class="navbar">
  
  <div class="logo">
    <a href="/"><img src="/images/mylogo.png" alt="LegendLeo Chen 的空间"></a>
  </div>
  
  <ul class="nav nav-main">
    
    <li class="nav-item">
      <a class="nav-item-link" href="/">🚀主页</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/archives">💾归档</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/categories">🧭分类</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags">🏷️标签</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/about">🛸关于</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/analytics">📊统计</a>
    </li>
    
  </ul>
</nav>
<nav class="navbar navbar-bottom">
  <ul class="nav">
    <li class="nav-item">
      
      <a class="nav-item-link nav-item-search"  title="搜索">
        <i class="ri-search-line"></i>
      </a>
      
      
    </li>
  </ul>
</nav>
<div class="search-form-wrap">
  <div class="local-search local-search-plugin">
  <input type="search" id="local-search-input" class="local-search-input" placeholder="Search...">
  <div id="local-search-result" class="local-search-result"></div>
</div>
</div>
    </aside>
    <div id="mask"></div>

<!-- #reward -->
<div id="reward">
  <span class="close"><i class="ri-close-line"></i></span>
  <p class="reward-p"><i class="ri-cup-line"></i>请我喝杯咖啡吧~</p>
  <div class="reward-box">
    
    <div class="reward-item">
      <img class="reward-img" src="/images/alipay.jpg">
      <span class="reward-type">支付宝</span>
    </div>
    
    
    <div class="reward-item">
      <img class="reward-img" src="/images/wechat.jpg">
      <span class="reward-type">微信</span>
    </div>
    
  </div>
</div>
    
<script src="/js/jquery-3.6.0.min.js"></script>
 
<script src="/js/lazyload.min.js"></script>

<!-- Tocbot -->
 
<script src="/js/tocbot.min.js"></script>

<script>
  tocbot.init({
    tocSelector: ".tocbot",
    contentSelector: ".article-entry",
    headingSelector: "h1, h2, h3, h4, h5, h6",
    hasInnerContainers: true,
    scrollSmooth: true,
    scrollContainer: "main",
    positionFixedSelector: ".tocbot",
    positionFixedClass: "is-position-fixed",
    fixedSidebarOffset: "auto",
  });
</script>

<script src="https://cdn.staticfile.org/jquery-modal/0.9.2/jquery.modal.min.js"></script>
<link
  rel="stylesheet"
  href="https://cdn.staticfile.org/jquery-modal/0.9.2/jquery.modal.min.css"
/>
<script src="https://cdn.staticfile.org/justifiedGallery/3.8.1/js/jquery.justifiedGallery.min.js"></script>

<script src="/dist/main.js"></script>

<!-- ImageViewer -->
 <!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                <!--  Controls are self-explanatory. Order can be changed. -->

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" style="display:none" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>

<link rel="stylesheet" href="https://cdn.staticfile.org/photoswipe/4.1.3/photoswipe.min.css">
<link rel="stylesheet" href="https://cdn.staticfile.org/photoswipe/4.1.3/default-skin/default-skin.min.css">
<script src="https://cdn.staticfile.org/photoswipe/4.1.3/photoswipe.min.js"></script>
<script src="https://cdn.staticfile.org/photoswipe/4.1.3/photoswipe-ui-default.min.js"></script>

<script>
    function viewer_init() {
        let pswpElement = document.querySelectorAll('.pswp')[0];
        let $imgArr = document.querySelectorAll(('.article-entry img:not(.reward-img)'))

        $imgArr.forEach(($em, i) => {
            $em.onclick = () => {
                // slider展开状态
                // todo: 这样不好，后面改成状态
                if (document.querySelector('.left-col.show')) return
                let items = []
                $imgArr.forEach(($em2, i2) => {
                    let img = $em2.getAttribute('data-idx', i2)
                    let src = $em2.getAttribute('data-target') || $em2.getAttribute('src')
                    let title = $em2.getAttribute('alt')
                    // 获得原图尺寸
                    const image = new Image()
                    image.src = src
                    items.push({
                        src: src,
                        w: image.width || $em2.width,
                        h: image.height || $em2.height,
                        title: title
                    })
                })
                var gallery = new PhotoSwipe(pswpElement, PhotoSwipeUI_Default, items, {
                    index: parseInt(i)
                });
                gallery.init()
            }
        })
    }
    viewer_init()
</script> 
<!-- MathJax -->

<!-- Katex -->

<!-- busuanzi  -->
 
<script src="/js/busuanzi-2.3.pure.min.js"></script>
 
<!-- ClickLove -->

<!-- ClickBoom1 -->

<!-- ClickBoom2 -->

<!-- CodeCopy -->
 
<link rel="stylesheet" href="/css/clipboard.css">
 <script src="https://cdn.staticfile.org/clipboard.js/2.0.10/clipboard.min.js"></script>
<script>
  function wait(callback, seconds) {
    var timelag = null;
    timelag = window.setTimeout(callback, seconds);
  }
  !function (e, t, a) {
    var initCopyCode = function(){
      var copyHtml = '';
      copyHtml += '<button class="btn-copy" data-clipboard-snippet="">';
      copyHtml += '<i class="ri-file-copy-2-line"></i><span>COPY</span>';
      copyHtml += '</button>';
      $(".highlight .code pre").before(copyHtml);
      $(".article pre code").before(copyHtml);
      var clipboard = new ClipboardJS('.btn-copy', {
        target: function(trigger) {
          return trigger.nextElementSibling;
        }
      });
      clipboard.on('success', function(e) {
        let $btn = $(e.trigger);
        $btn.addClass('copied');
        let $icon = $($btn.find('i'));
        $icon.removeClass('ri-file-copy-2-line');
        $icon.addClass('ri-checkbox-circle-line');
        let $span = $($btn.find('span'));
        $span[0].innerText = 'COPIED';
        
        wait(function () { // 等待两秒钟后恢复
          $icon.removeClass('ri-checkbox-circle-line');
          $icon.addClass('ri-file-copy-2-line');
          $span[0].innerText = 'COPY';
        }, 2000);
      });
      clipboard.on('error', function(e) {
        e.clearSelection();
        let $btn = $(e.trigger);
        $btn.addClass('copy-failed');
        let $icon = $($btn.find('i'));
        $icon.removeClass('ri-file-copy-2-line');
        $icon.addClass('ri-time-line');
        let $span = $($btn.find('span'));
        $span[0].innerText = 'COPY FAILED';
        
        wait(function () { // 等待两秒钟后恢复
          $icon.removeClass('ri-time-line');
          $icon.addClass('ri-file-copy-2-line');
          $span[0].innerText = 'COPY';
        }, 2000);
      });
    }
    initCopyCode();
  }(window, document);
</script>
 
<!-- CanvasBackground -->

<script>
  if (window.mermaid) {
    mermaid.initialize({ theme: "forest" });
  }
</script>


    
    <div id="music">
    
    
    
    <iframe frameborder="no" border="1" marginwidth="0" marginheight="0" width="200" height="52"
        src="//music.163.com/outchain/player?type=2&id=1491212&auto=1&height=32"></iframe>
</div>

<style>
    #music {
        position: fixed;
        right: 15px;
        bottom: 0;
        z-index: 998;
    }
</style>
    
    

  </div>
  <!-- 背景气泡 -->
  <!--
  <div class="balls-container">
    <div class="balls-particles">
      <span style="--i:11;"></span>
      <span style="--i:12;"></span>
      <span style="--i:24;"></span>
      <span style="--i:10;"></span>
      <span style="--i:14;"></span>
      <span style="--i:23;"></span>
      <span style="--i:18;"></span>
      <span style="--i:16;"></span>
      <span style="--i:19;"></span>
      <span style="--i:20;"></span>
      <span style="--i:22;"></span>
      <span style="--i:25;"></span>
      <span style="--i:18;"></span>
      <span style="--i:21;"></span>
      <span style="--i:13;"></span>
      <span style="--i:15;"></span>
      <span style="--i:26;"></span>
      <span style="--i:17;"></span>
      <span style="--i:13;"></span>
      <span style="--i:26;"></span>
      <span style="--i:28;"></span>
      <span style="--i:11;"></span>
      <span style="--i:12;"></span>
      <span style="--i:24;"></span>
      <span style="--i:10;"></span>
      <span style="--i:14;"></span>
      <span style="--i:23;"></span>
      <span style="--i:18;"></span>
      <span style="--i:16;"></span>
      <span style="--i:19;"></span>
      <span style="--i:20;"></span>
      <span style="--i:22;"></span>
      <span style="--i:25;"></span>
      <span style="--i:18;"></span>
      <span style="--i:21;"></span>
      <span style="--i:13;"></span>
      <span style="--i:15;"></span>
      <span style="--i:26;"></span>
      <span style="--i:17;"></span>
      <span style="--i:13;"></span>
      <span style="--i:26;"></span>
      <span style="--i:28;"></span>
    </div>
  </div>
  <style>
    *
    {
      margin: 0;
      padding: 0;
      box-sizing: border-box;
    }
    
    .balls-container
    { 
      position: fixed;
      top: 0px;
      left: 0px;
      width: 100%;
      height: 100vh;
      overflow: hidden;
      opacity: 0.3;
    }
    
    .balls-particles
    {
      position: fixed;
      display: flex;
      z-index: 3;
      padding: 0 20px;
    }
    
    .balls-particles span
    {
      position: relative;
      bottom: 30px;
      width: 30px;
      height: 30px;
      background-color: #4fc3dc;
      box-shadow: 0 0 0 10px #4fc3dc44,
      0 0 50px #4fc3dc,
      -100px 0 #4fc3dc99,
      100px 0 #ff2d7599;
      margin: 0 4px;
      border-radius: 50%;
      animation: animate 15s ease infinite;
      animation-delay: calc(125s / var(--i));
      transform: translateY(120vh);
    }
    .balls-particles span:nth-child(even) {
      background-color: #ff2d75;
      box-shadow: 0 0 0 10px #ff267544,
      0 0 50px #ff2d75,
      -100px 0 #4fc3dc99,
      100px 0 #4fc3dc99;
      ;
    }
    
    @keyframes animate {
      0%
      {
        transform: translateY(120vh) scale(0) rotate(0deg);
      }
      20%
      {
        transform: translateY(100vh) scale(1) rotate(0deg);
      }
      100%
      {
        transform: translateY(-50vh) scale(0.5) rotate(360deg);
      }
    }
  </style> -->
  <!-- 地月系统 -->
  <!-- <div class="earth-container" >
    <div class="planet"></div>
    <div class="satellite"></div>
   </div>
   <style>
    *{
      padding: 0;
      margin: 0;
      }
      .earth-container {
        width: 36.25em;
        height: 36.25em;
        position: absolute;
        top:5%;
        left: 93%;
        transform: translate(-50%, -50%);
        opacity: 0.3;
      }
      
      .planet{
        width: 15.62*3em;
        height: 15.62*3em;
        background-color: #02c0f5;
        border-radius: 50%;
        position: absolute;
        margin: auto;
        top:0;
        right: 0;
        bottom: 0;
        left: 0;
        z-index: 1;
      }
      
      .planet::before{
        content: '';
        width: 4em;
        height: 4em;
        background-color: #008fd6;
        position: absolute;
        top:10em;
        left: 8em;
        border-radius: 50%; 
        box-shadow: 15em 15em 0 2em #00d68b, 5em 8em 0 3em #10ade1;
      }
      
      .satellite{
        width: 5em;
        height: 5em;
        background-color: #dee517;
        border-radius: 50%;
        position: relative;
        left: -5em;
        bottom: -30em;
        animation: spin 5s infinite;
        z-index: 1;
      }
      
      @keyframes spin {
        49%{
          z-index: 1;
        }
        50%{
          bottom: 3em;
          left: 35em;
          z-index: -1;
        }
        100%{
          z-index: -1;
        }
      }
    </style> -->
<!-- 三角彩带背景 -->
  <canvas id="evanyou-canvas" style="opacity: 0.3; position: fixed; top: 0px; left: 0px; z-index: -1; width: 100%; height: 100%; pointer-events: none;"></canvas>
  <script src="https://cdn.jsdelivr.net/gh/XXXZhy/Blog_Image/js/evanyou_canvas.js"></script>
</body>

</html>