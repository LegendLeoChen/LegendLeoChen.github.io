<!DOCTYPE html>


<html lang="zh-CN">
  

    <head>
      <meta charset="utf-8" />
        
      <meta name="description" content="一个秘密空间" />
      
      <meta
        name="viewport"
        content="width=device-width, initial-scale=1, maximum-scale=1"
      />
      <title>SAM2架构概览和详细解读 |  LegendLeo Chen 的空间</title>
  <meta name="generator" content="hexo-theme-ayer">
      
      <link rel="shortcut icon" href="/mylogo.ico" />
       
<link rel="stylesheet" href="/dist/main.css">

      
<link rel="stylesheet" href="/css/fonts/remixicon.css">

      
<link rel="stylesheet" href="/css/custom.css">
 
      <script src="https://cdn.staticfile.org/pace/1.2.4/pace.min.js"></script>
       
 

      <link
        rel="stylesheet"
        href="https://cdn.jsdelivr.net/npm/@sweetalert2/theme-bulma@5.0.1/bulma.min.css"
      />
      <script src="https://cdn.jsdelivr.net/npm/sweetalert2@11.0.19/dist/sweetalert2.min.js"></script>

      <!-- mermaid -->
      
      <style>
        .swal2-styled.swal2-confirm {
          font-size: 1.6rem;
        }
      </style>
<!-- 封面标闪烁 -->
<link rel="stylesheet" href="/css/zhyBlogTitle.css">
<script src="https://cdn.bootcdn.net/ajax/libs/highlight.js/9.18.1/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad();</script>
<!-- jquery，懒加载、统计、说说需要的jquery -->
<script src="https://cdn.bootcdn.net/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css">
<link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"></head>
  </html>
</html>


<body>
  <div id="app">
    
      
    <main class="content on">
      <section class="outer">
  <article
  id="post-SAM2架构概览和详细解读"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h1 class="article-title sea-center" style="border-left:0" itemprop="name">
  SAM2架构概览和详细解读
</h1>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2025/06/12/SAM2%E6%9E%B6%E6%9E%84%E6%A6%82%E8%A7%88%E5%92%8C%E8%AF%A6%E7%BB%86%E8%A7%A3%E8%AF%BB/" class="article-date">
  <time datetime="2025-06-12T11:14:49.000Z" itemprop="datePublished">2025-06-12</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a> / <a class="article-category-link" href="/categories/%E7%BC%96%E7%A8%8B/">编程</a>
  </div>
  
<div class="word_count">
    <span class="post-time">
        <span class="post-meta-item-icon">
            <i class="ri-quill-pen-line"></i>
            <span class="post-meta-item-text"> 字数统计:</span>
            <span class="post-count">9.2k</span>
        </span>
    </span>

    <span class="post-time">
        &nbsp; | &nbsp;
        <span class="post-meta-item-icon">
            <i class="ri-book-open-line"></i>
            <span class="post-meta-item-text"> 阅读时长≈</span>
            <span class="post-count">44 分钟</span>
        </span>
    </span>
</div>
 
    </div>
      
    <div class="tocbot"></div>




  
    <div class="article-entry" itemprop="articleBody">
       
  <p>前文<a href="/2025/05/07/SAM2%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%BE%AE%E8%B0%83/" title="SAM2图像分割模型的微调">SAM2图像分割模型的微调</a>记录了如何使用SAM2在下游数据集进行微调、预测、评估等。</p>
<p>本文将通过对SAM2源码的阅读和debug，进一步了解SAM2的架构和流程。</p>
<p>本文目前只考虑图片任务，不考虑视频任务涉及的模块，所以SAM2架构上就是由image encoder、mask decoder、prompt encoder三个模块构成。<span id="more"></span><br><img src="/2025/06/12/SAM2%E6%9E%B6%E6%9E%84%E6%A6%82%E8%A7%88%E5%92%8C%E8%AF%A6%E7%BB%86%E8%A7%A3%E8%AF%BB/SAM2%E6%9E%B6%E6%9E%84.png" alt="SAM2架构"></p>
<h1 id="🔥项目结构"><a href="#🔥项目结构" class="headerlink" title="🔥项目结构"></a>🔥项目结构</h1><blockquote>
<p>assets：数据集文件夹；<br>checkpoints：下载官方提供的权重；<br>notebooks：使用jupyter进行预测、全景分割，用于上手体验；<br><strong>sam2：SAM2本体</strong>；<br>sam2_config：SAM2的架构配置，模型参数设置；<br>sav_dataset：官方的1B大小数据集相关；<br>tools：视频预测用的工具；</p>
</blockquote>
<p>所以实际上<strong>sam2</strong>文件夹当中就包含了所有重要的内容。</p>
<h1 id="🔥流程"><a href="#🔥流程" class="headerlink" title="🔥流程"></a>🔥流程</h1><p>根据<strong>训练的过程</strong>来解读SAM2是如何进行数据处理、前向传播等操作的。如果只想理解训练过程（验证和预测也差不多）并了解数据是如何在SAM2传播最终输出分割结果的，可以通过这节了解。</p>
<p><img src="/2025/06/12/SAM2%E6%9E%B6%E6%9E%84%E6%A6%82%E8%A7%88%E5%92%8C%E8%AF%A6%E7%BB%86%E8%A7%A3%E8%AF%BB/SAM2%E6%9E%B6%E6%9E%84%E7%AE%80%E5%9B%BE.png" alt="SAM2架构简图"></p>
<p>实际上去掉了SAM2新增的用于视频任务的内存机制，在图像任务中其结构可以用1代的来表示（上图）。</p>
<h2 id="🚠构建模型"><a href="#🚠构建模型" class="headerlink" title="🚠构建模型"></a>🚠构建模型</h2><pre class="line-numbers language-python"><code class="language-python">sam2_model <span class="token operator">=</span> build_sam2<span class="token punctuation">(</span>model_cfg<span class="token punctuation">,</span> sam2_checkpoint<span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">)</span>
predictor <span class="token operator">=</span> SAM2ImagePredictor<span class="token punctuation">(</span>sam2_model<span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>
<p>首先，无论是预测还是训练都有以上两行构建SAM2。</p>
<h3 id="加载权重：build-sam-py"><a href="#加载权重：build-sam-py" class="headerlink" title="加载权重：build_sam.py"></a>加载权重：build_sam.py</h3><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">build_sam2</span><span class="token punctuation">(</span>
    config_file<span class="token punctuation">,</span>
    ckpt_path<span class="token operator">=</span>None<span class="token punctuation">,</span>
    device<span class="token operator">=</span><span class="token string">"cuda"</span><span class="token punctuation">,</span>
    mode<span class="token operator">=</span><span class="token string">"eval"</span><span class="token punctuation">,</span>
    hydra_overrides_extra<span class="token operator">=</span><span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    apply_postprocessing<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
<span class="token punctuation">)</span><span class="token punctuation">:</span>

    <span class="token keyword">if</span> apply_postprocessing<span class="token punctuation">:</span>
        hydra_overrides_extra <span class="token operator">=</span> hydra_overrides_extra<span class="token punctuation">.</span>copy<span class="token punctuation">(</span><span class="token punctuation">)</span>
        hydra_overrides_extra <span class="token operator">+=</span> <span class="token punctuation">[</span>
            <span class="token comment" spellcheck="true"># dynamically fall back to multi-mask if the single mask is not stable</span>
            <span class="token string">"++model.sam_mask_decoder_extra_args.dynamic_multimask_via_stability=true"</span><span class="token punctuation">,</span>
            <span class="token string">"++model.sam_mask_decoder_extra_args.dynamic_multimask_stability_delta=0.05"</span><span class="token punctuation">,</span>
            <span class="token string">"++model.sam_mask_decoder_extra_args.dynamic_multimask_stability_thresh=0.98"</span><span class="token punctuation">,</span>
        <span class="token punctuation">]</span>
    <span class="token comment" spellcheck="true"># Read config and init model</span>
    cfg <span class="token operator">=</span> compose<span class="token punctuation">(</span>config_name<span class="token operator">=</span>config_file<span class="token punctuation">,</span> overrides<span class="token operator">=</span>hydra_overrides_extra<span class="token punctuation">)</span>
    OmegaConf<span class="token punctuation">.</span>resolve<span class="token punctuation">(</span>cfg<span class="token punctuation">)</span>
    model <span class="token operator">=</span> instantiate<span class="token punctuation">(</span>cfg<span class="token punctuation">.</span>model<span class="token punctuation">,</span> _recursive_<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
    _load_checkpoint<span class="token punctuation">(</span>model<span class="token punctuation">,</span> ckpt_path<span class="token punctuation">)</span>
    model <span class="token operator">=</span> model<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
    <span class="token keyword">if</span> mode <span class="token operator">==</span> <span class="token string">"eval"</span><span class="token punctuation">:</span>
        model<span class="token punctuation">.</span>eval<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> model
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p><code>build_sam2()</code>这个函数从config.yaml里面加载对应规模模型参数，构建模型。</p>
<h3 id="SAM2核心：sam2-image-predictor-py"><a href="#SAM2核心：sam2-image-predictor-py" class="headerlink" title="SAM2核心：sam2_image_predictor.py"></a>SAM2核心：sam2_image_predictor.py</h3><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">SAM2ImagePredictor</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>
        self<span class="token punctuation">,</span>
        sam_model<span class="token punctuation">:</span> SAM2Base<span class="token punctuation">,</span>
        mask_threshold<span class="token operator">=</span><span class="token number">0.0</span><span class="token punctuation">,</span>
        max_hole_area<span class="token operator">=</span><span class="token number">0.0</span><span class="token punctuation">,</span>
        max_sprinkle_area<span class="token operator">=</span><span class="token number">0.0</span><span class="token punctuation">,</span>
    <span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> None<span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""
        Uses SAM-2 to calculate the image embedding for an image, and then
        allow repeated, efficient mask prediction given prompts.

        Arguments:
          sam_model (Sam-2): The model to use for mask prediction.
          mask_threshold (float): The threshold to use when converting mask logits
            to binary masks. Masks are thresholded at 0 by default.
          fill_hole_area (int): If fill_hole_area > 0, we fill small holes in up to
            the maximum area of fill_hole_area in low_res_masks.
        """</span>
        super<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>model <span class="token operator">=</span> sam_model
        self<span class="token punctuation">.</span>_transforms <span class="token operator">=</span> SAM2Transforms<span class="token punctuation">(</span>
            resolution<span class="token operator">=</span>self<span class="token punctuation">.</span>model<span class="token punctuation">.</span>image_size<span class="token punctuation">,</span>
            mask_threshold<span class="token operator">=</span>mask_threshold<span class="token punctuation">,</span>
            max_hole_area<span class="token operator">=</span>max_hole_area<span class="token punctuation">,</span>
            max_sprinkle_area<span class="token operator">=</span>max_sprinkle_area<span class="token punctuation">,</span>
        <span class="token punctuation">)</span>

        <span class="token comment" spellcheck="true"># Predictor state</span>
        self<span class="token punctuation">.</span>_is_image_set <span class="token operator">=</span> <span class="token boolean">False</span>
        self<span class="token punctuation">.</span>_features <span class="token operator">=</span> None
        self<span class="token punctuation">.</span>_orig_hw <span class="token operator">=</span> None
        <span class="token comment" spellcheck="true"># Whether the predictor is set for single image or a batch of images</span>
        self<span class="token punctuation">.</span>_is_batch <span class="token operator">=</span> <span class="token boolean">False</span>

        <span class="token comment" spellcheck="true"># Predictor config</span>
        self<span class="token punctuation">.</span>mask_threshold <span class="token operator">=</span> mask_threshold

        <span class="token comment" spellcheck="true"># Spatial dim for backbone feature maps</span>
        self<span class="token punctuation">.</span>_bb_feat_sizes <span class="token operator">=</span> <span class="token punctuation">[</span>
            <span class="token punctuation">(</span><span class="token number">256</span><span class="token punctuation">,</span> <span class="token number">256</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            <span class="token punctuation">(</span><span class="token number">128</span><span class="token punctuation">,</span> <span class="token number">128</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            <span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        <span class="token punctuation">]</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p><code>SAM2ImagePredictor</code>包含了模型的提示处理、图像编码、预测，使得我们用它就可以进行预测所需的大部分操作。它的初始化其实就是在模型之外加了个<code>SAM2Transforms</code>，它包含对数据的预处理（resize、归一化等）和输出masks的后处理。</p>
<p><code>SAM2ImagePredictor</code>还提供了<code>predict</code>、<code>_predict</code>、<code>predict_batch</code>几个函数，用于预测，内容和训练的过程差不多，只不过没有梯度。</p>
<h2 id="🚠图像编码"><a href="#🚠图像编码" class="headerlink" title="🚠图像编码"></a>🚠图像编码</h2><p>前向传播开始，首先对图像进行编码：</p>
<pre class="line-numbers language-python"><code class="language-python">predictor<span class="token punctuation">.</span>set_image_batch<span class="token punctuation">(</span>image_list<span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<pre class="line-numbers language-python"><code class="language-python">    <span class="token keyword">def</span> <span class="token function">set_image_batch</span><span class="token punctuation">(</span>
        self<span class="token punctuation">,</span>
        image_list<span class="token punctuation">:</span> Union<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">,</span> List<span class="token punctuation">[</span>Union<span class="token punctuation">[</span>np<span class="token punctuation">.</span>ndarray<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    <span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> None<span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""
        Calculates the image embeddings for the provided image batch, allowing
        masks to be predicted with the 'predict_batch' method.

        Arguments:
          image_list (List[np.ndarray]): The input images to embed in RGB format. The image should be in HWC format if np.ndarray
          with pixel values in [0, 255].
        """</span>
        self<span class="token punctuation">.</span>reset_predictor<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token comment" spellcheck="true"># assert isinstance(image_list, list)</span>
        self<span class="token punctuation">.</span>_orig_hw <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        <span class="token keyword">if</span> isinstance<span class="token punctuation">(</span>image_list<span class="token punctuation">,</span> list<span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token keyword">for</span> image <span class="token keyword">in</span> image_list<span class="token punctuation">:</span>
                <span class="token keyword">assert</span> isinstance<span class="token punctuation">(</span>
                    image<span class="token punctuation">,</span> np<span class="token punctuation">.</span>ndarray
                <span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">"Images are expected to be an np.ndarray in RGB format, and of shape  HWC"</span>
                self<span class="token punctuation">.</span>_orig_hw<span class="token punctuation">.</span>append<span class="token punctuation">(</span>image<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
            <span class="token comment" spellcheck="true"># Transform the image to the form expected by the model</span>
            img_batch <span class="token operator">=</span> self<span class="token punctuation">.</span>_transforms<span class="token punctuation">.</span>forward_batch<span class="token punctuation">(</span>image_list<span class="token punctuation">)</span>
            img_batch <span class="token operator">=</span> img_batch<span class="token punctuation">.</span>to<span class="token punctuation">(</span>self<span class="token punctuation">.</span>device<span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>                   <span class="token comment" spellcheck="true"># 是torch张量</span>
            <span class="token keyword">for</span> image <span class="token keyword">in</span> torch<span class="token punctuation">.</span>unbind<span class="token punctuation">(</span>image_list<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
                self<span class="token punctuation">.</span>_orig_hw<span class="token punctuation">.</span>append<span class="token punctuation">(</span>image<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
            img_batch <span class="token operator">=</span> image_list<span class="token punctuation">.</span>to<span class="token punctuation">(</span>self<span class="token punctuation">.</span>device<span class="token punctuation">)</span>
        batch_size <span class="token operator">=</span> img_batch<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
        <span class="token keyword">assert</span> <span class="token punctuation">(</span>
            len<span class="token punctuation">(</span>img_batch<span class="token punctuation">.</span>shape<span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">4</span> <span class="token operator">and</span> img_batch<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">==</span> <span class="token number">3</span>
        <span class="token punctuation">)</span><span class="token punctuation">,</span> f<span class="token string">"img_batch must be of size Bx3xHxW, got {img_batch.shape}"</span>
        logging<span class="token punctuation">.</span>info<span class="token punctuation">(</span><span class="token string">"Computing image embeddings for the provided images..."</span><span class="token punctuation">)</span>
        backbone_out <span class="token operator">=</span> self<span class="token punctuation">.</span>model<span class="token punctuation">.</span>forward_image<span class="token punctuation">(</span>img_batch<span class="token punctuation">)</span>
        _<span class="token punctuation">,</span> vision_feats<span class="token punctuation">,</span> _<span class="token punctuation">,</span> _ <span class="token operator">=</span> self<span class="token punctuation">.</span>model<span class="token punctuation">.</span>_prepare_backbone_features<span class="token punctuation">(</span>backbone_out<span class="token punctuation">)</span>
        <span class="token comment" spellcheck="true"># Add no_mem_embed, which is added to the lowest rest feat. map during training on videos</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>model<span class="token punctuation">.</span>directly_add_no_mem_embed<span class="token punctuation">:</span>
            vision_feats<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">=</span> vision_feats<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">+</span> self<span class="token punctuation">.</span>model<span class="token punctuation">.</span>no_mem_embed

        feats <span class="token operator">=</span> <span class="token punctuation">[</span>
            feat<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">*</span>feat_size<span class="token punctuation">)</span>
            <span class="token keyword">for</span> feat<span class="token punctuation">,</span> feat_size <span class="token keyword">in</span> zip<span class="token punctuation">(</span>vision_feats<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>_bb_feat_sizes<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        <span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span>
        self<span class="token punctuation">.</span>_features <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token string">"image_embed"</span><span class="token punctuation">:</span> feats<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token string">"high_res_feats"</span><span class="token punctuation">:</span> feats<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">}</span>
        self<span class="token punctuation">.</span>_is_image_set <span class="token operator">=</span> <span class="token boolean">True</span>
        self<span class="token punctuation">.</span>_is_batch <span class="token operator">=</span> <span class="token boolean">True</span>
        logging<span class="token punctuation">.</span>info<span class="token punctuation">(</span><span class="token string">"Image embeddings computed."</span><span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p><code>SAM2ImagePredictor</code>的<code>set_image()</code>和<code>set_image_batch()</code>两个函数用于把输入的图像经过<code>SAM2Transforms</code>处理后，输入到image encoder当中进行编码，经过简单的处理后存在<code>self._features</code>当中，<code>image_embed</code>和<code>high_res_feats</code>分别是金字塔结构输出的最后一层和前面若干层的特征，前面的层的特征会有更高的分辨率，<strong>有助于输出的掩码还原分辨率</strong>。</p>
<h2 id="🚠提示编码"><a href="#🚠提示编码" class="headerlink" title="🚠提示编码"></a>🚠提示编码</h2><pre class="line-numbers language-python"><code class="language-python">    <span class="token keyword">if</span> use_prompt<span class="token punctuation">:</span>
        mask_input<span class="token punctuation">,</span> unnorm_coords<span class="token punctuation">,</span> labels<span class="token punctuation">,</span> unnorm_box <span class="token operator">=</span> predictor<span class="token punctuation">.</span>_prep_prompts<span class="token punctuation">(</span>input_point<span class="token punctuation">,</span> input_label<span class="token punctuation">,</span> box<span class="token operator">=</span>None<span class="token punctuation">,</span> mask_logits<span class="token operator">=</span>None<span class="token punctuation">,</span> normalize_coords<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>      <span class="token comment" spellcheck="true"># 有提示</span>
        sparse_embeddings<span class="token punctuation">,</span> dense_embeddings <span class="token operator">=</span> predictor<span class="token punctuation">.</span>model<span class="token punctuation">.</span>sam_prompt_encoder<span class="token punctuation">(</span>points<span class="token operator">=</span><span class="token punctuation">(</span>unnorm_coords<span class="token punctuation">,</span> labels<span class="token punctuation">)</span><span class="token punctuation">,</span> boxes<span class="token operator">=</span>None<span class="token punctuation">,</span> masks<span class="token operator">=</span>None<span class="token punctuation">)</span>
    <span class="token keyword">else</span><span class="token punctuation">:</span>
        sparse_embeddings<span class="token punctuation">,</span> dense_embeddings <span class="token operator">=</span> predictor<span class="token punctuation">.</span>model<span class="token punctuation">.</span>sam_prompt_encoder<span class="token punctuation">(</span>points<span class="token operator">=</span>None<span class="token punctuation">,</span> boxes<span class="token operator">=</span>None<span class="token punctuation">,</span> masks<span class="token operator">=</span>None<span class="token punctuation">)</span>       <span class="token comment" spellcheck="true"># 无提示</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>以上是对提示（点及其label、框、掩码等）进行编码的示例。</p>
<ul>
<li><p><code>SAM2ImagePredictor._prep_prompts()</code>：用来预处理提示，把列表这些格式转为张量；</p>
</li>
<li><p><code>predictor.model.sam_prompt_encoder</code>：则是直接找到prompt encoder进行前向传播，如下，输出的<code>sparse_embeddings</code>由点和框这种稀疏提示嵌入得来，<code>dense_embeddings</code>则由掩码这种密集提示嵌入得来。</p>
</li>
</ul>
<pre class="line-numbers language-python"><code class="language-python">        bs <span class="token operator">=</span> self<span class="token punctuation">.</span>_get_batch_size<span class="token punctuation">(</span>points<span class="token punctuation">,</span> boxes<span class="token punctuation">,</span> masks<span class="token punctuation">)</span>
        sparse_embeddings <span class="token operator">=</span> torch<span class="token punctuation">.</span>empty<span class="token punctuation">(</span>
            <span class="token punctuation">(</span>bs<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>embed_dim<span class="token punctuation">)</span><span class="token punctuation">,</span> device<span class="token operator">=</span>self<span class="token punctuation">.</span>_get_device<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token punctuation">)</span>
        <span class="token keyword">if</span> points <span class="token keyword">is</span> <span class="token operator">not</span> None<span class="token punctuation">:</span>
            coords<span class="token punctuation">,</span> labels <span class="token operator">=</span> points
            point_embeddings <span class="token operator">=</span> self<span class="token punctuation">.</span>_embed_points<span class="token punctuation">(</span>coords<span class="token punctuation">,</span> labels<span class="token punctuation">,</span> pad<span class="token operator">=</span><span class="token punctuation">(</span>boxes <span class="token keyword">is</span> None<span class="token punctuation">)</span><span class="token punctuation">)</span>
            sparse_embeddings <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>sparse_embeddings<span class="token punctuation">,</span> point_embeddings<span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
        <span class="token keyword">if</span> boxes <span class="token keyword">is</span> <span class="token operator">not</span> None<span class="token punctuation">:</span>
            box_embeddings <span class="token operator">=</span> self<span class="token punctuation">.</span>_embed_boxes<span class="token punctuation">(</span>boxes<span class="token punctuation">)</span>
            sparse_embeddings <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>sparse_embeddings<span class="token punctuation">,</span> box_embeddings<span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>

        <span class="token keyword">if</span> masks <span class="token keyword">is</span> <span class="token operator">not</span> None<span class="token punctuation">:</span>
            dense_embeddings <span class="token operator">=</span> self<span class="token punctuation">.</span>_embed_masks<span class="token punctuation">(</span>masks<span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            dense_embeddings <span class="token operator">=</span> self<span class="token punctuation">.</span>no_mask_embed<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>expand<span class="token punctuation">(</span>
                bs<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>image_embedding_size<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>image_embedding_size<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>
            <span class="token punctuation">)</span>

        <span class="token keyword">return</span> sparse_embeddings<span class="token punctuation">,</span> dense_embeddings
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h2 id="🚠掩码解码"><a href="#🚠掩码解码" class="headerlink" title="🚠掩码解码"></a>🚠掩码解码</h2><p>接下来就是将提示和图像的编码通过mask decoder进行解码输出掩码：</p>
<pre class="line-numbers language-python"><code class="language-python">    high_res_features <span class="token operator">=</span> <span class="token punctuation">[</span>feat_level<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span> <span class="token keyword">for</span> feat_level <span class="token keyword">in</span> predictor<span class="token punctuation">.</span>_features<span class="token punctuation">[</span><span class="token string">"high_res_feats"</span><span class="token punctuation">]</span><span class="token punctuation">]</span>
    low_res_masks<span class="token punctuation">,</span> prd_scores<span class="token punctuation">,</span> _<span class="token punctuation">,</span> _ <span class="token operator">=</span> predictor<span class="token punctuation">.</span>model<span class="token punctuation">.</span>sam_mask_decoder<span class="token punctuation">(</span>
        image_embeddings<span class="token operator">=</span>predictor<span class="token punctuation">.</span>_features<span class="token punctuation">[</span><span class="token string">"image_embed"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        image_pe<span class="token operator">=</span>predictor<span class="token punctuation">.</span>model<span class="token punctuation">.</span>sam_prompt_encoder<span class="token punctuation">.</span>get_dense_pe<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        sparse_prompt_embeddings<span class="token operator">=</span>sparse_embeddings<span class="token punctuation">,</span>
        dense_prompt_embeddings<span class="token operator">=</span>dense_embeddings<span class="token punctuation">,</span>
        multimask_output<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>
        repeat_image<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>
        high_res_features<span class="token operator">=</span>high_res_features
    <span class="token punctuation">)</span>
    prd_masks <span class="token operator">=</span> predictor<span class="token punctuation">.</span>_transforms<span class="token punctuation">.</span>postprocess_masks<span class="token punctuation">(</span>low_res_masks<span class="token punctuation">,</span> predictor<span class="token punctuation">.</span>_orig_hw<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h3 id="解码：mask-decoder-py"><a href="#解码：mask-decoder-py" class="headerlink" title="解码：mask decoder.py"></a>解码：mask decoder.py</h3><p><code>sam_mask_decoder</code>是找到mask decoder进行前向传播，输入包含图像编码<code>image_embeddings</code>、图像位置编码<code>image_pe</code>、提示的稀疏编码<code>sparse_prompt_embeddings</code>、提示的密集编码<code>dense_prompt_embeddings</code>、是否预测多掩码<code>multimask_output</code>、图像高分辨率特征<code>high_res_features</code>；</p>
<pre class="line-numbers language-python"><code class="language-python">        masks<span class="token punctuation">,</span> iou_pred<span class="token punctuation">,</span> mask_tokens_out<span class="token punctuation">,</span> object_score_logits <span class="token operator">=</span> self<span class="token punctuation">.</span>predict_masks<span class="token punctuation">(</span>
            image_embeddings<span class="token operator">=</span>image_embeddings<span class="token punctuation">,</span>
            image_pe<span class="token operator">=</span>image_pe<span class="token punctuation">,</span>
            sparse_prompt_embeddings<span class="token operator">=</span>sparse_prompt_embeddings<span class="token punctuation">,</span>
            dense_prompt_embeddings<span class="token operator">=</span>dense_prompt_embeddings<span class="token punctuation">,</span>
            repeat_image<span class="token operator">=</span>repeat_image<span class="token punctuation">,</span>
            high_res_features<span class="token operator">=</span>high_res_features<span class="token punctuation">,</span>
        <span class="token punctuation">)</span>

        <span class="token comment" spellcheck="true"># Select the correct mask or masks for output</span>
        <span class="token keyword">if</span> multimask_output<span class="token punctuation">:</span>
            masks <span class="token operator">=</span> masks<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span>
            iou_pred <span class="token operator">=</span> iou_pred<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">]</span>
        <span class="token keyword">elif</span> self<span class="token punctuation">.</span>dynamic_multimask_via_stability <span class="token operator">and</span> <span class="token operator">not</span> self<span class="token punctuation">.</span>training<span class="token punctuation">:</span>
            masks<span class="token punctuation">,</span> iou_pred <span class="token operator">=</span> self<span class="token punctuation">.</span>_dynamic_multimask_via_stability<span class="token punctuation">(</span>masks<span class="token punctuation">,</span> iou_pred<span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            masks <span class="token operator">=</span> masks<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">:</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span>
            iou_pred <span class="token operator">=</span> iou_pred<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">:</span><span class="token number">1</span><span class="token punctuation">]</span>

        <span class="token keyword">if</span> multimask_output <span class="token operator">and</span> self<span class="token punctuation">.</span>use_multimask_token_for_obj_ptr<span class="token punctuation">:</span>
            sam_tokens_out <span class="token operator">=</span> mask_tokens_out<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">]</span>  <span class="token comment" spellcheck="true"># [b, 3, c] shape</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            <span class="token comment" spellcheck="true"># Take the mask output token. Here we *always* use the token for single mask output.</span>
            <span class="token comment" spellcheck="true"># At test time, even if we track after 1-click (and using multimask_output=True),</span>
            <span class="token comment" spellcheck="true"># we still take the single mask token here. The rationale is that we always track</span>
            <span class="token comment" spellcheck="true"># after multiple clicks during training, so the past tokens seen during training</span>
            <span class="token comment" spellcheck="true"># are always the single mask token (and we'll let it be the object-memory token).</span>
            sam_tokens_out <span class="token operator">=</span> mask_tokens_out<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">:</span><span class="token number">1</span><span class="token punctuation">]</span>  <span class="token comment" spellcheck="true"># [b, 1, c] shape</span>

        <span class="token comment" spellcheck="true"># Prepare output</span>
        <span class="token keyword">return</span> masks<span class="token punctuation">,</span> iou_pred<span class="token punctuation">,</span> sam_tokens_out<span class="token punctuation">,</span> object_score_logits
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>重点显然在<code>predict_masks()</code>函数：</p>
<pre class="line-numbers language-python"><code class="language-python">    <span class="token keyword">def</span> <span class="token function">predict_masks</span><span class="token punctuation">(</span>
        self<span class="token punctuation">,</span>
        image_embeddings<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">,</span>
        image_pe<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">,</span>
        sparse_prompt_embeddings<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">,</span>
        dense_prompt_embeddings<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">,</span>
        repeat_image<span class="token punctuation">:</span> bool<span class="token punctuation">,</span>
        high_res_features<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>List<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> None<span class="token punctuation">,</span>
    <span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> Tuple<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">,</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">]</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""Predicts masks. See 'forward' for more details."""</span>
        <span class="token comment" spellcheck="true"># Concatenate output tokens</span>
        s <span class="token operator">=</span> <span class="token number">0</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>pred_obj_scores<span class="token punctuation">:</span>
            output_tokens <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span>
                <span class="token punctuation">[</span>
                    self<span class="token punctuation">.</span>obj_score_token<span class="token punctuation">.</span>weight<span class="token punctuation">,</span>
                    self<span class="token punctuation">.</span>iou_token<span class="token punctuation">.</span>weight<span class="token punctuation">,</span>
                    self<span class="token punctuation">.</span>mask_tokens<span class="token punctuation">.</span>weight<span class="token punctuation">,</span>
                <span class="token punctuation">]</span><span class="token punctuation">,</span>
                dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span>
            <span class="token punctuation">)</span>
            s <span class="token operator">=</span> <span class="token number">1</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            output_tokens <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span>
                <span class="token punctuation">[</span>self<span class="token punctuation">.</span>iou_token<span class="token punctuation">.</span>weight<span class="token punctuation">,</span> self<span class="token punctuation">.</span>mask_tokens<span class="token punctuation">.</span>weight<span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span>
            <span class="token punctuation">)</span>
        output_tokens <span class="token operator">=</span> output_tokens<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>expand<span class="token punctuation">(</span>
            sparse_prompt_embeddings<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span>
        <span class="token punctuation">)</span>
        tokens <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">(</span>output_tokens<span class="token punctuation">,</span> sparse_prompt_embeddings<span class="token punctuation">)</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>

        <span class="token comment" spellcheck="true"># Expand per-image data in batch direction to be per-mask</span>
        <span class="token keyword">if</span> repeat_image<span class="token punctuation">:</span>
            src <span class="token operator">=</span> torch<span class="token punctuation">.</span>repeat_interleave<span class="token punctuation">(</span>image_embeddings<span class="token punctuation">,</span> tokens<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            <span class="token comment" spellcheck="true"># assert image_embeddings.shape[0] == tokens.shape[0]</span>
            src <span class="token operator">=</span> image_embeddings
        src <span class="token operator">=</span> src <span class="token operator">+</span> dense_prompt_embeddings
        <span class="token keyword">assert</span> <span class="token punctuation">(</span>
            image_pe<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">1</span>
        <span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">"image_pe should have size 1 in batch dim (from `get_dense_pe()`)"</span>
        pos_src <span class="token operator">=</span> torch<span class="token punctuation">.</span>repeat_interleave<span class="token punctuation">(</span>image_pe<span class="token punctuation">,</span> tokens<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
        b<span class="token punctuation">,</span> c<span class="token punctuation">,</span> h<span class="token punctuation">,</span> w <span class="token operator">=</span> src<span class="token punctuation">.</span>shape

        <span class="token comment" spellcheck="true"># Run the transformer</span>
        hs<span class="token punctuation">,</span> src <span class="token operator">=</span> self<span class="token punctuation">.</span>transformer<span class="token punctuation">(</span>src<span class="token punctuation">,</span> pos_src<span class="token punctuation">,</span> tokens<span class="token punctuation">)</span>
        iou_token_out <span class="token operator">=</span> hs<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> s<span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span>
        mask_tokens_out <span class="token operator">=</span> hs<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> s <span class="token operator">+</span> <span class="token number">1</span> <span class="token punctuation">:</span> <span class="token punctuation">(</span>s <span class="token operator">+</span> <span class="token number">1</span> <span class="token operator">+</span> self<span class="token punctuation">.</span>num_mask_tokens<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span>

        <span class="token comment" spellcheck="true"># Upscale mask embeddings and predict masks using the mask tokens</span>
        src <span class="token operator">=</span> src<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span>b<span class="token punctuation">,</span> c<span class="token punctuation">,</span> h<span class="token punctuation">,</span> w<span class="token punctuation">)</span>
        <span class="token keyword">if</span> <span class="token operator">not</span> self<span class="token punctuation">.</span>use_high_res_features<span class="token punctuation">:</span>
            upscaled_embedding <span class="token operator">=</span> self<span class="token punctuation">.</span>output_upscaling<span class="token punctuation">(</span>src<span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            dc1<span class="token punctuation">,</span> ln1<span class="token punctuation">,</span> act1<span class="token punctuation">,</span> dc2<span class="token punctuation">,</span> act2 <span class="token operator">=</span> self<span class="token punctuation">.</span>output_upscaling
            feat_s0<span class="token punctuation">,</span> feat_s1 <span class="token operator">=</span> high_res_features
            upscaled_embedding <span class="token operator">=</span> act1<span class="token punctuation">(</span>ln1<span class="token punctuation">(</span>dc1<span class="token punctuation">(</span>src<span class="token punctuation">)</span> <span class="token operator">+</span> feat_s1<span class="token punctuation">)</span><span class="token punctuation">)</span>
            upscaled_embedding <span class="token operator">=</span> act2<span class="token punctuation">(</span>dc2<span class="token punctuation">(</span>upscaled_embedding<span class="token punctuation">)</span> <span class="token operator">+</span> feat_s0<span class="token punctuation">)</span>

        hyper_in_list<span class="token punctuation">:</span> List<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>self<span class="token punctuation">.</span>num_mask_tokens<span class="token punctuation">)</span><span class="token punctuation">:</span>
            hyper_in_list<span class="token punctuation">.</span>append<span class="token punctuation">(</span>
                self<span class="token punctuation">.</span>output_hypernetworks_mlps<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">(</span>mask_tokens_out<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> i<span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
            <span class="token punctuation">)</span>
        hyper_in <span class="token operator">=</span> torch<span class="token punctuation">.</span>stack<span class="token punctuation">(</span>hyper_in_list<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
        b<span class="token punctuation">,</span> c<span class="token punctuation">,</span> h<span class="token punctuation">,</span> w <span class="token operator">=</span> upscaled_embedding<span class="token punctuation">.</span>shape
        masks <span class="token operator">=</span> <span class="token punctuation">(</span>hyper_in @ upscaled_embedding<span class="token punctuation">.</span>view<span class="token punctuation">(</span>b<span class="token punctuation">,</span> c<span class="token punctuation">,</span> h <span class="token operator">*</span> w<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span>b<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> h<span class="token punctuation">,</span> w<span class="token punctuation">)</span>

        <span class="token comment" spellcheck="true"># Generate mask quality predictions</span>
        iou_pred <span class="token operator">=</span> self<span class="token punctuation">.</span>iou_prediction_head<span class="token punctuation">(</span>iou_token_out<span class="token punctuation">)</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>pred_obj_scores<span class="token punctuation">:</span>
            <span class="token keyword">assert</span> s <span class="token operator">==</span> <span class="token number">1</span>
            object_score_logits <span class="token operator">=</span> self<span class="token punctuation">.</span>pred_obj_score_head<span class="token punctuation">(</span>hs<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            <span class="token comment" spellcheck="true"># Obj scores logits - default to 10.0, i.e. assuming the object is present, sigmoid(10)=1</span>
            object_score_logits <span class="token operator">=</span> <span class="token number">10.0</span> <span class="token operator">*</span> iou_pred<span class="token punctuation">.</span>new_ones<span class="token punctuation">(</span>iou_pred<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>

        <span class="token keyword">return</span> masks<span class="token punctuation">,</span> iou_pred<span class="token punctuation">,</span> mask_tokens_out<span class="token punctuation">,</span> object_score_logits
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p><img src="/2025/06/12/SAM2%E6%9E%B6%E6%9E%84%E6%A6%82%E8%A7%88%E5%92%8C%E8%AF%A6%E7%BB%86%E8%A7%A3%E8%AF%BB/%E6%8E%A9%E7%A0%81%E8%A7%A3%E7%A0%81%E5%99%A8%E7%BB%93%E6%9E%84.png" alt="掩码解码器结构"></p>
<p>代码与上图结合看（<em>斜体</em>代表图中对应的名称），<strong>输入端</strong>，输出层权重<code>output_tokens</code>（<code>iou_token</code>和<code>mask_tokens</code>）与稀疏提示编码拼接成<code>tokens</code><em>（output tokens+prompt tokens）</em>，而图像编码与密集提示编码（掩码）直接相加成<code>src</code><em>（image embedding）</em>，然后把<code>tokens</code>、<code>src</code>、图像位置编码<code>pos_src</code>输入到<code>self.transformer</code>（SAM2用的2层的双路transformer，<em>红色部分</em>）。</p>
<p><strong>输出端</strong>，高分辨率特征（<em>stride 4, 8 feats</em>）可选择地被用在上采样环节（转置卷积，<em>conv.trans</em>）生成masks，注意力输出层的输出<code>hs</code>会分解成3个部分，分别经过各自的MLP（<em>右边3个mlp</em>）输出掩码、IoU预测分数、遮挡分数，第一部分在经过mlp之前会直接作为目标帧的指针（遮挡分数和指针仅用于视频任务）。masks由上采样模块和第一份MLP的输出进行矩阵乘法得到，如果一次生成多个masks，则每个mask都会对应一个MLP（<code>self.output_hypernetworks_mlps</code>里面）。</p>
<h3 id="后处理生成最终masks"><a href="#后处理生成最终masks" class="headerlink" title="后处理生成最终masks"></a>后处理生成最终masks</h3><p>输出的低分辨率图像需要转换为高分辨率的，用到之前说的<code>SAM2Transforms</code>的后处理函数<code>postprocess_masks()</code>：</p>
<pre class="line-numbers language-python"><code class="language-python">    <span class="token keyword">def</span> <span class="token function">postprocess_masks</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> masks<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">,</span> orig_hw<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""
        Perform PostProcessing on output masks.
        """</span>
        <span class="token keyword">from</span> sam2<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>misc <span class="token keyword">import</span> get_connected_components

        masks <span class="token operator">=</span> masks<span class="token punctuation">.</span>float<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>max_hole_area <span class="token operator">></span> <span class="token number">0</span><span class="token punctuation">:</span>
            <span class="token comment" spellcheck="true"># Holes are those connected components in background with area &lt;= self.fill_hole_area</span>
            <span class="token comment" spellcheck="true"># (background regions are those with mask scores &lt;= self.mask_threshold)</span>
            mask_flat <span class="token operator">=</span> masks<span class="token punctuation">.</span>flatten<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># flatten as 1-channel image</span>
            labels<span class="token punctuation">,</span> areas <span class="token operator">=</span> get_connected_components<span class="token punctuation">(</span>mask_flat <span class="token operator">&lt;=</span> self<span class="token punctuation">.</span>mask_threshold<span class="token punctuation">)</span>
            is_hole <span class="token operator">=</span> <span class="token punctuation">(</span>labels <span class="token operator">></span> <span class="token number">0</span><span class="token punctuation">)</span> <span class="token operator">&amp;</span> <span class="token punctuation">(</span>areas <span class="token operator">&lt;=</span> self<span class="token punctuation">.</span>max_hole_area<span class="token punctuation">)</span>
            is_hole <span class="token operator">=</span> is_hole<span class="token punctuation">.</span>reshape_as<span class="token punctuation">(</span>masks<span class="token punctuation">)</span>
            <span class="token comment" spellcheck="true"># We fill holes with a small positive mask score (10.0) to change them to foreground.</span>
            masks <span class="token operator">=</span> torch<span class="token punctuation">.</span>where<span class="token punctuation">(</span>is_hole<span class="token punctuation">,</span> self<span class="token punctuation">.</span>mask_threshold <span class="token operator">+</span> <span class="token number">10.0</span><span class="token punctuation">,</span> masks<span class="token punctuation">)</span>

        <span class="token keyword">if</span> self<span class="token punctuation">.</span>max_sprinkle_area <span class="token operator">></span> <span class="token number">0</span><span class="token punctuation">:</span>
            labels<span class="token punctuation">,</span> areas <span class="token operator">=</span> get_connected_components<span class="token punctuation">(</span>mask_flat <span class="token operator">></span> self<span class="token punctuation">.</span>mask_threshold<span class="token punctuation">)</span>
            is_hole <span class="token operator">=</span> <span class="token punctuation">(</span>labels <span class="token operator">></span> <span class="token number">0</span><span class="token punctuation">)</span> <span class="token operator">&amp;</span> <span class="token punctuation">(</span>areas <span class="token operator">&lt;=</span> self<span class="token punctuation">.</span>max_sprinkle_area<span class="token punctuation">)</span>
            is_hole <span class="token operator">=</span> is_hole<span class="token punctuation">.</span>reshape_as<span class="token punctuation">(</span>masks<span class="token punctuation">)</span>
            <span class="token comment" spellcheck="true"># We fill holes with negative mask score (-10.0) to change them to background.</span>
            masks <span class="token operator">=</span> torch<span class="token punctuation">.</span>where<span class="token punctuation">(</span>is_hole<span class="token punctuation">,</span> self<span class="token punctuation">.</span>mask_threshold <span class="token operator">-</span> <span class="token number">10.0</span><span class="token punctuation">,</span> masks<span class="token punctuation">)</span>

        masks <span class="token operator">=</span> F<span class="token punctuation">.</span>interpolate<span class="token punctuation">(</span>masks<span class="token punctuation">,</span> orig_hw<span class="token punctuation">,</span> mode<span class="token operator">=</span><span class="token string">"bilinear"</span><span class="token punctuation">,</span> align_corners<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> masks
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>简单来说，该函数完成了填坑、去噪、插值还原尺寸，返回了真正的原尺寸预测掩码masks。</p>
<p>&#x3D;&#x3D;至此，我们对训练、验证、预测都会用到的关键部分代码都进行了了解，还有更细节的前向传播和网络结构参见下一大节。&#x3D;&#x3D;</p>
<h1 id="🔥网络结构"><a href="#🔥网络结构" class="headerlink" title="🔥网络结构"></a>🔥网络结构</h1><p>接下来直接细看网络各个部分的结构，以及用了写什么技术。sam2&#x2F;modeling文件夹包含了整个SAM2的结构。</p>
<blockquote>
<p>modeling&#x2F;<br>├── backbones&#x2F;<br>│   ├── hierdet.py<br>│   ├── image_encoder.py<br>│   └── utils.py<br>└── sam&#x2F;<br>│   ├── mask_decoder.py<br>│   ├── prompt_encoder.py<br>│   ├── transformer.py<br>├── memory_attention.py<br>├── memory_encoder.py<br>├── position_encoding.py<br>├── sam2_base.py<br>└── sam2_utils.py</p>
</blockquote>
<p>可以看到图像编码器在backbones，mask解码器和提示编码器在sam，SAM2新增的内存机制在modeling（与视频相关，不细看）。</p>
<h2 id="🚠工具：sam2-util-py"><a href="#🚠工具：sam2-util-py" class="headerlink" title="🚠工具：sam2_util.py"></a>🚠工具：sam2_util.py</h2><p>里面有SAM2的一些工具函数和类：</p>
<ul>
<li><p><code>select_closest_cond_frames()</code>：寻找最近的提示帧，用于视频；</p>
</li>
<li><p><code>get_1d_sine_pe()</code>：获取一维的正弦位置编码；</p>
</li>
<li><p><code>get_activation_fn()</code>：根据配置获取激活函数；</p>
</li>
<li><p>自定义的dropout<code>DropPath</code>、<code>MLP</code>和<code>LayerNorm2d</code>层。</p>
</li>
</ul>
<h2 id="🚠位置编码：position-encoding-py"><a href="#🚠位置编码：position-encoding-py" class="headerlink" title="🚠位置编码：position_encoding.py"></a>🚠位置编码：position_encoding.py</h2><p>提供了三种位置编码，2D正弦位置编码<code>PositionEmbeddingSine</code>、随机空间频率位置编码<code>PositionEmbeddingRandom</code>、旋转位置编码RoPE。</p>
<p>SAM2的image encoder用的应该是<code>PositionEmbeddingSine</code>；而prompt encoder用的是<code>PositionEmbeddingRandom</code>，它将坐标映射到[0, 1]区间，然后使用随机生成的矩阵进行变换，最后通过正弦和余弦函数生成编码。</p>
<h2 id="🚠SAM2网络：sam2-base-py"><a href="#🚠SAM2网络：sam2-base-py" class="headerlink" title="🚠SAM2网络：sam2_base.py"></a>🚠SAM2网络：sam2_base.py</h2><p><code>SAM2Base</code>就是<strong>整个SAM2网络</strong>，初始化函数是一堆网络结构方面的参数设置，不详细展开了，其中最重要的是调用了<code>SAM2Base._build_sam_heads()</code>。</p>
<pre class="line-numbers language-python"><code class="language-python">    <span class="token keyword">def</span> <span class="token function">_build_sam_heads</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""Build SAM-style prompt encoder and mask decoder."""</span>
        self<span class="token punctuation">.</span>sam_prompt_embed_dim <span class="token operator">=</span> self<span class="token punctuation">.</span>hidden_dim
        self<span class="token punctuation">.</span>sam_image_embedding_size <span class="token operator">=</span> self<span class="token punctuation">.</span>image_size <span class="token operator">//</span> self<span class="token punctuation">.</span>backbone_stride

        <span class="token comment" spellcheck="true"># build PromptEncoder and MaskDecoder from SAM</span>
        <span class="token comment" spellcheck="true"># (their hyperparameters like `mask_in_chans=16` are from SAM code)</span>
        self<span class="token punctuation">.</span>sam_prompt_encoder <span class="token operator">=</span> PromptEncoder<span class="token punctuation">(</span>
            embed_dim<span class="token operator">=</span>self<span class="token punctuation">.</span>sam_prompt_embed_dim<span class="token punctuation">,</span>
            image_embedding_size<span class="token operator">=</span><span class="token punctuation">(</span>
                self<span class="token punctuation">.</span>sam_image_embedding_size<span class="token punctuation">,</span>
                self<span class="token punctuation">.</span>sam_image_embedding_size<span class="token punctuation">,</span>
            <span class="token punctuation">)</span><span class="token punctuation">,</span>
            input_image_size<span class="token operator">=</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>image_size<span class="token punctuation">,</span> self<span class="token punctuation">.</span>image_size<span class="token punctuation">)</span><span class="token punctuation">,</span>
            mask_in_chans<span class="token operator">=</span><span class="token number">16</span><span class="token punctuation">,</span>
        <span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>sam_mask_decoder <span class="token operator">=</span> MaskDecoder<span class="token punctuation">(</span>
            num_multimask_outputs<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span>
            transformer<span class="token operator">=</span>TwoWayTransformer<span class="token punctuation">(</span>
                depth<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span>
                embedding_dim<span class="token operator">=</span>self<span class="token punctuation">.</span>sam_prompt_embed_dim<span class="token punctuation">,</span>
                mlp_dim<span class="token operator">=</span><span class="token number">2048</span><span class="token punctuation">,</span>
                num_heads<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">,</span>
            <span class="token punctuation">)</span><span class="token punctuation">,</span>
            transformer_dim<span class="token operator">=</span>self<span class="token punctuation">.</span>sam_prompt_embed_dim<span class="token punctuation">,</span>
            iou_head_depth<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span>
            iou_head_hidden_dim<span class="token operator">=</span><span class="token number">256</span><span class="token punctuation">,</span>
            use_high_res_features<span class="token operator">=</span>self<span class="token punctuation">.</span>use_high_res_features_in_sam<span class="token punctuation">,</span>
            iou_prediction_use_sigmoid<span class="token operator">=</span>self<span class="token punctuation">.</span>iou_prediction_use_sigmoid<span class="token punctuation">,</span>
            pred_obj_scores<span class="token operator">=</span>self<span class="token punctuation">.</span>pred_obj_scores<span class="token punctuation">,</span>
            pred_obj_scores_mlp<span class="token operator">=</span>self<span class="token punctuation">.</span>pred_obj_scores_mlp<span class="token punctuation">,</span>
            use_multimask_token_for_obj_ptr<span class="token operator">=</span>self<span class="token punctuation">.</span>use_multimask_token_for_obj_ptr<span class="token punctuation">,</span>
            <span class="token operator">**</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>sam_mask_decoder_extra_args <span class="token operator">or</span> <span class="token punctuation">{</span><span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        <span class="token punctuation">)</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>use_obj_ptrs_in_encoder<span class="token punctuation">:</span>
            <span class="token comment" spellcheck="true"># a linear projection on SAM output tokens to turn them into object pointers</span>
            self<span class="token punctuation">.</span>obj_ptr_proj <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>self<span class="token punctuation">.</span>hidden_dim<span class="token punctuation">,</span> self<span class="token punctuation">.</span>hidden_dim<span class="token punctuation">)</span>
            <span class="token keyword">if</span> self<span class="token punctuation">.</span>use_mlp_for_obj_ptr_proj<span class="token punctuation">:</span>
                self<span class="token punctuation">.</span>obj_ptr_proj <span class="token operator">=</span> MLP<span class="token punctuation">(</span>
                    self<span class="token punctuation">.</span>hidden_dim<span class="token punctuation">,</span> self<span class="token punctuation">.</span>hidden_dim<span class="token punctuation">,</span> self<span class="token punctuation">.</span>hidden_dim<span class="token punctuation">,</span> <span class="token number">3</span>
                <span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>obj_ptr_proj <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Identity<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>proj_tpos_enc_in_obj_ptrs<span class="token punctuation">:</span>
            <span class="token comment" spellcheck="true"># a linear projection on temporal positional encoding in object pointers to</span>
            <span class="token comment" spellcheck="true"># avoid potential interference with spatial positional encoding</span>
            self<span class="token punctuation">.</span>obj_ptr_tpos_proj <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>self<span class="token punctuation">.</span>hidden_dim<span class="token punctuation">,</span> self<span class="token punctuation">.</span>mem_dim<span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>obj_ptr_tpos_proj <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Identity<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p><code>_build_sam_heads()</code>主要构建的是prompt encoder和mask decoder两个部分，image encoder是外部传入的（yaml文件设置）。所以可以看到yaml文件里面是不会有这两个部分的参数的，因为在这里设置了。</p>
<ul>
<li><p><code>_forward_sam_heads()</code>：prompt encoder和mask decoder两个部分的前向传播；</p>
</li>
<li><p><code>forward_image()</code>：视觉主干image encoder的前向传播；</p>
</li>
<li><p><code>_prepare_backbone_features()</code>：之前提到过，图像编码后会解析输出用作后续的特征；</p>
</li>
<li><p><code>SAM2Base</code>禁用了<code>forward</code>，需要在之前提到的<code>SAM2ImagePredictor</code>里面调用。</p>
</li>
<li><p><code>SAM2Base</code>中除了以上函数，剩下的都是和视频任务有关的。</p>
</li>
</ul>
<p>&#x3D;&#x3D;至此我们完成了对整个SAM2模型有宏观的了解，可以看到 SAM2ImagePredictor 和 SAM2VideoPredictor 本质上算是SAM2Base的“子类”，只不过两者不同程度地用到SAM2Base的功能，所以官方分开给不同需求者使用。&#x3D;&#x3D;</p>
<h2 id="🚠图像编码器：image-encoder-py"><a href="#🚠图像编码器：image-encoder-py" class="headerlink" title="🚠图像编码器：image_encoder.py"></a>🚠图像编码器：image_encoder.py</h2><p>image encoder的构建是读取如下的yaml文件，这是SAM2 b+模型的配置，省去了内存机制部分。</p>
<pre class="line-numbers language-yaml"><code class="language-yaml"><span class="token key atrule">model</span><span class="token punctuation">:</span>
  <span class="token key atrule">_target_</span><span class="token punctuation">:</span> sam2.modeling.sam2_base.SAM2Base
  <span class="token key atrule">image_encoder</span><span class="token punctuation">:</span>
    <span class="token key atrule">_target_</span><span class="token punctuation">:</span> sam2.modeling.backbones.image_encoder.ImageEncoder
    <span class="token key atrule">scalp</span><span class="token punctuation">:</span> <span class="token number">1</span>
    <span class="token key atrule">trunk</span><span class="token punctuation">:</span>
      <span class="token key atrule">_target_</span><span class="token punctuation">:</span> sam2.modeling.backbones.hieradet.Hiera
      <span class="token key atrule">embed_dim</span><span class="token punctuation">:</span> <span class="token number">112</span>
      <span class="token key atrule">num_heads</span><span class="token punctuation">:</span> <span class="token number">2</span>
    <span class="token key atrule">neck</span><span class="token punctuation">:</span>
      <span class="token key atrule">_target_</span><span class="token punctuation">:</span> sam2.modeling.backbones.image_encoder.FpnNeck
      <span class="token key atrule">position_encoding</span><span class="token punctuation">:</span>
        <span class="token key atrule">_target_</span><span class="token punctuation">:</span> sam2.modeling.position_encoding.PositionEmbeddingSine
        <span class="token key atrule">num_pos_feats</span><span class="token punctuation">:</span> <span class="token number">256</span>
        <span class="token key atrule">normalize</span><span class="token punctuation">:</span> <span class="token boolean important">true</span>
        <span class="token key atrule">scale</span><span class="token punctuation">:</span> <span class="token null important">null</span>
        <span class="token key atrule">temperature</span><span class="token punctuation">:</span> <span class="token number">10000</span>
      <span class="token key atrule">d_model</span><span class="token punctuation">:</span> <span class="token number">256</span>
      <span class="token key atrule">backbone_channel_list</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token number">896</span><span class="token punctuation">,</span> <span class="token number">448</span><span class="token punctuation">,</span> <span class="token number">224</span><span class="token punctuation">,</span> <span class="token number">112</span><span class="token punctuation">]</span>
      <span class="token key atrule">fpn_top_down_levels</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span>  <span class="token comment" spellcheck="true"># output level 0 and 1 directly use the backbone features</span>
      <span class="token key atrule">fpn_interp_model</span><span class="token punctuation">:</span> nearest
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">ImageEncoder</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>
        self<span class="token punctuation">,</span>
        trunk<span class="token punctuation">:</span> nn<span class="token punctuation">.</span>Module<span class="token punctuation">,</span>
        neck<span class="token punctuation">:</span> nn<span class="token punctuation">.</span>Module<span class="token punctuation">,</span>
        scalp<span class="token punctuation">:</span> int <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">,</span>
    <span class="token punctuation">)</span><span class="token punctuation">:</span>
        super<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>trunk <span class="token operator">=</span> trunk
        self<span class="token punctuation">.</span>neck <span class="token operator">=</span> neck
        self<span class="token punctuation">.</span>scalp <span class="token operator">=</span> scalp
        <span class="token keyword">assert</span> <span class="token punctuation">(</span>
            self<span class="token punctuation">.</span>trunk<span class="token punctuation">.</span>channel_list <span class="token operator">==</span> self<span class="token punctuation">.</span>neck<span class="token punctuation">.</span>backbone_channel_list
        <span class="token punctuation">)</span><span class="token punctuation">,</span> f<span class="token string">"Channel dims of trunk and neck do not match. Trunk: {self.trunk.channel_list}, neck: {self.neck.backbone_channel_list}"</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> sample<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment" spellcheck="true"># Forward through backbone</span>
        features<span class="token punctuation">,</span> pos <span class="token operator">=</span> self<span class="token punctuation">.</span>neck<span class="token punctuation">(</span>self<span class="token punctuation">.</span>trunk<span class="token punctuation">(</span>sample<span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>scalp <span class="token operator">></span> <span class="token number">0</span><span class="token punctuation">:</span>
            <span class="token comment" spellcheck="true"># Discard the lowest resolution features</span>
            features<span class="token punctuation">,</span> pos <span class="token operator">=</span> features<span class="token punctuation">[</span><span class="token punctuation">:</span> <span class="token operator">-</span>self<span class="token punctuation">.</span>scalp<span class="token punctuation">]</span><span class="token punctuation">,</span> pos<span class="token punctuation">[</span><span class="token punctuation">:</span> <span class="token operator">-</span>self<span class="token punctuation">.</span>scalp<span class="token punctuation">]</span>

        src <span class="token operator">=</span> features<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span>
        output <span class="token operator">=</span> <span class="token punctuation">{</span>
            <span class="token string">"vision_features"</span><span class="token punctuation">:</span> src<span class="token punctuation">,</span>
            <span class="token string">"vision_pos_enc"</span><span class="token punctuation">:</span> pos<span class="token punctuation">,</span>
            <span class="token string">"backbone_fpn"</span><span class="token punctuation">:</span> features<span class="token punctuation">,</span>
        <span class="token punctuation">}</span>
        <span class="token keyword">return</span> output
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p><code>ImageEncoder</code>由trunk、neck组成，它们已经在build_sam.py里面读取yaml配置时完成了加载，所以这里直接赋值给属性就好了。而对于这两个部分，<strong>可以类比YOLO系列网络的trunk、neck部分（下图左、中部分），它们理念上几乎完全一样</strong>，随着代码的解读就能明白这点。</p>
<p><img src="/2025/06/12/SAM2%E6%9E%B6%E6%9E%84%E6%A6%82%E8%A7%88%E5%92%8C%E8%AF%A6%E7%BB%86%E8%A7%A3%E8%AF%BB/YOLOv8.png" alt="YOLOv8"></p>
<p>前向传播也比较简单，生成的<code>output</code>包含了最终层输出<code>vision_features</code>、位置编码<code>vision_pos_enc</code>、多尺度特征<code>backbone_fpn</code>，具体需要从这两个部分去详细理解。所以接下来我们看看image encoder的两个部分。</p>
<h3 id="trunk：hieradet-py"><a href="#trunk：hieradet-py" class="headerlink" title="trunk：hieradet.py"></a>trunk：hieradet.py</h3><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">MultiScaleBlock</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>
        self<span class="token punctuation">,</span>
        dim<span class="token punctuation">:</span> int<span class="token punctuation">,</span>
        dim_out<span class="token punctuation">:</span> int<span class="token punctuation">,</span>
        num_heads<span class="token punctuation">:</span> int<span class="token punctuation">,</span>
        mlp_ratio<span class="token punctuation">:</span> float <span class="token operator">=</span> <span class="token number">4.0</span><span class="token punctuation">,</span>
        drop_path<span class="token punctuation">:</span> float <span class="token operator">=</span> <span class="token number">0.0</span><span class="token punctuation">,</span>
        norm_layer<span class="token punctuation">:</span> Union<span class="token punctuation">[</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">,</span> str<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token string">"LayerNorm"</span><span class="token punctuation">,</span>
        q_stride<span class="token punctuation">:</span> Tuple<span class="token punctuation">[</span>int<span class="token punctuation">,</span> int<span class="token punctuation">]</span> <span class="token operator">=</span> None<span class="token punctuation">,</span>
        act_layer<span class="token punctuation">:</span> nn<span class="token punctuation">.</span>Module <span class="token operator">=</span> nn<span class="token punctuation">.</span>GELU<span class="token punctuation">,</span>
        window_size<span class="token punctuation">:</span> int <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">,</span>
    <span class="token punctuation">)</span><span class="token punctuation">:</span>
        super<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>

        <span class="token keyword">if</span> isinstance<span class="token punctuation">(</span>norm_layer<span class="token punctuation">,</span> str<span class="token punctuation">)</span><span class="token punctuation">:</span>
            norm_layer <span class="token operator">=</span> partial<span class="token punctuation">(</span>getattr<span class="token punctuation">(</span>nn<span class="token punctuation">,</span> norm_layer<span class="token punctuation">)</span><span class="token punctuation">,</span> eps<span class="token operator">=</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">6</span><span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>dim <span class="token operator">=</span> dim
        self<span class="token punctuation">.</span>dim_out <span class="token operator">=</span> dim_out
        self<span class="token punctuation">.</span>norm1 <span class="token operator">=</span> norm_layer<span class="token punctuation">(</span>dim<span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>window_size <span class="token operator">=</span> window_size

        self<span class="token punctuation">.</span>pool<span class="token punctuation">,</span> self<span class="token punctuation">.</span>q_stride <span class="token operator">=</span> None<span class="token punctuation">,</span> q_stride
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>q_stride<span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>pool <span class="token operator">=</span> nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span>
                kernel_size<span class="token operator">=</span>q_stride<span class="token punctuation">,</span> stride<span class="token operator">=</span>q_stride<span class="token punctuation">,</span> ceil_mode<span class="token operator">=</span><span class="token boolean">False</span>
            <span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>attn <span class="token operator">=</span> MultiScaleAttention<span class="token punctuation">(</span>
            dim<span class="token punctuation">,</span>
            dim_out<span class="token punctuation">,</span>
            num_heads<span class="token operator">=</span>num_heads<span class="token punctuation">,</span>
            q_pool<span class="token operator">=</span>self<span class="token punctuation">.</span>pool<span class="token punctuation">,</span>
        <span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>drop_path <span class="token operator">=</span> DropPath<span class="token punctuation">(</span>drop_path<span class="token punctuation">)</span> <span class="token keyword">if</span> drop_path <span class="token operator">></span> <span class="token number">0.0</span> <span class="token keyword">else</span> nn<span class="token punctuation">.</span>Identity<span class="token punctuation">(</span><span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>norm2 <span class="token operator">=</span> norm_layer<span class="token punctuation">(</span>dim_out<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>mlp <span class="token operator">=</span> MLP<span class="token punctuation">(</span>
            dim_out<span class="token punctuation">,</span>
            int<span class="token punctuation">(</span>dim_out <span class="token operator">*</span> mlp_ratio<span class="token punctuation">)</span><span class="token punctuation">,</span>
            dim_out<span class="token punctuation">,</span>
            num_layers<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span>
            activation<span class="token operator">=</span>act_layer<span class="token punctuation">,</span>
        <span class="token punctuation">)</span>

        <span class="token keyword">if</span> dim <span class="token operator">!=</span> dim_out<span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>proj <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>dim<span class="token punctuation">,</span> dim_out<span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p><code>MultiScaleBlock</code>是基本的transformer块，包括了该有的归一化层<code>LayerNorm</code>、注意力层<code>MultiScaleAttention</code>、dropout层<code>DropPath</code>、FFN层<code>MLP</code>，其中<code>MultiScaleAttention</code>是同文件下实现的多头点积注意力机制（和ViT的没多大区别）。前向传播没什么特别的就不展示。</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Hiera</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""
    Reference: https://arxiv.org/abs/2306.00989
    """</span>

    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>
        self<span class="token punctuation">,</span>
        embed_dim<span class="token punctuation">:</span> int <span class="token operator">=</span> <span class="token number">96</span><span class="token punctuation">,</span>  <span class="token comment" spellcheck="true"># initial embed dim</span>
        num_heads<span class="token punctuation">:</span> int <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">,</span>  <span class="token comment" spellcheck="true"># initial number of heads</span>
        drop_path_rate<span class="token punctuation">:</span> float <span class="token operator">=</span> <span class="token number">0.0</span><span class="token punctuation">,</span>  <span class="token comment" spellcheck="true"># stochastic depth</span>
        q_pool<span class="token punctuation">:</span> int <span class="token operator">=</span> <span class="token number">3</span><span class="token punctuation">,</span>  <span class="token comment" spellcheck="true"># number of q_pool stages</span>
        q_stride<span class="token punctuation">:</span> Tuple<span class="token punctuation">[</span>int<span class="token punctuation">,</span> int<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>  <span class="token comment" spellcheck="true"># downsample stride bet. stages</span>
        stages<span class="token punctuation">:</span> Tuple<span class="token punctuation">[</span>int<span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">16</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span>  <span class="token comment" spellcheck="true"># blocks per stage</span>
        dim_mul<span class="token punctuation">:</span> float <span class="token operator">=</span> <span class="token number">2.0</span><span class="token punctuation">,</span>  <span class="token comment" spellcheck="true"># dim_mul factor at stage shift</span>
        head_mul<span class="token punctuation">:</span> float <span class="token operator">=</span> <span class="token number">2.0</span><span class="token punctuation">,</span>  <span class="token comment" spellcheck="true"># head_mul factor at stage shift</span>
        window_pos_embed_bkg_spatial_size<span class="token punctuation">:</span> Tuple<span class="token punctuation">[</span>int<span class="token punctuation">,</span> int<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token number">14</span><span class="token punctuation">,</span> <span class="token number">14</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        <span class="token comment" spellcheck="true"># window size per stage, when not using global att.</span>
        window_spec<span class="token punctuation">:</span> Tuple<span class="token punctuation">[</span>int<span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">(</span>
            <span class="token number">8</span><span class="token punctuation">,</span>
            <span class="token number">4</span><span class="token punctuation">,</span>
            <span class="token number">14</span><span class="token punctuation">,</span>
            <span class="token number">7</span><span class="token punctuation">,</span>
        <span class="token punctuation">)</span><span class="token punctuation">,</span>
        <span class="token comment" spellcheck="true"># global attn in these blocks</span>
        global_att_blocks<span class="token punctuation">:</span> Tuple<span class="token punctuation">[</span>int<span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">(</span>
            <span class="token number">12</span><span class="token punctuation">,</span>
            <span class="token number">16</span><span class="token punctuation">,</span>
            <span class="token number">20</span><span class="token punctuation">,</span>
        <span class="token punctuation">)</span><span class="token punctuation">,</span>
        return_interm_layers<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>  <span class="token comment" spellcheck="true"># return feats from every stage</span>
    <span class="token punctuation">)</span><span class="token punctuation">:</span>
        super<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>

        <span class="token keyword">assert</span> len<span class="token punctuation">(</span>stages<span class="token punctuation">)</span> <span class="token operator">==</span> len<span class="token punctuation">(</span>window_spec<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>window_spec <span class="token operator">=</span> window_spec

        depth <span class="token operator">=</span> sum<span class="token punctuation">(</span>stages<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>q_stride <span class="token operator">=</span> q_stride
        self<span class="token punctuation">.</span>stage_ends <span class="token operator">=</span> <span class="token punctuation">[</span>sum<span class="token punctuation">(</span>stages<span class="token punctuation">[</span><span class="token punctuation">:</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">-</span> <span class="token number">1</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> len<span class="token punctuation">(</span>stages<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">]</span>
        <span class="token keyword">assert</span> <span class="token number">0</span> <span class="token operator">&lt;=</span> q_pool <span class="token operator">&lt;=</span> len<span class="token punctuation">(</span>self<span class="token punctuation">.</span>stage_ends<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>q_pool_blocks <span class="token operator">=</span> <span class="token punctuation">[</span>x <span class="token operator">+</span> <span class="token number">1</span> <span class="token keyword">for</span> x <span class="token keyword">in</span> self<span class="token punctuation">.</span>stage_ends<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token punctuation">:</span>q_pool<span class="token punctuation">]</span>
        self<span class="token punctuation">.</span>return_interm_layers <span class="token operator">=</span> return_interm_layers

        self<span class="token punctuation">.</span>patch_embed <span class="token operator">=</span> PatchEmbed<span class="token punctuation">(</span>
            embed_dim<span class="token operator">=</span>embed_dim<span class="token punctuation">,</span>
        <span class="token punctuation">)</span>
        <span class="token comment" spellcheck="true"># Which blocks have global att?</span>
        self<span class="token punctuation">.</span>global_att_blocks <span class="token operator">=</span> global_att_blocks

        <span class="token comment" spellcheck="true"># Windowed positional embedding (https://arxiv.org/abs/2311.05613)</span>
        self<span class="token punctuation">.</span>window_pos_embed_bkg_spatial_size <span class="token operator">=</span> window_pos_embed_bkg_spatial_size
        self<span class="token punctuation">.</span>pos_embed <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>
            torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> embed_dim<span class="token punctuation">,</span> <span class="token operator">*</span>self<span class="token punctuation">.</span>window_pos_embed_bkg_spatial_size<span class="token punctuation">)</span>
        <span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>pos_embed_window <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>
            torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> embed_dim<span class="token punctuation">,</span> self<span class="token punctuation">.</span>window_spec<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>window_spec<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        <span class="token punctuation">)</span>

        dpr <span class="token operator">=</span> <span class="token punctuation">[</span>
            x<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">for</span> x <span class="token keyword">in</span> torch<span class="token punctuation">.</span>linspace<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> drop_path_rate<span class="token punctuation">,</span> depth<span class="token punctuation">)</span>
        <span class="token punctuation">]</span>  <span class="token comment" spellcheck="true"># stochastic depth decay rule</span>

        cur_stage <span class="token operator">=</span> <span class="token number">1</span>
        self<span class="token punctuation">.</span>blocks <span class="token operator">=</span> nn<span class="token punctuation">.</span>ModuleList<span class="token punctuation">(</span><span class="token punctuation">)</span>

        <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>depth<span class="token punctuation">)</span><span class="token punctuation">:</span>
            dim_out <span class="token operator">=</span> embed_dim
            <span class="token comment" spellcheck="true"># lags by a block, so first block of</span>
            <span class="token comment" spellcheck="true"># next stage uses an initial window size</span>
            <span class="token comment" spellcheck="true"># of previous stage and final window size of current stage</span>
            window_size <span class="token operator">=</span> self<span class="token punctuation">.</span>window_spec<span class="token punctuation">[</span>cur_stage <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">]</span>

            <span class="token keyword">if</span> self<span class="token punctuation">.</span>global_att_blocks <span class="token keyword">is</span> <span class="token operator">not</span> None<span class="token punctuation">:</span>
                window_size <span class="token operator">=</span> <span class="token number">0</span> <span class="token keyword">if</span> i <span class="token keyword">in</span> self<span class="token punctuation">.</span>global_att_blocks <span class="token keyword">else</span> window_size

            <span class="token keyword">if</span> i <span class="token operator">-</span> <span class="token number">1</span> <span class="token keyword">in</span> self<span class="token punctuation">.</span>stage_ends<span class="token punctuation">:</span>
                dim_out <span class="token operator">=</span> int<span class="token punctuation">(</span>embed_dim <span class="token operator">*</span> dim_mul<span class="token punctuation">)</span>
                num_heads <span class="token operator">=</span> int<span class="token punctuation">(</span>num_heads <span class="token operator">*</span> head_mul<span class="token punctuation">)</span>
                cur_stage <span class="token operator">+=</span> <span class="token number">1</span>

            block <span class="token operator">=</span> MultiScaleBlock<span class="token punctuation">(</span>
                dim<span class="token operator">=</span>embed_dim<span class="token punctuation">,</span>
                dim_out<span class="token operator">=</span>dim_out<span class="token punctuation">,</span>
                num_heads<span class="token operator">=</span>num_heads<span class="token punctuation">,</span>
                drop_path<span class="token operator">=</span>dpr<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">,</span>
                q_stride<span class="token operator">=</span>self<span class="token punctuation">.</span>q_stride <span class="token keyword">if</span> i <span class="token keyword">in</span> self<span class="token punctuation">.</span>q_pool_blocks <span class="token keyword">else</span> None<span class="token punctuation">,</span>
                window_size<span class="token operator">=</span>window_size<span class="token punctuation">,</span>
            <span class="token punctuation">)</span>

            embed_dim <span class="token operator">=</span> dim_out
            self<span class="token punctuation">.</span>blocks<span class="token punctuation">.</span>append<span class="token punctuation">(</span>block<span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>channel_list <span class="token operator">=</span> <span class="token punctuation">(</span>
            <span class="token punctuation">[</span>self<span class="token punctuation">.</span>blocks<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">.</span>dim_out <span class="token keyword">for</span> i <span class="token keyword">in</span> self<span class="token punctuation">.</span>stage_ends<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">]</span>
            <span class="token keyword">if</span> return_interm_layers
            <span class="token keyword">else</span> <span class="token punctuation">[</span>self<span class="token punctuation">.</span>blocks<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>dim_out<span class="token punctuation">]</span>
        <span class="token punctuation">)</span>
  
    <span class="token keyword">def</span> <span class="token function">_get_pos_embed</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> hw<span class="token punctuation">:</span> Tuple<span class="token punctuation">[</span>int<span class="token punctuation">,</span> int<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">:</span>
        h<span class="token punctuation">,</span> w <span class="token operator">=</span> hw
        window_embed <span class="token operator">=</span> self<span class="token punctuation">.</span>pos_embed_window
        pos_embed <span class="token operator">=</span> F<span class="token punctuation">.</span>interpolate<span class="token punctuation">(</span>self<span class="token punctuation">.</span>pos_embed<span class="token punctuation">,</span> size<span class="token operator">=</span><span class="token punctuation">(</span>h<span class="token punctuation">,</span> w<span class="token punctuation">)</span><span class="token punctuation">,</span> mode<span class="token operator">=</span><span class="token string">"bicubic"</span><span class="token punctuation">)</span>
        pos_embed <span class="token operator">=</span> pos_embed <span class="token operator">+</span> window_embed<span class="token punctuation">.</span>tile<span class="token punctuation">(</span>
            <span class="token punctuation">[</span>x <span class="token operator">//</span> y <span class="token keyword">for</span> x<span class="token punctuation">,</span> y <span class="token keyword">in</span> zip<span class="token punctuation">(</span>pos_embed<span class="token punctuation">.</span>shape<span class="token punctuation">,</span> window_embed<span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token punctuation">]</span>
        <span class="token punctuation">)</span>
        pos_embed <span class="token operator">=</span> pos_embed<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> pos_embed

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> List<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">]</span><span class="token punctuation">:</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>patch_embed<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        <span class="token comment" spellcheck="true"># x: (B, H, W, C)</span>

        <span class="token comment" spellcheck="true"># Add pos embed</span>
        x <span class="token operator">=</span> x <span class="token operator">+</span> self<span class="token punctuation">.</span>_get_pos_embed<span class="token punctuation">(</span>x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

        outputs <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        <span class="token keyword">for</span> i<span class="token punctuation">,</span> blk <span class="token keyword">in</span> enumerate<span class="token punctuation">(</span>self<span class="token punctuation">.</span>blocks<span class="token punctuation">)</span><span class="token punctuation">:</span>
            x <span class="token operator">=</span> blk<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
            <span class="token keyword">if</span> <span class="token punctuation">(</span>i <span class="token operator">==</span> self<span class="token punctuation">.</span>stage_ends<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">or</span> <span class="token punctuation">(</span>
                i <span class="token keyword">in</span> self<span class="token punctuation">.</span>stage_ends <span class="token operator">and</span> self<span class="token punctuation">.</span>return_interm_layers
            <span class="token punctuation">)</span><span class="token punctuation">:</span>
                feats <span class="token operator">=</span> x<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>
                outputs<span class="token punctuation">.</span>append<span class="token punctuation">(</span>feats<span class="token punctuation">)</span>

        <span class="token keyword">return</span> outputs
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p><code>Hiera</code>就是trunk本身，<code>__init__()</code>看似比较复杂，可以先看<code>forward()</code>：就是将<strong>图片分块</strong>（patch）后，做<strong>位置编码</strong>（注意这里不是正弦编码而是用一个层去学）后将<code>x</code>输入到<strong>编码器</strong>（多层<code>MultiScaleBlock</code>）中，编码器每一级的特征都会存到<code>outputs</code>中，传递给neck部分，这和transformer工作当中的编码器没多大区别。</p>
<p>所以反过来看，init也就是做了<strong>分块嵌入层</strong>、<strong>位置编码层</strong>、<strong>编码器</strong>的构建。其中<code>PatchEmbed</code>是用卷积分块，每个<code>MultiScaleBlock</code>有不同的窗口（patch）大小，相当于输出特征有多个不同的尺度。</p>
<h3 id="neck：FpnNeck"><a href="#neck：FpnNeck" class="headerlink" title="neck：FpnNeck"></a>neck：FpnNeck</h3><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">FpnNeck</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""
    A modified variant of Feature Pyramid Network (FPN) neck
    (we remove output conv and also do bicubic interpolation similar to ViT
    pos embed interpolation)
    """</span>

    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>
        self<span class="token punctuation">,</span>
        position_encoding<span class="token punctuation">:</span> nn<span class="token punctuation">.</span>Module<span class="token punctuation">,</span>
        d_model<span class="token punctuation">:</span> int<span class="token punctuation">,</span>
        backbone_channel_list<span class="token punctuation">:</span> List<span class="token punctuation">[</span>int<span class="token punctuation">]</span><span class="token punctuation">,</span>
        kernel_size<span class="token punctuation">:</span> int <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">,</span>
        stride<span class="token punctuation">:</span> int <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">,</span>
        padding<span class="token punctuation">:</span> int <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">,</span>
        fpn_interp_model<span class="token punctuation">:</span> str <span class="token operator">=</span> <span class="token string">"bilinear"</span><span class="token punctuation">,</span>
        fuse_type<span class="token punctuation">:</span> str <span class="token operator">=</span> <span class="token string">"sum"</span><span class="token punctuation">,</span>
        fpn_top_down_levels<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>List<span class="token punctuation">[</span>int<span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> None<span class="token punctuation">,</span>
    <span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""Initialize the neck
        :param trunk: the backbone
        :param position_encoding: the positional encoding to use
        :param d_model: the dimension of the model
        :param neck_norm: the normalization to use
        """</span>
        super<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>position_encoding <span class="token operator">=</span> position_encoding
        self<span class="token punctuation">.</span>convs <span class="token operator">=</span> nn<span class="token punctuation">.</span>ModuleList<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>backbone_channel_list <span class="token operator">=</span> backbone_channel_list
        <span class="token keyword">for</span> dim <span class="token keyword">in</span> backbone_channel_list<span class="token punctuation">:</span>
            current <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span><span class="token punctuation">)</span>
            current<span class="token punctuation">.</span>add_module<span class="token punctuation">(</span>
                <span class="token string">"conv"</span><span class="token punctuation">,</span>
                nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>
                    in_channels<span class="token operator">=</span>dim<span class="token punctuation">,</span>
                    out_channels<span class="token operator">=</span>d_model<span class="token punctuation">,</span>
                    kernel_size<span class="token operator">=</span>kernel_size<span class="token punctuation">,</span>
                    stride<span class="token operator">=</span>stride<span class="token punctuation">,</span>
                    padding<span class="token operator">=</span>padding<span class="token punctuation">,</span>
                <span class="token punctuation">)</span><span class="token punctuation">,</span>
            <span class="token punctuation">)</span>

            self<span class="token punctuation">.</span>convs<span class="token punctuation">.</span>append<span class="token punctuation">(</span>current<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>fpn_interp_model <span class="token operator">=</span> fpn_interp_model
        <span class="token keyword">assert</span> fuse_type <span class="token keyword">in</span> <span class="token punctuation">[</span><span class="token string">"sum"</span><span class="token punctuation">,</span> <span class="token string">"avg"</span><span class="token punctuation">]</span>
        self<span class="token punctuation">.</span>fuse_type <span class="token operator">=</span> fuse_type

        <span class="token comment" spellcheck="true"># levels to have top-down features in its outputs</span>
        <span class="token comment" spellcheck="true"># e.g. if fpn_top_down_levels is [2, 3], then only outputs of level 2 and 3</span>
        <span class="token comment" spellcheck="true"># have top-down propagation, while outputs of level 0 and level 1 have only</span>
        <span class="token comment" spellcheck="true"># lateral features from the same backbone level.</span>
        <span class="token keyword">if</span> fpn_top_down_levels <span class="token keyword">is</span> None<span class="token punctuation">:</span>
            <span class="token comment" spellcheck="true"># default is to have top-down features on all levels</span>
            fpn_top_down_levels <span class="token operator">=</span> range<span class="token punctuation">(</span>len<span class="token punctuation">(</span>self<span class="token punctuation">.</span>convs<span class="token punctuation">)</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>fpn_top_down_levels <span class="token operator">=</span> list<span class="token punctuation">(</span>fpn_top_down_levels<span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>图像编码器的neck部分是一个特征金字塔，初始化了<strong>位置编码</strong>（<code>PositionEmbeddingSine</code>），以及<strong>卷积网络</strong>（列表）。</p>
<pre class="line-numbers language-python"><code class="language-python">    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> xs<span class="token punctuation">:</span> List<span class="token punctuation">[</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        out <span class="token operator">=</span> <span class="token punctuation">[</span>None<span class="token punctuation">]</span> <span class="token operator">*</span> len<span class="token punctuation">(</span>self<span class="token punctuation">.</span>convs<span class="token punctuation">)</span>
        pos <span class="token operator">=</span> <span class="token punctuation">[</span>None<span class="token punctuation">]</span> <span class="token operator">*</span> len<span class="token punctuation">(</span>self<span class="token punctuation">.</span>convs<span class="token punctuation">)</span>
        <span class="token keyword">assert</span> len<span class="token punctuation">(</span>xs<span class="token punctuation">)</span> <span class="token operator">==</span> len<span class="token punctuation">(</span>self<span class="token punctuation">.</span>convs<span class="token punctuation">)</span>
        <span class="token comment" spellcheck="true"># fpn forward pass</span>
        <span class="token comment" spellcheck="true"># see https://github.com/facebookresearch/detectron2/blob/main/detectron2/modeling/backbone/fpn.py</span>
        prev_features <span class="token operator">=</span> None
        <span class="token comment" spellcheck="true"># forward in top-down order (from low to high resolution)</span>
        n <span class="token operator">=</span> len<span class="token punctuation">(</span>self<span class="token punctuation">.</span>convs<span class="token punctuation">)</span> <span class="token operator">-</span> <span class="token number">1</span>
        <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>n<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            x <span class="token operator">=</span> xs<span class="token punctuation">[</span>i<span class="token punctuation">]</span>
            lateral_features <span class="token operator">=</span> self<span class="token punctuation">.</span>convs<span class="token punctuation">[</span>n <span class="token operator">-</span> i<span class="token punctuation">]</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>
            <span class="token keyword">if</span> i <span class="token keyword">in</span> self<span class="token punctuation">.</span>fpn_top_down_levels <span class="token operator">and</span> prev_features <span class="token keyword">is</span> <span class="token operator">not</span> None<span class="token punctuation">:</span>
                top_down_features <span class="token operator">=</span> F<span class="token punctuation">.</span>interpolate<span class="token punctuation">(</span>
                    prev_features<span class="token punctuation">.</span>to<span class="token punctuation">(</span>dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float32<span class="token punctuation">)</span><span class="token punctuation">,</span>
                    scale_factor<span class="token operator">=</span><span class="token number">2.0</span><span class="token punctuation">,</span>
                    mode<span class="token operator">=</span>self<span class="token punctuation">.</span>fpn_interp_model<span class="token punctuation">,</span>
                    align_corners<span class="token operator">=</span><span class="token punctuation">(</span>
                        None <span class="token keyword">if</span> self<span class="token punctuation">.</span>fpn_interp_model <span class="token operator">==</span> <span class="token string">"nearest"</span> <span class="token keyword">else</span> <span class="token boolean">False</span>
                    <span class="token punctuation">)</span><span class="token punctuation">,</span>
                    antialias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>
                <span class="token punctuation">)</span>
                prev_features <span class="token operator">=</span> lateral_features <span class="token operator">+</span> top_down_features
                <span class="token keyword">if</span> self<span class="token punctuation">.</span>fuse_type <span class="token operator">==</span> <span class="token string">"avg"</span><span class="token punctuation">:</span>
                    prev_features <span class="token operator">/=</span> <span class="token number">2</span>
            <span class="token keyword">else</span><span class="token punctuation">:</span>
                prev_features <span class="token operator">=</span> lateral_features
            x_out <span class="token operator">=</span> prev_features
            out<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> x_out
            pos<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> self<span class="token punctuation">.</span>position_encoding<span class="token punctuation">(</span>x_out<span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>x_out<span class="token punctuation">.</span>dtype<span class="token punctuation">)</span>

        <span class="token keyword">return</span> out<span class="token punctuation">,</span> pos
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>前向传播，将trunk传来的多尺度特征分别输入到各自的卷积网络中生成<code>lateral_features</code>，在<code>self.fpn_top_down_levels</code>中的层需要进行上采样，其结果会作为上一大节的&#x3D;&#x3D;🚠图像编码&#x3D;&#x3D;当中的<code>high_res_feats</code>用于协助还原高分辨率的masks。卷积输出<code>out</code>在进行位置编码得到<code>pos</code>。</p>
<p>&#x3D;&#x3D;所以回到<code>ImageEncoder</code>的层次看，整个图片编码器就是像YOLO一样提取多层不同尺度特征，然后分别输入到金字塔当中进一步提取特征，最后输出图像编码。区别就是主干用的是注意力而不是卷积、金字塔不像YOLO的PAN那样复杂地来回融合。&#x3D;&#x3D;</p>
<h2 id="🚠提示编码器：prompt-encoder-py"><a href="#🚠提示编码器：prompt-encoder-py" class="headerlink" title="🚠提示编码器：prompt_encoder.py"></a>🚠提示编码器：prompt_encoder.py</h2><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">PromptEncoder</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>
        self<span class="token punctuation">,</span>
        embed_dim<span class="token punctuation">:</span> int<span class="token punctuation">,</span>
        image_embedding_size<span class="token punctuation">:</span> Tuple<span class="token punctuation">[</span>int<span class="token punctuation">,</span> int<span class="token punctuation">]</span><span class="token punctuation">,</span>
        input_image_size<span class="token punctuation">:</span> Tuple<span class="token punctuation">[</span>int<span class="token punctuation">,</span> int<span class="token punctuation">]</span><span class="token punctuation">,</span>
        mask_in_chans<span class="token punctuation">:</span> int<span class="token punctuation">,</span>
        activation<span class="token punctuation">:</span> Type<span class="token punctuation">[</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">]</span> <span class="token operator">=</span> nn<span class="token punctuation">.</span>GELU<span class="token punctuation">,</span>
    <span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> None<span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""
        Encodes prompts for input to SAM's mask decoder.

        Arguments:
          embed_dim (int): The prompts' embedding dimension
          image_embedding_size (tuple(int, int)): The spatial size of the
            image embedding, as (H, W).
          input_image_size (int): The padded size of the image as input
            to the image encoder, as (H, W).
          mask_in_chans (int): The number of hidden channels used for
            encoding input masks.
          activation (nn.Module): The activation to use when encoding
            input masks.
        """</span>
        super<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>embed_dim <span class="token operator">=</span> embed_dim
        self<span class="token punctuation">.</span>input_image_size <span class="token operator">=</span> input_image_size
        self<span class="token punctuation">.</span>image_embedding_size <span class="token operator">=</span> image_embedding_size
        self<span class="token punctuation">.</span>pe_layer <span class="token operator">=</span> PositionEmbeddingRandom<span class="token punctuation">(</span>embed_dim <span class="token operator">//</span> <span class="token number">2</span><span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>num_point_embeddings<span class="token punctuation">:</span> int <span class="token operator">=</span> <span class="token number">4</span>  <span class="token comment" spellcheck="true"># pos/neg point + 2 box corners</span>
        point_embeddings <span class="token operator">=</span> <span class="token punctuation">[</span>
            nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> embed_dim<span class="token punctuation">)</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>self<span class="token punctuation">.</span>num_point_embeddings<span class="token punctuation">)</span>
        <span class="token punctuation">]</span>
        self<span class="token punctuation">.</span>point_embeddings <span class="token operator">=</span> nn<span class="token punctuation">.</span>ModuleList<span class="token punctuation">(</span>point_embeddings<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>not_a_point_embed <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> embed_dim<span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>mask_input_size <span class="token operator">=</span> <span class="token punctuation">(</span>
            <span class="token number">4</span> <span class="token operator">*</span> image_embedding_size<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
            <span class="token number">4</span> <span class="token operator">*</span> image_embedding_size<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>mask_downscaling <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>
            nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> mask_in_chans <span class="token operator">//</span> <span class="token number">4</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            LayerNorm2d<span class="token punctuation">(</span>mask_in_chans <span class="token operator">//</span> <span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            activation<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>mask_in_chans <span class="token operator">//</span> <span class="token number">4</span><span class="token punctuation">,</span> mask_in_chans<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            LayerNorm2d<span class="token punctuation">(</span>mask_in_chans<span class="token punctuation">)</span><span class="token punctuation">,</span>
            activation<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>mask_in_chans<span class="token punctuation">,</span> embed_dim<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        <span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>no_mask_embed <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> embed_dim<span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p><code>PromptEncoder</code>主要初始化了位置编码<code>pe_layer</code>、点嵌入层<code>point_embeddings</code>（2+2共4个编码层，分别给<strong>正&#x2F;负两种提示点坐标</strong>和<strong>框的两个顶点坐标</strong>用）、mask嵌入层<code>mask_downscaling</code>（用卷积降采样），没有提示时也会用长度1的嵌入层代替。</p>
<p>对三种提示的编码如下，它们<code>forward()</code>当中用到（上一大节&#x3D;&#x3D;🚠提示编码&#x3D;&#x3D;部分介绍过）。</p>
<pre class="line-numbers language-python"><code class="language-python">    <span class="token keyword">def</span> <span class="token function">_embed_points</span><span class="token punctuation">(</span>
        self<span class="token punctuation">,</span>
        points<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">,</span>
        labels<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">,</span>
        pad<span class="token punctuation">:</span> bool<span class="token punctuation">,</span>
    <span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""Embeds point prompts."""</span>
        points <span class="token operator">=</span> points <span class="token operator">+</span> <span class="token number">0.5</span>  <span class="token comment" spellcheck="true"># Shift to center of pixel</span>
        <span class="token keyword">if</span> pad<span class="token punctuation">:</span>
            padding_point <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token punctuation">(</span>points<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> device<span class="token operator">=</span>points<span class="token punctuation">.</span>device<span class="token punctuation">)</span>
            padding_label <span class="token operator">=</span> <span class="token operator">-</span>torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token punctuation">(</span>labels<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> device<span class="token operator">=</span>labels<span class="token punctuation">.</span>device<span class="token punctuation">)</span>
            points <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>points<span class="token punctuation">,</span> padding_point<span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
            labels <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>labels<span class="token punctuation">,</span> padding_label<span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
        point_embedding <span class="token operator">=</span> self<span class="token punctuation">.</span>pe_layer<span class="token punctuation">.</span>forward_with_coords<span class="token punctuation">(</span>
            points<span class="token punctuation">,</span> self<span class="token punctuation">.</span>input_image_size
        <span class="token punctuation">)</span>
        point_embedding<span class="token punctuation">[</span>labels <span class="token operator">==</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">0.0</span>
        point_embedding<span class="token punctuation">[</span>labels <span class="token operator">==</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">+=</span> self<span class="token punctuation">.</span>not_a_point_embed<span class="token punctuation">.</span>weight
        point_embedding<span class="token punctuation">[</span>labels <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">+=</span> self<span class="token punctuation">.</span>point_embeddings<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>weight
        point_embedding<span class="token punctuation">[</span>labels <span class="token operator">==</span> <span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">+=</span> self<span class="token punctuation">.</span>point_embeddings<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>weight
        point_embedding<span class="token punctuation">[</span>labels <span class="token operator">==</span> <span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">+=</span> self<span class="token punctuation">.</span>point_embeddings<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">.</span>weight
        point_embedding<span class="token punctuation">[</span>labels <span class="token operator">==</span> <span class="token number">3</span><span class="token punctuation">]</span> <span class="token operator">+=</span> self<span class="token punctuation">.</span>point_embeddings<span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">.</span>weight
        <span class="token keyword">return</span> point_embedding

    <span class="token keyword">def</span> <span class="token function">_embed_boxes</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> boxes<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""Embeds box prompts."""</span>
        boxes <span class="token operator">=</span> boxes <span class="token operator">+</span> <span class="token number">0.5</span>  <span class="token comment" spellcheck="true"># Shift to center of pixel</span>
        coords <span class="token operator">=</span> boxes<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>
        corner_embedding <span class="token operator">=</span> self<span class="token punctuation">.</span>pe_layer<span class="token punctuation">.</span>forward_with_coords<span class="token punctuation">(</span>
            coords<span class="token punctuation">,</span> self<span class="token punctuation">.</span>input_image_size
        <span class="token punctuation">)</span>
        corner_embedding<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token operator">+=</span> self<span class="token punctuation">.</span>point_embeddings<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">.</span>weight
        corner_embedding<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token operator">+=</span> self<span class="token punctuation">.</span>point_embeddings<span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">.</span>weight
        <span class="token keyword">return</span> corner_embedding

    <span class="token keyword">def</span> <span class="token function">_embed_masks</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> masks<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""Embeds mask inputs."""</span>
        mask_embedding <span class="token operator">=</span> self<span class="token punctuation">.</span>mask_downscaling<span class="token punctuation">(</span>masks<span class="token punctuation">)</span>
        <span class="token keyword">return</span> mask_embedding
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<ul>
<li><p>点编码：对pos&#x2F;neg两种提示点在位置编码之后，再分别用第0、1两个编码层编码即可，在没有框提示时会用0把2维坐标补齐到4维，保证4个编码层都有参与前向传播；</p>
</li>
<li><p>框编码：直接对2个坐标点进行位置编码，再分别用第2、3两个编码层进行编码；</p>
</li>
<li><p>掩码编码：输入到降采样的卷积网络即可。</p>
</li>
</ul>
<p>&#x3D;&#x3D;提示编码器的结构很简单，参数量很小，在没有提示时整个编码器几乎不发挥作用。&#x3D;&#x3D;</p>
<h2 id="🚠掩码解码器：mask-decoder-py"><a href="#🚠掩码解码器：mask-decoder-py" class="headerlink" title="🚠掩码解码器：mask_decoder.py"></a>🚠掩码解码器：mask_decoder.py</h2><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">MaskDecoder</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>
        self<span class="token punctuation">,</span>
        <span class="token operator">*</span><span class="token punctuation">,</span>
        transformer_dim<span class="token punctuation">:</span> int<span class="token punctuation">,</span>
        transformer<span class="token punctuation">:</span> nn<span class="token punctuation">.</span>Module<span class="token punctuation">,</span>
        num_multimask_outputs<span class="token punctuation">:</span> int <span class="token operator">=</span> <span class="token number">3</span><span class="token punctuation">,</span>
        activation<span class="token punctuation">:</span> Type<span class="token punctuation">[</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">]</span> <span class="token operator">=</span> nn<span class="token punctuation">.</span>GELU<span class="token punctuation">,</span>
        iou_head_depth<span class="token punctuation">:</span> int <span class="token operator">=</span> <span class="token number">3</span><span class="token punctuation">,</span>
        iou_head_hidden_dim<span class="token punctuation">:</span> int <span class="token operator">=</span> <span class="token number">256</span><span class="token punctuation">,</span>
        use_high_res_features<span class="token punctuation">:</span> bool <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">,</span>
        iou_prediction_use_sigmoid<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>
        dynamic_multimask_via_stability<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>
        dynamic_multimask_stability_delta<span class="token operator">=</span><span class="token number">0.05</span><span class="token punctuation">,</span>
        dynamic_multimask_stability_thresh<span class="token operator">=</span><span class="token number">0.98</span><span class="token punctuation">,</span>
        pred_obj_scores<span class="token punctuation">:</span> bool <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">,</span>
        pred_obj_scores_mlp<span class="token punctuation">:</span> bool <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">,</span>
        use_multimask_token_for_obj_ptr<span class="token punctuation">:</span> bool <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">,</span>
    <span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> None<span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""
        Predicts masks given an image and prompt embeddings, using a
        transformer architecture.

        Arguments:
          transformer_dim (int): the channel dimension of the transformer
          transformer (nn.Module): the transformer used to predict masks
          num_multimask_outputs (int): the number of masks to predict
            when disambiguating masks
          activation (nn.Module): the type of activation to use when
            upscaling masks
          iou_head_depth (int): the depth of the MLP used to predict
            mask quality
          iou_head_hidden_dim (int): the hidden dimension of the MLP
            used to predict mask quality
        """</span>
        super<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>transformer_dim <span class="token operator">=</span> transformer_dim
        self<span class="token punctuation">.</span>transformer <span class="token operator">=</span> transformer

        self<span class="token punctuation">.</span>num_multimask_outputs <span class="token operator">=</span> num_multimask_outputs

        self<span class="token punctuation">.</span>iou_token <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> transformer_dim<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>num_mask_tokens <span class="token operator">=</span> num_multimask_outputs <span class="token operator">+</span> <span class="token number">1</span>
        self<span class="token punctuation">.</span>mask_tokens <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>self<span class="token punctuation">.</span>num_mask_tokens<span class="token punctuation">,</span> transformer_dim<span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>pred_obj_scores <span class="token operator">=</span> pred_obj_scores
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>pred_obj_scores<span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>obj_score_token <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> transformer_dim<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>use_multimask_token_for_obj_ptr <span class="token operator">=</span> use_multimask_token_for_obj_ptr

        self<span class="token punctuation">.</span>output_upscaling <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>
            nn<span class="token punctuation">.</span>ConvTranspose2d<span class="token punctuation">(</span>
                transformer_dim<span class="token punctuation">,</span> transformer_dim <span class="token operator">//</span> <span class="token number">4</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span>
            <span class="token punctuation">)</span><span class="token punctuation">,</span>
            LayerNorm2d<span class="token punctuation">(</span>transformer_dim <span class="token operator">//</span> <span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            activation<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>ConvTranspose2d<span class="token punctuation">(</span>
                transformer_dim <span class="token operator">//</span> <span class="token number">4</span><span class="token punctuation">,</span> transformer_dim <span class="token operator">//</span> <span class="token number">8</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span>
            <span class="token punctuation">)</span><span class="token punctuation">,</span>
            activation<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        <span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>use_high_res_features <span class="token operator">=</span> use_high_res_features
        <span class="token keyword">if</span> use_high_res_features<span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>conv_s0 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>
                transformer_dim<span class="token punctuation">,</span> transformer_dim <span class="token operator">//</span> <span class="token number">8</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">1</span>
            <span class="token punctuation">)</span>
            self<span class="token punctuation">.</span>conv_s1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>
                transformer_dim<span class="token punctuation">,</span> transformer_dim <span class="token operator">//</span> <span class="token number">4</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">1</span>
            <span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>output_hypernetworks_mlps <span class="token operator">=</span> nn<span class="token punctuation">.</span>ModuleList<span class="token punctuation">(</span>
            <span class="token punctuation">[</span>
                MLP<span class="token punctuation">(</span>transformer_dim<span class="token punctuation">,</span> transformer_dim<span class="token punctuation">,</span> transformer_dim <span class="token operator">//</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span>
                <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>self<span class="token punctuation">.</span>num_mask_tokens<span class="token punctuation">)</span>
            <span class="token punctuation">]</span>
        <span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>iou_prediction_head <span class="token operator">=</span> MLP<span class="token punctuation">(</span>
            transformer_dim<span class="token punctuation">,</span>
            iou_head_hidden_dim<span class="token punctuation">,</span>
            self<span class="token punctuation">.</span>num_mask_tokens<span class="token punctuation">,</span>
            iou_head_depth<span class="token punctuation">,</span>
            sigmoid_output<span class="token operator">=</span>iou_prediction_use_sigmoid<span class="token punctuation">,</span>
        <span class="token punctuation">)</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>pred_obj_scores<span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>pred_obj_score_head <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>transformer_dim<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
            <span class="token keyword">if</span> pred_obj_scores_mlp<span class="token punctuation">:</span>
                self<span class="token punctuation">.</span>pred_obj_score_head <span class="token operator">=</span> MLP<span class="token punctuation">(</span>transformer_dim<span class="token punctuation">,</span> transformer_dim<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span>

        <span class="token comment" spellcheck="true"># When outputting a single mask, optionally we can dynamically fall back to the best</span>
        <span class="token comment" spellcheck="true"># multimask output token if the single mask output token gives low stability scores.</span>
        self<span class="token punctuation">.</span>dynamic_multimask_via_stability <span class="token operator">=</span> dynamic_multimask_via_stability
        self<span class="token punctuation">.</span>dynamic_multimask_stability_delta <span class="token operator">=</span> dynamic_multimask_stability_delta
        self<span class="token punctuation">.</span>dynamic_multimask_stability_thresh <span class="token operator">=</span> dynamic_multimask_stability_thresh
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>解码器核心是个<strong>transformer</strong>，除此之外有<strong>预测IoU和mask用的编码层</strong>（<code>self.iou_token</code>和<code>self.mask_tokens</code>）、<strong>上采样mask的转置卷积网络</strong><code>self.output_upscaling</code>、<strong>生成多掩码的MLP列表</strong><code>self.output_hypernetworks_mlps</code>、<strong>预测IoU分数的MLP</strong><code>self.iou_prediction_head</code>、<strong>预测遮挡分数的MLP</strong><code>self.pred_obj_score_head</code>，后面这些就没有什么特别之处。</p>
<p>前向传播在网络中的过程在上一大节的&#x3D;&#x3D;🚠掩码解码&#x3D;&#x3D;已经详细介绍了，这里补充一下<code>forward()</code>函数当中在网络输出后调用的<code>masks, iou_pred = self._dynamic_multimask_via_stability(masks, iou_pred)</code>：</p>
<pre class="line-numbers language-python"><code class="language-python">    <span class="token keyword">def</span> <span class="token function">_get_stability_scores</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> mask_logits<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""
        Compute stability scores of the mask logits based on the IoU between upper and
        lower thresholds, similar to https://github.com/fairinternal/onevision/pull/568.
        """</span>
        mask_logits <span class="token operator">=</span> mask_logits<span class="token punctuation">.</span>flatten<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">)</span>
        stability_delta <span class="token operator">=</span> self<span class="token punctuation">.</span>dynamic_multimask_stability_delta
        area_i <span class="token operator">=</span> torch<span class="token punctuation">.</span>sum<span class="token punctuation">(</span>mask_logits <span class="token operator">></span> stability_delta<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>float<span class="token punctuation">(</span><span class="token punctuation">)</span>
        area_u <span class="token operator">=</span> torch<span class="token punctuation">.</span>sum<span class="token punctuation">(</span>mask_logits <span class="token operator">></span> <span class="token operator">-</span>stability_delta<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>float<span class="token punctuation">(</span><span class="token punctuation">)</span>
        stability_scores <span class="token operator">=</span> torch<span class="token punctuation">.</span>where<span class="token punctuation">(</span>area_u <span class="token operator">></span> <span class="token number">0</span><span class="token punctuation">,</span> area_i <span class="token operator">/</span> area_u<span class="token punctuation">,</span> <span class="token number">1.0</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> stability_scores

    <span class="token keyword">def</span> <span class="token function">_dynamic_multimask_via_stability</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> all_mask_logits<span class="token punctuation">,</span> all_iou_scores<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""
        When outputting a single mask, if the stability score from the current single-mask
        output (based on output token 0) falls below a threshold, we instead select from
        multi-mask outputs (based on output token 1~3) the mask with the highest predicted
        IoU score. This is intended to ensure a valid mask for both clicking and tracking.
        """</span>
        <span class="token comment" spellcheck="true"># The best mask from multimask output tokens (1~3)</span>
        multimask_logits <span class="token operator">=</span> all_mask_logits<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span>
        multimask_iou_scores <span class="token operator">=</span> all_iou_scores<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">]</span>
        best_scores_inds <span class="token operator">=</span> torch<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>multimask_iou_scores<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
        batch_inds <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span>
            multimask_iou_scores<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> device<span class="token operator">=</span>all_iou_scores<span class="token punctuation">.</span>device
        <span class="token punctuation">)</span>
        best_multimask_logits <span class="token operator">=</span> multimask_logits<span class="token punctuation">[</span>batch_inds<span class="token punctuation">,</span> best_scores_inds<span class="token punctuation">]</span>
        best_multimask_logits <span class="token operator">=</span> best_multimask_logits<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>
        best_multimask_iou_scores <span class="token operator">=</span> multimask_iou_scores<span class="token punctuation">[</span>batch_inds<span class="token punctuation">,</span> best_scores_inds<span class="token punctuation">]</span>
        best_multimask_iou_scores <span class="token operator">=</span> best_multimask_iou_scores<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>

        <span class="token comment" spellcheck="true"># The mask from singlemask output token 0 and its stability score</span>
        singlemask_logits <span class="token operator">=</span> all_mask_logits<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">:</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span>
        singlemask_iou_scores <span class="token operator">=</span> all_iou_scores<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">:</span><span class="token number">1</span><span class="token punctuation">]</span>
        stability_scores <span class="token operator">=</span> self<span class="token punctuation">.</span>_get_stability_scores<span class="token punctuation">(</span>singlemask_logits<span class="token punctuation">)</span>
        is_stable <span class="token operator">=</span> stability_scores <span class="token operator">>=</span> self<span class="token punctuation">.</span>dynamic_multimask_stability_thresh

        <span class="token comment" spellcheck="true"># Dynamically fall back to best multimask output upon low stability scores.</span>
        mask_logits_out <span class="token operator">=</span> torch<span class="token punctuation">.</span>where<span class="token punctuation">(</span>
            is_stable<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> None<span class="token punctuation">,</span> None<span class="token punctuation">]</span><span class="token punctuation">.</span>expand_as<span class="token punctuation">(</span>singlemask_logits<span class="token punctuation">)</span><span class="token punctuation">,</span>
            singlemask_logits<span class="token punctuation">,</span>
            best_multimask_logits<span class="token punctuation">,</span>
        <span class="token punctuation">)</span>
        iou_scores_out <span class="token operator">=</span> torch<span class="token punctuation">.</span>where<span class="token punctuation">(</span>
            is_stable<span class="token punctuation">.</span>expand_as<span class="token punctuation">(</span>singlemask_iou_scores<span class="token punctuation">)</span><span class="token punctuation">,</span>
            singlemask_iou_scores<span class="token punctuation">,</span>
            best_multimask_iou_scores<span class="token punctuation">,</span>
        <span class="token punctuation">)</span>
        <span class="token keyword">return</span> mask_logits_out<span class="token punctuation">,</span> iou_scores_out
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>该函数的目的是在仅生成单个掩码时，如果基于输出token 0的掩码的稳定性分数低于阈值，则从多掩码输出（基于输出token 1~3）中选择具有最高预测IoU分数的掩码。稳定性分数就是根据 <code>area_u</code>（上界阈值对应的区域面积）是否大于0来计算稳定性分数 <code>stability_scores</code>，本质上是个类似IoU的值。所以该函数就是对生成掩码的一种优化手段。</p>
<h3 id="双路Transformer：transformer-py"><a href="#双路Transformer：transformer-py" class="headerlink" title="双路Transformer：transformer.py"></a>双路Transformer：transformer.py</h3><p><img src="/2025/06/12/SAM2%E6%9E%B6%E6%9E%84%E6%A6%82%E8%A7%88%E5%92%8C%E8%AF%A6%E7%BB%86%E8%A7%A3%E8%AF%BB/%E6%8E%A9%E7%A0%81%E8%A7%A3%E7%A0%81%E5%99%A8%E5%8F%8C%E8%B7%AFtransformer.png" alt="掩码解码器双路transformer"></p>
<p>这是掩码解码器用到的transformer模块，TwoWay指的应该是有两个输入形成两条传播路径，里面有：</p>
<ul>
<li><p><code>class Attention(nn.Module)</code>：掩码解码器内++所有的注意力机制++都是这个模块，就是一个<strong>点积多头注意力层</strong>，添加了线性层可以先改变输入q、k、v的维度再进行注意力计算；</p>
</li>
<li><p><code>class TwoWayAttentionBlock(nn.Module)</code>：如上图小框，包含一个<strong>自注意力</strong>、两个<strong>交叉注意力</strong>和一个<strong>MLP</strong>，还有<strong>LN</strong>层。</p>
</li>
<li><p><code>class TwoWayTransformer(nn.Module)</code>：如上图大框，包含2层<code>TwoWayAttentionBlock</code>和一个<strong>输出注意力层</strong><code>self.final_attn_token_to_image</code>。</p>
</li>
</ul>
<p>代码如下，前向传播确实是上图中的传播路径，<code>TwoWayAttentionBlock.forward()</code>的<code>queries</code>和<code>keys</code>分别是提示编码（稀疏）和加了mask提示的图像编码（密集）；值得注意的是两个交叉注意力的q、k是相反的，符合上图的 <strong>token to image</strong> 和 <strong>image to token</strong>；还有<code>TwoWayAttentionBlock</code>最终输出的<code>queries</code>是MLP之后的，而<code>keys</code>是最后一个交叉注意力之后的，符合图中两个不同位置的出口箭头，这两个输出如果是最后一个块的则会输出给<code>self.final_attn_token_to_image</code>和降采样模块，否则会作为q、k输入到下一个块。</p>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">TwoWayTransformer</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>
        self<span class="token punctuation">,</span>
        depth<span class="token punctuation">:</span> int<span class="token punctuation">,</span>
        embedding_dim<span class="token punctuation">:</span> int<span class="token punctuation">,</span>
        num_heads<span class="token punctuation">:</span> int<span class="token punctuation">,</span>
        mlp_dim<span class="token punctuation">:</span> int<span class="token punctuation">,</span>
        activation<span class="token punctuation">:</span> Type<span class="token punctuation">[</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">]</span> <span class="token operator">=</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">,</span>
        attention_downsample_rate<span class="token punctuation">:</span> int <span class="token operator">=</span> <span class="token number">2</span><span class="token punctuation">,</span>
    <span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> None<span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""
        A transformer decoder that attends to an input image using
        queries whose positional embedding is supplied.

        Args:
          depth (int): number of layers in the transformer
          embedding_dim (int): the channel dimension for the input embeddings
          num_heads (int): the number of heads for multihead attention. Must
            divide embedding_dim
          mlp_dim (int): the channel dimension internal to the MLP block
          activation (nn.Module): the activation to use in the MLP block
        """</span>
        super<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>depth <span class="token operator">=</span> depth
        self<span class="token punctuation">.</span>embedding_dim <span class="token operator">=</span> embedding_dim
        self<span class="token punctuation">.</span>num_heads <span class="token operator">=</span> num_heads
        self<span class="token punctuation">.</span>mlp_dim <span class="token operator">=</span> mlp_dim
        self<span class="token punctuation">.</span>layers <span class="token operator">=</span> nn<span class="token punctuation">.</span>ModuleList<span class="token punctuation">(</span><span class="token punctuation">)</span>

        <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>depth<span class="token punctuation">)</span><span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>append<span class="token punctuation">(</span>
                TwoWayAttentionBlock<span class="token punctuation">(</span>
                    embedding_dim<span class="token operator">=</span>embedding_dim<span class="token punctuation">,</span>
                    num_heads<span class="token operator">=</span>num_heads<span class="token punctuation">,</span>
                    mlp_dim<span class="token operator">=</span>mlp_dim<span class="token punctuation">,</span>
                    activation<span class="token operator">=</span>activation<span class="token punctuation">,</span>
                    attention_downsample_rate<span class="token operator">=</span>attention_downsample_rate<span class="token punctuation">,</span>
                    skip_first_layer_pe<span class="token operator">=</span><span class="token punctuation">(</span>i <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                <span class="token punctuation">)</span>
            <span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>final_attn_token_to_image <span class="token operator">=</span> Attention<span class="token punctuation">(</span>
            embedding_dim<span class="token punctuation">,</span> num_heads<span class="token punctuation">,</span> downsample_rate<span class="token operator">=</span>attention_downsample_rate
        <span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>norm_final_attn <span class="token operator">=</span> nn<span class="token punctuation">.</span>LayerNorm<span class="token punctuation">(</span>embedding_dim<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>
        self<span class="token punctuation">,</span>
        image_embedding<span class="token punctuation">:</span> Tensor<span class="token punctuation">,</span>
        image_pe<span class="token punctuation">:</span> Tensor<span class="token punctuation">,</span>
        point_embedding<span class="token punctuation">:</span> Tensor<span class="token punctuation">,</span>
    <span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> Tuple<span class="token punctuation">[</span>Tensor<span class="token punctuation">,</span> Tensor<span class="token punctuation">]</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""
        Args:
          image_embedding (torch.Tensor): image to attend to. Should be shape
            B x embedding_dim x h x w for any h and w.
          image_pe (torch.Tensor): the positional encoding to add to the image. Must
            have the same shape as image_embedding.
          point_embedding (torch.Tensor): the embedding to add to the query points.
            Must have shape B x N_points x embedding_dim for any N_points.

        Returns:
          torch.Tensor: the processed point_embedding
          torch.Tensor: the processed image_embedding
        """</span>
        <span class="token comment" spellcheck="true"># BxCxHxW -> BxHWxC == B x N_image_tokens x C</span>
        bs<span class="token punctuation">,</span> c<span class="token punctuation">,</span> h<span class="token punctuation">,</span> w <span class="token operator">=</span> image_embedding<span class="token punctuation">.</span>shape
        image_embedding <span class="token operator">=</span> image_embedding<span class="token punctuation">.</span>flatten<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
        image_pe <span class="token operator">=</span> image_pe<span class="token punctuation">.</span>flatten<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>

        <span class="token comment" spellcheck="true"># Prepare queries</span>
        queries <span class="token operator">=</span> point_embedding
        keys <span class="token operator">=</span> image_embedding

        <span class="token comment" spellcheck="true"># Apply transformer blocks and final layernorm</span>
        <span class="token keyword">for</span> layer <span class="token keyword">in</span> self<span class="token punctuation">.</span>layers<span class="token punctuation">:</span>
            queries<span class="token punctuation">,</span> keys <span class="token operator">=</span> layer<span class="token punctuation">(</span>
                queries<span class="token operator">=</span>queries<span class="token punctuation">,</span>
                keys<span class="token operator">=</span>keys<span class="token punctuation">,</span>
                query_pe<span class="token operator">=</span>point_embedding<span class="token punctuation">,</span>
                key_pe<span class="token operator">=</span>image_pe<span class="token punctuation">,</span>
            <span class="token punctuation">)</span>

        <span class="token comment" spellcheck="true"># Apply the final attention layer from the points to the image</span>
        q <span class="token operator">=</span> queries <span class="token operator">+</span> point_embedding
        k <span class="token operator">=</span> keys <span class="token operator">+</span> image_pe
        attn_out <span class="token operator">=</span> self<span class="token punctuation">.</span>final_attn_token_to_image<span class="token punctuation">(</span>q<span class="token operator">=</span>q<span class="token punctuation">,</span> k<span class="token operator">=</span>k<span class="token punctuation">,</span> v<span class="token operator">=</span>keys<span class="token punctuation">)</span>
        queries <span class="token operator">=</span> queries <span class="token operator">+</span> attn_out
        queries <span class="token operator">=</span> self<span class="token punctuation">.</span>norm_final_attn<span class="token punctuation">(</span>queries<span class="token punctuation">)</span>

        <span class="token keyword">return</span> queries<span class="token punctuation">,</span> keys
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">TwoWayAttentionBlock</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>
        self<span class="token punctuation">,</span>
        embedding_dim<span class="token punctuation">:</span> int<span class="token punctuation">,</span>
        num_heads<span class="token punctuation">:</span> int<span class="token punctuation">,</span>
        mlp_dim<span class="token punctuation">:</span> int <span class="token operator">=</span> <span class="token number">2048</span><span class="token punctuation">,</span>
        activation<span class="token punctuation">:</span> Type<span class="token punctuation">[</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">]</span> <span class="token operator">=</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">,</span>
        attention_downsample_rate<span class="token punctuation">:</span> int <span class="token operator">=</span> <span class="token number">2</span><span class="token punctuation">,</span>
        skip_first_layer_pe<span class="token punctuation">:</span> bool <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">,</span>
    <span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> None<span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""
        A transformer block with four layers: (1) self-attention of sparse
        inputs, (2) cross attention of sparse inputs to dense inputs, (3) mlp
        block on sparse inputs, and (4) cross attention of dense inputs to sparse
        inputs.

        Arguments:
          embedding_dim (int): the channel dimension of the embeddings
          num_heads (int): the number of heads in the attention layers
          mlp_dim (int): the hidden dimension of the mlp block
          activation (nn.Module): the activation of the mlp block
          skip_first_layer_pe (bool): skip the PE on the first layer
        """</span>
        super<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>self_attn <span class="token operator">=</span> Attention<span class="token punctuation">(</span>embedding_dim<span class="token punctuation">,</span> num_heads<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>norm1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>LayerNorm<span class="token punctuation">(</span>embedding_dim<span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>cross_attn_token_to_image <span class="token operator">=</span> Attention<span class="token punctuation">(</span>
            embedding_dim<span class="token punctuation">,</span> num_heads<span class="token punctuation">,</span> downsample_rate<span class="token operator">=</span>attention_downsample_rate
        <span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>norm2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>LayerNorm<span class="token punctuation">(</span>embedding_dim<span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>mlp <span class="token operator">=</span> MLP<span class="token punctuation">(</span>
            embedding_dim<span class="token punctuation">,</span> mlp_dim<span class="token punctuation">,</span> embedding_dim<span class="token punctuation">,</span> num_layers<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> activation<span class="token operator">=</span>activation
        <span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>norm3 <span class="token operator">=</span> nn<span class="token punctuation">.</span>LayerNorm<span class="token punctuation">(</span>embedding_dim<span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>norm4 <span class="token operator">=</span> nn<span class="token punctuation">.</span>LayerNorm<span class="token punctuation">(</span>embedding_dim<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>cross_attn_image_to_token <span class="token operator">=</span> Attention<span class="token punctuation">(</span>
            embedding_dim<span class="token punctuation">,</span> num_heads<span class="token punctuation">,</span> downsample_rate<span class="token operator">=</span>attention_downsample_rate
        <span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>skip_first_layer_pe <span class="token operator">=</span> skip_first_layer_pe

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>
        self<span class="token punctuation">,</span> queries<span class="token punctuation">:</span> Tensor<span class="token punctuation">,</span> keys<span class="token punctuation">:</span> Tensor<span class="token punctuation">,</span> query_pe<span class="token punctuation">:</span> Tensor<span class="token punctuation">,</span> key_pe<span class="token punctuation">:</span> Tensor
    <span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">></span> Tuple<span class="token punctuation">[</span>Tensor<span class="token punctuation">,</span> Tensor<span class="token punctuation">]</span><span class="token punctuation">:</span>
        <span class="token comment" spellcheck="true"># Self attention block</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>skip_first_layer_pe<span class="token punctuation">:</span>
            queries <span class="token operator">=</span> self<span class="token punctuation">.</span>self_attn<span class="token punctuation">(</span>q<span class="token operator">=</span>queries<span class="token punctuation">,</span> k<span class="token operator">=</span>queries<span class="token punctuation">,</span> v<span class="token operator">=</span>queries<span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            q <span class="token operator">=</span> queries <span class="token operator">+</span> query_pe
            attn_out <span class="token operator">=</span> self<span class="token punctuation">.</span>self_attn<span class="token punctuation">(</span>q<span class="token operator">=</span>q<span class="token punctuation">,</span> k<span class="token operator">=</span>q<span class="token punctuation">,</span> v<span class="token operator">=</span>queries<span class="token punctuation">)</span>
            queries <span class="token operator">=</span> queries <span class="token operator">+</span> attn_out
        queries <span class="token operator">=</span> self<span class="token punctuation">.</span>norm1<span class="token punctuation">(</span>queries<span class="token punctuation">)</span>

        <span class="token comment" spellcheck="true"># Cross attention block, tokens attending to image embedding</span>
        q <span class="token operator">=</span> queries <span class="token operator">+</span> query_pe
        k <span class="token operator">=</span> keys <span class="token operator">+</span> key_pe
        attn_out <span class="token operator">=</span> self<span class="token punctuation">.</span>cross_attn_token_to_image<span class="token punctuation">(</span>q<span class="token operator">=</span>q<span class="token punctuation">,</span> k<span class="token operator">=</span>k<span class="token punctuation">,</span> v<span class="token operator">=</span>keys<span class="token punctuation">)</span>
        queries <span class="token operator">=</span> queries <span class="token operator">+</span> attn_out
        queries <span class="token operator">=</span> self<span class="token punctuation">.</span>norm2<span class="token punctuation">(</span>queries<span class="token punctuation">)</span>

        <span class="token comment" spellcheck="true"># MLP block</span>
        mlp_out <span class="token operator">=</span> self<span class="token punctuation">.</span>mlp<span class="token punctuation">(</span>queries<span class="token punctuation">)</span>
        queries <span class="token operator">=</span> queries <span class="token operator">+</span> mlp_out
        queries <span class="token operator">=</span> self<span class="token punctuation">.</span>norm3<span class="token punctuation">(</span>queries<span class="token punctuation">)</span>

        <span class="token comment" spellcheck="true"># Cross attention block, image embedding attending to tokens</span>
        q <span class="token operator">=</span> queries <span class="token operator">+</span> query_pe
        k <span class="token operator">=</span> keys <span class="token operator">+</span> key_pe
        attn_out <span class="token operator">=</span> self<span class="token punctuation">.</span>cross_attn_image_to_token<span class="token punctuation">(</span>q<span class="token operator">=</span>k<span class="token punctuation">,</span> k<span class="token operator">=</span>q<span class="token punctuation">,</span> v<span class="token operator">=</span>queries<span class="token punctuation">)</span>
        keys <span class="token operator">=</span> keys <span class="token operator">+</span> attn_out
        keys <span class="token operator">=</span> self<span class="token punctuation">.</span>norm4<span class="token punctuation">(</span>keys<span class="token punctuation">)</span>

        <span class="token keyword">return</span> queries<span class="token punctuation">,</span> keys
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>&#x3D;&#x3D;掩码解码器的结构相对复杂，但是那张图画的和代码实现别无二致，所以只需要看懂那张图就够了。&#x3D;&#x3D;</p>
<h1 id="🔥图解架构"><a href="#🔥图解架构" class="headerlink" title="🔥图解架构"></a>🔥图解架构</h1><p>&#x3D;&#x3D;至此，我们通过训练的过程了解了高层次下SAM2构建和前向传播过程，再通过sam2文件夹的解读了解了更细致的网络结构、前向传播细节以及一些技术要点。&#x3D;&#x3D;</p>
<p>这是我结合源码和论文绘制的总结SAM2的图，右上角是简图，该图能较准确展示SAM2的结构和工作原理：</p>
<p><img src="/2025/06/12/SAM2%E6%9E%B6%E6%9E%84%E6%A6%82%E8%A7%88%E5%92%8C%E8%AF%A6%E7%BB%86%E8%A7%A3%E8%AF%BB/%E6%9C%80%E8%AF%A6%E7%BB%86SAM2%E6%9E%B6%E6%9E%84.png" alt="最详细SAM2架构"></p>
<p><img src="/2025/06/12/SAM2%E6%9E%B6%E6%9E%84%E6%A6%82%E8%A7%88%E5%92%8C%E8%AF%A6%E7%BB%86%E8%A7%A3%E8%AF%BB/%E6%95%B0%E6%8D%AE%E6%B5%81%E5%8A%A8%E5%9B%BE.png" alt="数据流动图"></p>
<p>上图是预测的一个样例，渐变框展示了每个关键节点数据的形状，输入2个图像，每个图像有4个目标框作为提示，预测效果见下图，就是实例分割了，接下来讲一下一些关键部分是如何体现前面介绍的模型特点的：</p>
<ul>
<li><p>图像编码器：输入3通道图像经过分块降低到256尺寸，在图像编码器的trunk得到四个阶段的不同尺寸的特征（256，128，64，32），四组特征分别一一对应地给到neck部分的卷积，输出通道数统一为256，高分辨率high_res_feats保存128和256尺寸，64尺寸则作为图像编码特征image_embed。</p>
</li>
<li><p>提示编码器：输出通道数也是256，稀疏和密集的提示编码显然参数量上有区别，输入到解码器前，图像和提示编码都会变形（其中4表示的是4个提示框）；</p>
</li>
<li><p>进入解码器，output_tokens由iou_token（1）、mask_tokens（4，多掩码输出功能1+3）、obj_token（1，图像任务没用就没写）一共6个token；在解码器当中不难发现紫线都是密集特征，绿色线是稀疏特征。</p>
</li>
<li><p>输出层面：</p>
<ol>
<li><p>最后的交叉注意力层的输出会重新取回iou_token、mask_tokens、obj_token，其中mask_tokens每一个都会对应一个MLP预测掩码，所以输出2×4×4×32；</p>
</li>
<li><p>IoU scores的形状是（2→batch，4→框个数，4→多掩码输出）是因为每个掩码都要做一个IoU预测；</p>
</li>
<li><p>遮挡分数是（2→batch，4→框个数，1）是因为每个物体只要判断一次是不是被挡住（图像任务没用）；</p>
</li>
<li><p>在乘法的位置输出了（2→batch，4→框个数，4→多掩码输出，256→尺寸，256→尺寸）的masks。</p>
</li>
</ol>
</li>
<li><p>在解码器外，如果每个目标的第0个mask稳定性分数不太行，则会在剩下3个中选择IoU最大的掩码输出（之前提到的<code>self._dynamic_multimask_via_stability</code>），只剩下（2，4，1，256，256），IoU分数自然也只剩一个，最后masks会还原为原图像尺寸。</p>
</li>
<li><p>值得注意的是，预测时，由于每张图提示个数不同，意味着要预测的mask个数也不同，所以导致不能用张量表示，而是用列表存储，这意味着只能一张一张地传进提示编码器和掩码解码器，batch size就变成了每张图要预测的目标个数（解码器里面的2只是个示意）。</p>
</li>
</ul>
<p><img src="/2025/06/12/SAM2%E6%9E%B6%E6%9E%84%E6%A6%82%E8%A7%88%E5%92%8C%E8%AF%A6%E7%BB%86%E8%A7%A3%E8%AF%BB/%E9%A2%84%E6%B5%8B%E7%A4%BA%E6%84%8F%E5%9B%BE.png" alt="预测示意图"></p>
<p>如下，简单描述总结每一个模块的作用：</p>
<p><img src="/2025/06/12/SAM2%E6%9E%B6%E6%9E%84%E6%A6%82%E8%A7%88%E5%92%8C%E8%AF%A6%E7%BB%86%E8%A7%A3%E8%AF%BB/SAM2%E6%A8%A1%E5%9D%97%E5%88%86%E5%B7%A5%E6%A6%82%E6%8B%AC.png" alt="SAM2模块功能概括"></p>
<p>从项目结构，总结sam2&#x2F;modeling下各个脚本的作用如下：</p>
<p><img src="/2025/06/12/SAM2%E6%9E%B6%E6%9E%84%E6%A6%82%E8%A7%88%E5%92%8C%E8%AF%A6%E7%BB%86%E8%A7%A3%E8%AF%BB/SAM2%E6%96%87%E4%BB%B6%E5%8A%9F%E8%83%BD%E6%A6%82%E6%8B%AC.png" alt="SAM2文件功能概括"></p>
<h1 id="🔥优劣分析"><a href="#🔥优劣分析" class="headerlink" title="🔥优劣分析"></a>🔥优劣分析</h1><p>SAM2能取得较好的效果离不开以下原因：</p>
<ol>
<li><p>性能提升上，最重要的是<strong>注意力机制</strong>的引入，大参数量和大规模数据集的情况下比传统卷积要强很多；</p>
</li>
<li><p><strong>多尺度</strong>的使用，在图像抽特征和最后上采样的部分都用了多个尺度的图像特征，这是YOLO等模型使用的老方法但是依然有用；</p>
</li>
<li><p><strong>提示功能</strong>的引入，比传统架构最具创新性的地方，它可以以不同形式传递使用者的目的，相比于预设分类再训练，这种方法很直接地满足宽泛意义上的具象需求（抽象的不能满足），SAM2对不同类型提示的编码方式也不同，密集和稀疏的提示也会被不同地使用；</p>
</li>
<li><p>图像编码器应该是用了预训练的权重，已经有很不错的能力，再加上SAM2做了一个1B的<strong>数据集</strong>，用模型本身就能源源不断地产生新的数据，相当好用；</p>
</li>
<li><p>掩码解码器的<strong>结构设计</strong>，可以不错地融合提示和图像两种模态的编码，输出解耦和YOLO系列一样能够独立输出不同模态的预测；</p>
</li>
<li><p><strong>轻量</strong>，其速度虽然不比YOLO，但是模型依然不算大，多尺度的设计和很多降采样操作有效降低了模型参数和计算速度，对效果和速度做了不错的权衡，也提供了多个规模的模型。</p>
</li>
</ol>
<p>总的来说SAM2上能看到ViT、YOLO等工作用到的很多好的技术，本身也在模态上进行了创新，加上transformer本身的强度，会得到很好的效果。</p>
<p>缺点和改进可能性：</p>
<ol>
<li><p>SAM1的论文中提示里面包含了<strong>文本</strong>，SAM2的示意图中去掉了文本这个提示，但是两个项目的源码都没有文本能被作为提示输入模型的迹象，SAM1原文是外接了一个CLIP的文本编码器，编码后再输入SAM作为提示，效果上对简单的词有效果，最好还是得配合点作为提示。如果文本能实现更好的预测那其实是超越了其它提示形式，因为点、框和掩码输入需要用户本身知道这些东西在哪，并且应用场景允许用户进行交互去给模型这些信息，而这些信息带有“作弊”的性质，而文本就不需要交互也不带有“作弊”性质，而是准确地反应需求，这样SAM就能部分地接近LISA的效果；</p>
</li>
<li><p>对于分割任务，SAM2本身不具有<strong>分类</strong>的能力，也就是它不关心类别只关系它长什么样，它只知道给它一个提示它就为此分割出一个掩码，而不给提示的情况下模型只会输出一个目标的掩码，所以如果对分类有需要或者想自动化生成提示，SAM1原文的方法是先用一个目标检测器ViTDet生成框作为提示，这其实也是有效的，因为如果外接一个小模型也不会有很大影响，同时这一定程度反映了SAM的局限性。</p>
</li>
<li><p>结构上是否可以<strong>简化</strong>；</p>
</li>
<li><p><strong>位置编码</strong>是否可以用更新的，里面有实现旋转位置编码，但是没有调用；</p>
</li>
</ol>
<p>总的来说SAM2的对于图像分割领域的定位更像是一个提供提示输入的基座模型。不能把它当作一个全能的分割模型，也就是在它主打的提示分割功能之外，如果要利用它，比较合适的用法就是像LISA一样把它当作一个分割器模块去使用，还有它的视觉编码器单独拿出来也比较有价值（类似ResNet-50），因为它学过海量的图像信息。</p>
<p>所以我认为如果要改进SAM，一种路径是在它的提示分割领域内做效果上的提升，还有一种就是把它当做系统的一部分做出更有用的功能。</p>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
<div class="share-btn">
      <span class="share-sns share-outer">
        <i class="ri-share-forward-line"></i>
        分享
      </span>
      <div class="share-wrap">
        <i class="arrow"></i>
        <div class="share-icons">
          
          <a class="weibo share-sns" href="javascript:;" data-type="weibo">
            <i class="ri-weibo-fill"></i>
          </a>
          <a class="weixin share-sns wxFab" href="javascript:;" data-type="weixin">
            <i class="ri-wechat-fill"></i>
          </a>
          <a class="qq share-sns" href="javascript:;" data-type="qq">
            <i class="ri-qq-fill"></i>
          </a>
          <a class="douban share-sns" href="javascript:;" data-type="douban">
            <i class="ri-douban-line"></i>
          </a>
          <!-- <a class="qzone share-sns" href="javascript:;" data-type="qzone">
            <i class="icon icon-qzone"></i>
          </a> -->
          
          <a class="facebook share-sns" href="javascript:;" data-type="facebook">
            <i class="ri-facebook-circle-fill"></i>
          </a>
          <a class="twitter share-sns" href="javascript:;" data-type="twitter">
            <i class="ri-twitter-fill"></i>
          </a>
          <a class="google share-sns" href="javascript:;" data-type="google">
            <i class="ri-google-fill"></i>
          </a>
        </div>
      </div>
</div>

<div class="wx-share-modal">
    <a class="modal-close" href="javascript:;"><i class="ri-close-circle-line"></i></a>
    <p>扫一扫，分享到微信</p>
    <div class="wx-qrcode">
      <img src="//api.qrserver.com/v1/create-qr-code/?size=150x150&data=https://legendleochen.top/2025/06/12/SAM2%E6%9E%B6%E6%9E%84%E6%A6%82%E8%A7%88%E5%92%8C%E8%AF%A6%E7%BB%86%E8%A7%A3%E8%AF%BB/" alt="微信分享二维码">
    </div>
</div>

<div id="share-mask"></div>  
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Python/" rel="tag">Python</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/SAM2/" rel="tag">SAM2</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2/" rel="tag">图像分割</a></li></ul>

    </footer>
  </div>

   
  <nav class="article-nav">
    
      <a href="/2025/07/12/LISA%E5%90%8E%E6%8E%A2%E7%B4%A2/" class="article-nav-link">
        <strong class="article-nav-caption">上一篇</strong>
        <div class="article-nav-title">
          
            LISA后探索
          
        </div>
      </a>
    
    
      <a href="/2025/05/21/LISA%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%B8%8A%E6%89%8B%E5%92%8C%E5%BE%AE%E8%B0%83/" class="article-nav-link">
        <strong class="article-nav-caption">下一篇</strong>
        <div class="article-nav-title">LISA图像分割模型的上手和微调</div>
      </a>
    
  </nav>

  
   
  
    
</article>

</section>
      <footer class="footer">
  <div class="outer">
    <ul>
      <li>
        Copyrights &copy;
        2023-2025
        <i class="ri-heart-fill heart_icon"></i> LegendLeo Chen
      </li>
    </ul>
    <ul>
      <li>
        
      </li>
    </ul>
    <ul>
      <li>
        
        
        <span>
  <span><i class="ri-user-3-fill"></i>访问人数:<span id="busuanzi_value_site_uv"></span></span>
  <span class="division">|</span>
  <span><i class="ri-eye-fill"></i>浏览次数:<span id="busuanzi_value_page_pv"></span></span>
</span>
        
      </li>
    </ul>
    <ul>
      
    </ul>
    <ul>
      
    </ul>
    <ul>
      <li>
        <!-- cnzz统计 -->
        
        <script type="text/javascript" src='https://s9.cnzz.com/z_stat.php?id=1278069914&amp;web_id=1278069914'></script>
        
      </li>
    </ul>
  </div>
</footer>    
    </main>
    <div class="float_btns">
      <div class="totop" id="totop">
  <i class="ri-arrow-up-line"></i>
</div>

<div class="todark" id="todark">
  <i class="ri-moon-line"></i>
</div>

    </div>
    <aside class="sidebar on">
      <button class="navbar-toggle"></button>
<nav class="navbar">
  
  <div class="logo">
    <a href="/"><img src="/images/mylogo.png" alt="LegendLeo Chen 的空间"></a>
  </div>
  
  <ul class="nav nav-main">
    
    <li class="nav-item">
      <a class="nav-item-link" href="/">🚀主页</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/archives">💾归档</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/categories">🧭分类</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags">🏷️标签</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/about">🛸关于</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/analytics">📊统计</a>
    </li>
    
  </ul>
</nav>
<nav class="navbar navbar-bottom">
  <ul class="nav">
    <li class="nav-item">
      
      <a class="nav-item-link nav-item-search"  title="搜索">
        <i class="ri-search-line"></i>
      </a>
      
      
    </li>
  </ul>
</nav>
<div class="search-form-wrap">
  <div class="local-search local-search-plugin">
  <input type="search" id="local-search-input" class="local-search-input" placeholder="Search...">
  <div id="local-search-result" class="local-search-result"></div>
</div>
</div>
    </aside>
    <div id="mask"></div>

<!-- #reward -->
<div id="reward">
  <span class="close"><i class="ri-close-line"></i></span>
  <p class="reward-p"><i class="ri-cup-line"></i>请我喝杯咖啡吧~</p>
  <div class="reward-box">
    
    <div class="reward-item">
      <img class="reward-img" src="/images/alipay.jpg">
      <span class="reward-type">支付宝</span>
    </div>
    
    
    <div class="reward-item">
      <img class="reward-img" src="/images/wechat.jpg">
      <span class="reward-type">微信</span>
    </div>
    
  </div>
</div>
    
<script src="/js/jquery-3.6.0.min.js"></script>
 
<script src="/js/lazyload.min.js"></script>

<!-- Tocbot -->
 
<script src="/js/tocbot.min.js"></script>

<script>
  tocbot.init({
    tocSelector: ".tocbot",
    contentSelector: ".article-entry",
    headingSelector: "h1, h2, h3, h4, h5, h6",
    hasInnerContainers: true,
    scrollSmooth: true,
    scrollContainer: "main",
    positionFixedSelector: ".tocbot",
    positionFixedClass: "is-position-fixed",
    fixedSidebarOffset: "auto",
  });
</script>

<script src="https://cdn.staticfile.org/jquery-modal/0.9.2/jquery.modal.min.js"></script>
<link
  rel="stylesheet"
  href="https://cdn.staticfile.org/jquery-modal/0.9.2/jquery.modal.min.css"
/>
<script src="https://cdn.staticfile.org/justifiedGallery/3.8.1/js/jquery.justifiedGallery.min.js"></script>

<script src="/dist/main.js"></script>

<!-- ImageViewer -->
 <!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                <!--  Controls are self-explanatory. Order can be changed. -->

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" style="display:none" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>

<link rel="stylesheet" href="https://cdn.staticfile.org/photoswipe/4.1.3/photoswipe.min.css">
<link rel="stylesheet" href="https://cdn.staticfile.org/photoswipe/4.1.3/default-skin/default-skin.min.css">
<script src="https://cdn.staticfile.org/photoswipe/4.1.3/photoswipe.min.js"></script>
<script src="https://cdn.staticfile.org/photoswipe/4.1.3/photoswipe-ui-default.min.js"></script>

<script>
    function viewer_init() {
        let pswpElement = document.querySelectorAll('.pswp')[0];
        let $imgArr = document.querySelectorAll(('.article-entry img:not(.reward-img)'))

        $imgArr.forEach(($em, i) => {
            $em.onclick = () => {
                // slider展开状态
                // todo: 这样不好，后面改成状态
                if (document.querySelector('.left-col.show')) return
                let items = []
                $imgArr.forEach(($em2, i2) => {
                    let img = $em2.getAttribute('data-idx', i2)
                    let src = $em2.getAttribute('data-target') || $em2.getAttribute('src')
                    let title = $em2.getAttribute('alt')
                    // 获得原图尺寸
                    const image = new Image()
                    image.src = src
                    items.push({
                        src: src,
                        w: image.width || $em2.width,
                        h: image.height || $em2.height,
                        title: title
                    })
                })
                var gallery = new PhotoSwipe(pswpElement, PhotoSwipeUI_Default, items, {
                    index: parseInt(i)
                });
                gallery.init()
            }
        })
    }
    viewer_init()
</script> 
<!-- MathJax -->

<!-- Katex -->

<!-- busuanzi  -->
 
<script src="/js/busuanzi-2.3.pure.min.js"></script>
 
<!-- ClickLove -->

<!-- ClickBoom1 -->

<!-- ClickBoom2 -->

<!-- CodeCopy -->
 
<link rel="stylesheet" href="/css/clipboard.css">
 <script src="https://cdn.staticfile.org/clipboard.js/2.0.10/clipboard.min.js"></script>
<script>
  function wait(callback, seconds) {
    var timelag = null;
    timelag = window.setTimeout(callback, seconds);
  }
  !function (e, t, a) {
    var initCopyCode = function(){
      var copyHtml = '';
      copyHtml += '<button class="btn-copy" data-clipboard-snippet="">';
      copyHtml += '<i class="ri-file-copy-2-line"></i><span>COPY</span>';
      copyHtml += '</button>';
      $(".highlight .code pre").before(copyHtml);
      $(".article pre code").before(copyHtml);
      var clipboard = new ClipboardJS('.btn-copy', {
        target: function(trigger) {
          return trigger.nextElementSibling;
        }
      });
      clipboard.on('success', function(e) {
        let $btn = $(e.trigger);
        $btn.addClass('copied');
        let $icon = $($btn.find('i'));
        $icon.removeClass('ri-file-copy-2-line');
        $icon.addClass('ri-checkbox-circle-line');
        let $span = $($btn.find('span'));
        $span[0].innerText = 'COPIED';
        
        wait(function () { // 等待两秒钟后恢复
          $icon.removeClass('ri-checkbox-circle-line');
          $icon.addClass('ri-file-copy-2-line');
          $span[0].innerText = 'COPY';
        }, 2000);
      });
      clipboard.on('error', function(e) {
        e.clearSelection();
        let $btn = $(e.trigger);
        $btn.addClass('copy-failed');
        let $icon = $($btn.find('i'));
        $icon.removeClass('ri-file-copy-2-line');
        $icon.addClass('ri-time-line');
        let $span = $($btn.find('span'));
        $span[0].innerText = 'COPY FAILED';
        
        wait(function () { // 等待两秒钟后恢复
          $icon.removeClass('ri-time-line');
          $icon.addClass('ri-file-copy-2-line');
          $span[0].innerText = 'COPY';
        }, 2000);
      });
    }
    initCopyCode();
  }(window, document);
</script>
 
<!-- CanvasBackground -->

<script>
  if (window.mermaid) {
    mermaid.initialize({ theme: "forest" });
  }
</script>


    
    <div id="music">
    
    
    
    <iframe frameborder="no" border="1" marginwidth="0" marginheight="0" width="200" height="52"
        src="//music.163.com/outchain/player?type=2&id=1491212&auto=1&height=32"></iframe>
</div>

<style>
    #music {
        position: fixed;
        right: 15px;
        bottom: 0;
        z-index: 998;
    }
</style>
    
    

  </div>
  <!-- 背景气泡 -->
  <!--
  <div class="balls-container">
    <div class="balls-particles">
      <span style="--i:11;"></span>
      <span style="--i:12;"></span>
      <span style="--i:24;"></span>
      <span style="--i:10;"></span>
      <span style="--i:14;"></span>
      <span style="--i:23;"></span>
      <span style="--i:18;"></span>
      <span style="--i:16;"></span>
      <span style="--i:19;"></span>
      <span style="--i:20;"></span>
      <span style="--i:22;"></span>
      <span style="--i:25;"></span>
      <span style="--i:18;"></span>
      <span style="--i:21;"></span>
      <span style="--i:13;"></span>
      <span style="--i:15;"></span>
      <span style="--i:26;"></span>
      <span style="--i:17;"></span>
      <span style="--i:13;"></span>
      <span style="--i:26;"></span>
      <span style="--i:28;"></span>
      <span style="--i:11;"></span>
      <span style="--i:12;"></span>
      <span style="--i:24;"></span>
      <span style="--i:10;"></span>
      <span style="--i:14;"></span>
      <span style="--i:23;"></span>
      <span style="--i:18;"></span>
      <span style="--i:16;"></span>
      <span style="--i:19;"></span>
      <span style="--i:20;"></span>
      <span style="--i:22;"></span>
      <span style="--i:25;"></span>
      <span style="--i:18;"></span>
      <span style="--i:21;"></span>
      <span style="--i:13;"></span>
      <span style="--i:15;"></span>
      <span style="--i:26;"></span>
      <span style="--i:17;"></span>
      <span style="--i:13;"></span>
      <span style="--i:26;"></span>
      <span style="--i:28;"></span>
    </div>
  </div>
  <style>
    *
    {
      margin: 0;
      padding: 0;
      box-sizing: border-box;
    }
    
    .balls-container
    { 
      position: fixed;
      top: 0px;
      left: 0px;
      width: 100%;
      height: 100vh;
      overflow: hidden;
      opacity: 0.3;
    }
    
    .balls-particles
    {
      position: fixed;
      display: flex;
      z-index: 3;
      padding: 0 20px;
    }
    
    .balls-particles span
    {
      position: relative;
      bottom: 30px;
      width: 30px;
      height: 30px;
      background-color: #4fc3dc;
      box-shadow: 0 0 0 10px #4fc3dc44,
      0 0 50px #4fc3dc,
      -100px 0 #4fc3dc99,
      100px 0 #ff2d7599;
      margin: 0 4px;
      border-radius: 50%;
      animation: animate 15s ease infinite;
      animation-delay: calc(125s / var(--i));
      transform: translateY(120vh);
    }
    .balls-particles span:nth-child(even) {
      background-color: #ff2d75;
      box-shadow: 0 0 0 10px #ff267544,
      0 0 50px #ff2d75,
      -100px 0 #4fc3dc99,
      100px 0 #4fc3dc99;
      ;
    }
    
    @keyframes animate {
      0%
      {
        transform: translateY(120vh) scale(0) rotate(0deg);
      }
      20%
      {
        transform: translateY(100vh) scale(1) rotate(0deg);
      }
      100%
      {
        transform: translateY(-50vh) scale(0.5) rotate(360deg);
      }
    }
  </style> -->
  <!-- 地月系统 -->
  <!-- <div class="earth-container" >
    <div class="planet"></div>
    <div class="satellite"></div>
   </div>
   <style>
    *{
      padding: 0;
      margin: 0;
      }
      .earth-container {
        width: 36.25em;
        height: 36.25em;
        position: absolute;
        top:5%;
        left: 93%;
        transform: translate(-50%, -50%);
        opacity: 0.3;
      }
      
      .planet{
        width: 15.62*3em;
        height: 15.62*3em;
        background-color: #02c0f5;
        border-radius: 50%;
        position: absolute;
        margin: auto;
        top:0;
        right: 0;
        bottom: 0;
        left: 0;
        z-index: 1;
      }
      
      .planet::before{
        content: '';
        width: 4em;
        height: 4em;
        background-color: #008fd6;
        position: absolute;
        top:10em;
        left: 8em;
        border-radius: 50%; 
        box-shadow: 15em 15em 0 2em #00d68b, 5em 8em 0 3em #10ade1;
      }
      
      .satellite{
        width: 5em;
        height: 5em;
        background-color: #dee517;
        border-radius: 50%;
        position: relative;
        left: -5em;
        bottom: -30em;
        animation: spin 5s infinite;
        z-index: 1;
      }
      
      @keyframes spin {
        49%{
          z-index: 1;
        }
        50%{
          bottom: 3em;
          left: 35em;
          z-index: -1;
        }
        100%{
          z-index: -1;
        }
      }
    </style> -->
<!-- 三角彩带背景 -->
  <canvas id="evanyou-canvas" style="opacity: 0.3; position: fixed; top: 0px; left: 0px; z-index: -1; width: 100%; height: 100%; pointer-events: none;"></canvas>
  <script src="https://cdn.jsdelivr.net/gh/XXXZhy/Blog_Image/js/evanyou_canvas.js"></script>
</body>

</html>